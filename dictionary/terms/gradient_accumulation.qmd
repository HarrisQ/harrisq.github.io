---
title: Gradient Accumulation
description: "A technique for running or fitting large [models](model.qmd) on a not-so-powerful GPU."
---

---

Let's say you want to use a batch size of 64, but the [model](model.qmd) doesn't fit with that size on your GPU.

1. First determine the largest possible batch size that can fit on your GPU. Let's say it's 16. It _may_ be better to use batch sizes that are a power of 2.
1. Calculate the [gradients](gradient.qmd) for $X$ batches without updating the parameters.
    - $X$ is your desired batch size divided by the batch size you are using.
    - Desired batch size is 64; batch size we are using is 16.
    - $64 ÷ 16 = 4$
    - $X$ is 4. This is because the size of 4 batches, in this case, sums to 64.
1. Next, sum all respective [gradients](gradient.qmd) — hence the term 'gradient accumulation'.
1. Now update your parameters based on these summed [gradients](gradient.qmd). This will have the same effect as if you used a batch size of 64.

:::{.callout-note appearance="simple" collapse="true"}
# Note

Using a smaller batch size to fit a larger [model](model.qmd) onto your GPU isn't optimal. A smaller batch size means you would have to tweak your optimal hyperparameters, such as the [learning rate](learning_rate.qmd). Your [loss](loss.qmd) would also become less accurate since it is being calculated on a smaller group of items.
:::