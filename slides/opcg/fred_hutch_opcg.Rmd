--- 
title: "On Forward Sufficient Dimension Reduction for Categorical and Ordinal Responses"
# subtitle: "with Multinomial Response"
author: "Harris Quach"
institute: "Pennsylvania State University"
date: "2022/04/04 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [xaringan-themer.css, "mytheme.css"]
    # "hygge-duke","cols.css", "ninjutsu" ,"assets/ninpo.css", 
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      # beforeInit: "my_macros.js" # Macros File
      # For Live Preview, run xaringan::inf_mr() in console
    # toc: true
    # toc_depth: 3
    includes:
      before_body: local_tex.html
    seal: false
   
# "rutgers-fonts","rutgers", "hygge", "shinobi"

---

```{r xaringan-themer, include = FALSE}

library(xaringanthemer)

style_xaringan(
  text_color = "#000",
  header_color = "#2d60ba",
  background_color = "#FFF",
  link_color = "rgb(249, 38, 114)",
  text_bold_color = "#2d60ba", 
  padding = "16px 64px 16px 64px", 
  code_highlight_color = "rgba(255,255,0,0.5)",
  code_inline_color = "#000",
  code_inline_background_color = NULL,
  code_inline_font_size = "1em",
  inverse_background_color = "#1e407c",
  inverse_text_color = "#d6d6d6",
  inverse_text_shadow = FALSE, 
  footnote_color = NULL,
  footnote_font_size = "0.9em",
  footnote_position_bottom = "60px",
  left_column_subtle_color = "#777",
  left_column_selected_color = "#000",
  blockquote_left_border_color = "lightgray",
  table_border_color = "#666",
  table_row_border_color = "#ddd",
  table_row_even_background_color = "#eee",
  base_font_size = "20px",
  text_font_size = "1rem",
  header_h1_font_size = "1.5rem",
  header_h2_font_size = "1.25rem",
  header_h3_font_size = "1.25rem", 
  header_background_ignore_classes = c("normal", "inverse", "title", "middle",
                                       "bottom"),
  text_slide_number_font_size = "0.9em", 
  extra_css = list("h2" = list("color" = "#9ab6e7")  #03A696;
  ), 
  outfile = "xaringan-themer.css"
)
```

```{r, load_refs, include=FALSE, cache=FALSE}
library(RefManageR)
BibOptions(check.entries = FALSE,
           bib.style = "authoryear",
           cite.style = "text",
           style = "markdown",
           hyperlink = FALSE,
           dashed = FALSE)
bib_sdr <- ReadBib("bib_sdr.bib",
                   check = FALSE)
bib_opg <- ReadBib("bib_opg.bib",
                   check = FALSE)
```
 


<!-- class: title-slide -->

# On Forward Sufficient Dimension Reduction for Categorical and Ordinal Responses
<!-- # .bg-text[Generalized Forward Sufficient Dimension Reduction for Classification] -->
<!-- <hr width="700" align="left" /> -->
<hr/>

Harris Quach (joint work with Dr. Bing Li) <br/> Date: "2022/04/04"


---
class: inverse, middle

<!-- inverse makes the background black and text white -->

# .bg-text.center[Overview of Talk]  

.md-text[
  
1. A brief introduction to sufficient dimension reduction (SDR)
  
2. Broad idea of the SDR method in the paper
  
3. SDR for categorical and ordinal responses
  
]

---
class: left, top
# What is Sufficient Dimension Reduction? 


Suppose we have a large dataset with some response $Y \in \R^m$ and predictors $X \in \R^p$.

  - When $p$ is large, lower dimensional summaries of $X$ are helpful for visualization and application of conventional statistical methods

  - Finding a lower dimensional summary of $X$ means finding $\beta \in \R^{p \times d}$, where $d < p$, in order to construct the lower dimensional summary $\beta^\top X$
  
  - Sufficient Dimension Reduction (SDR) are approaches for finding $\beta$ such that $\beta^\top X$ retains all relevant information about $Y$
  
  - $\beta$ can preserve different information by satisfying different criteria.

---
class: left, top
# Central Subspaces

The information preserved by $\beta$ is characterized by conditional independence:

  - $Y \indep X | \beta^\top X$; `r Cite(bib_sdr, author="Li", title="dimension", year="2018")`
    - $\beta$ preserves all information between $Y$ and $X$ and $\mathrm{span}(\beta)$ is an SDR subspace for $Y|X$; the smallest such subspace is called the **Central Subspace**. 

  - $Y \indep E(Y|X) | \beta^\top X$; `r Cite(bib_sdr, author=c("ma", "Li"), title="mean")`
    - $\beta$ preserves all information between $Y$ and $E(Y|X)$ and $\mathrm{span}(\beta)$ is an SDR subspace for $E(Y|X)$; the smallest such subspace is called the **Central Mean Subspace**. 

--

**NOTE**

We actually want the subspace generated by $\beta$, i.e. $\mathrm{span}(\beta)$.
  
  - **When we say we are estimating $\beta$, we mean estimating a basis for the subspace generated by $\beta$, i.e a basis for $\mathrm{span}(\beta)$.**
  
  
---
class: left, top
# Alternative Subspaces

The information preserved by $\beta$ is characterized by conditional independence:

  - $Y - E(Y|X) \indep \mathrm{Var}(Y|X) | \beta^\top X$; `r Cite(bib_sdr, author="zhu", title="variance", year="2009")`   
    - $\beta$ preserves all information between $Y - E(Y|X)$ and $\mathrm{Var}(Y|X)$; 
    <!-- the smallest such subspace is called the **Central Variance Subspace** -->
    
  - $Y  \indep Q_\tau(X) | \beta^\top X$; `r Cite(bib_sdr, author=c("kong", "li"), title=c("index", "functional"), year=c("2012", "2014") )`
    - $\beta$ preserves all information between $Y$ and $Q_\tau(X)=\inf\{y : P(Y \leq y |X) \geq \tau\}$; 
    <!-- the smallest such subspace is called the **Central $\tau$-th Quantile Subspace**   -->
  
  - $Y  \indep \theta(X) | \beta^\top X$; `r Cite(bib_sdr, author=c("li"), title=c("functional"), year=c("2014") )`
    - $\beta$ preserves all information between $Y$ and $\theta(X)$, where $\theta(X)$ is defined through a conditional statistical functional; 
    
  - $N(B_1),\ldots, N(B_k)  \indep X(B_1),\ldots,X(B_k) | \beta^\top X(B_1),\ldots,\beta^\top X(B_k)$; `r Cite(bib_sdr, author=c("guan"), title=c("spatial"), year=c("2010") )`
    - $\beta$ preserves all information between the counting process $N$ and the Gaussian random field $X$ over sets $B_1,\ldots,B_k$;  

---
class: left, top
# Why Sufficient Dimension Reduction? 

<blockquote>
"...I’ve never been comfortable with the way in which
our discipline has embraced sparsity to the exclusion of
everything else. I mentioned before that I like the availability
of options in statistics, but for a long time now the
prevailing attitude toward high-dimensional problems has
been that its natural to assume sparsity. I find no sense in
which it’s natural. That overwhelming emphasis on sparsity
has, I think, kept us from seeing other solutions and
options. To be clear, I have nothing against sparsity per se,
and I think it’s a reasonable modeling construct. But it’s
not reasonable just because you have high-dimensional
data."  
.right[~ <cite>*Dennis Cook* (2021)</cite>]
.right[`r Cite(bib_sdr, author="Li", title="Cook", year="2021")`]
</blockquote>
  
---
class: left, top
# Why Sufficient Dimension Reduction? 

  - Sufficient Dimension Reduction is a method for **supervised feature extraction**
  
  - unique in that the extracted features, $\beta^\top X$, SDR preserves specific information

  - This is in contrast to other dimension reduction methods, which serve different purposes:
    - supervised feature selection (e.g. LASSO)
    - unsupervised feature extraction (e.g. PCA, tsne)
  
  
---
class: inverse, center, middle

# .bg-text[Our approach to Forward Linear SDR]  

---
class: left, top
# Motivating Example: 

Consider a response $Y$ and predictor $X = (X_1, X_2) \in [0,1]^2$. 
Let $Y=X_1^2$.
Then $Y= (\beta^\top X)^2$, where $\beta = (1,0) \in \R^2$. 
- we want to recover $span(\beta) = \{ (c,0): c \in \R\}$; we take $d=1$ as known.
.center[
<img align="centered" class="image" src="images/sdr_plot.png" width="50%">
]


---
class: left, top  

# Forward Regression for SDR
## Outer Product of Gradients (OPG) `r Cite(bib_sdr, author="Xia", title="adaptive")`

.center[
<img class="image" src="images/fsdr_plot1.png" width="55%">
] 

---
count: false
class: left, top <!-- formatting the slide -->

<!-- the title --> 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) `r Cite(bib_sdr, author="Xia", title="adaptive")`


.center[
<img class="image" src="images/fsdr_plot2.png" width="55%">
] 

---
count: false
class: left, top <!-- formatting the slide -->

<!-- the title --> 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) `r Cite(bib_sdr, author="Xia", title="adaptive")`
 

.center[
<img class="image" src="images/fsdr_plot3.png" width="55%">
] 



---
count: false
class: left, top <!-- formatting the slide -->

<!-- the title --> 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) `r Cite(bib_sdr, author="Xia", title="adaptive")`
 

.center[
<img class="image" src="images/fsdr_plot3-1.png" width="55%">
] 

--
- "Forward" Regression because we are estimating $E(Y|x)$ and $\partial E(Y|x)/\partial x^\top$

---
count: false
class: left, top <!-- formatting the slide -->

<!-- the title --> 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) `r Cite(bib_sdr, author="Xia", title="adaptive")`


.center[
<img class="image" src="images/fsdr_plot4.png" width="55%">
] 

---
count: false
class: left, top
# Forward Regression for SDR
## Outer Product of Gradients (OPG) `r Cite(bib_sdr, author="Xia", title="adaptive")`


.center[
<img class="image" src="images/fsdr_plot5.png" width="55%">
]

---
class: inverse, middle

<!-- inverse makes the background black and text white -->

# .center[.bg-text[Forward SDR for Categorical and Ordinal Response]  ]

.md-text[
  
1. We show ordinal random variables are linear exponential families
  
2. We extend a popular Sufficient Dimension Reduction method to linear exponential families, and provide some theoretical results
  
3. We propose a K-means tuning procedure for our method
  
]


---
class: left, top
# Forward Regression for SDR

.center[
<img class="image" src="images/fsdr_plot3-1.png" width="45%">
<img class="image" src="images/fsdr_plot5.png" width="45%">
]

- OPG applies to uni-variate, continuous $Y$
- Doesn't work as well when $Y$ is categorical or ordinal 
- **We fit a local multivariate GLM**


---
class: left, top
# Why fit a local multivariate GLM?

We are interested in categorical and ordinal $Y$.

--

Categorical variables are linear exponential families (hence GLM) because they are a special case of the multinomial

  - the multivariate Logistic and Expit functions are the canonical and inverse canonical links

--

**Contribution:** We show ordinal variables can be represented explicitly as a linear exponential family by deriving its multivariate canonical link, inverse canonical link and cumulant generating function.

  - the canonical link the multivariate **Adjacent Categories** logistic link `r Cite(bib_sdr, author="agresti", title="ordinal")` 
 

---
class: left, top
# Ordinal Response

Suppose $Y \in \{1,...,m\}$ is an ordinal-categorical variable for $m$ ordered categories.  

  - We can represent $Y$ with a vector $T = (T_1,...T_{m-1}) \in \{0,1\}^{m-1}$;

  - We can interpret the entries of $T$ as $T_j = I\{Y > j\}$ for categories $j=1,\ldots,m-1$ 
  
  <!-- - If $Y = k$, then $T_j = 1$ for $j \leq {k-1}$ and $T_j=0$ for $j > k-1$.  -->

  - Eg. $m=5$; if $Y=3$, then $T=(1,1,0,0)$; if $Y=1$, then $T=(0,0,0,0)$

---
class: left, top
# Ordinal Response

We show the random vector $T$ has log-likelihood:
\begin{align*}
\ell(\theta; T)
= \theta^{\top} T - \log ( 1 + e_{1}^{\top}  P   L \exp (L \theta ) )  
,
\end{align*}
$P$ is a permutation matrix and $L$ is lower triangular matrix of $1$'s. 

 - So $T$ is a linear exponential family (hence GLM). 
 - We say the random vector $T$ has a **ordinal-categorical (Or-Cat)** distribution.

--

The **adjacent-categories (Ad-Cat)** link is the canonical link 
\begin{align*}
\theta(\tau) 
=
\log \left\{ \mathrm{diag}[ (P^{-1} - I)\tau ]^{-1} (P^{-1} - I)\tau \right \},
\end{align*}
where $E(T)=\tau$.

--

The inverse canonical link (i.e. mean function) is
\begin{align*}
\tau(\theta)
=\frac{Q P  L \exp (L \theta ) }{1 + e_1^{\top} P   L \exp (L \theta ) }
,
\end{align*}
where $Q$ is a difference matrix 

---
class: left, top
# Forward Regression for SDR

.center[
<img class="image" src="images/fsdr_plot3-1.png" width="45%">
<img class="image" src="images/fsdr_plot5.png" width="45%">
]

- OPG developed for uni-variate, continuous $Y$
- Doesn't work as well when $Y$ is categorical or ordinal 
- **We fiit a local multivariate GLM**

---
class: left, top
# Forward Regression for SDR 
##Proposal: Fit a local multivariate GLM

Existing work in this direction:

1. Generalized Single Index Model (GSIM): `r Citet(bib_sdr, author=c("Lambert"),title=c("Local"))`
  - Local Linear GLM for uni-variate $Y$; 
  - Goal was to generalize OPG, but method is actually the Average Derivative Estimator (ADE) `r Citet(bib_sdr, author=c("stoker"),title=c("derivatives"))`; 
  - ADE has known drawbacks; e.g. requires gradient has non-zero mean. 

2. Minium Average Deviance Estimation (MADE): `r Citet(bib_sdr, author=c("adragni"),title=c("Minimum"))`
  - Local Linear GLM for uni-variate $Y$ 
    - Generalizes the Minimum Average Variance Estimator (MAVE) of `r Citet(bib_sdr, author=c("xia"),title=c("adaptive"))`  

<!-- Only binary $Y$ can be reduced to uni-variate $Y$. -->
<!-- <br/><br> -->
<!-- Eg. Response with 3 labels needs bi-variate $Y$ representation -->
**Can't handle multi-labels simultaneously.**


---
class: inverse, center, middle

<!-- inverse makes the background black and text white -->

# .bg-text[Outer Product of Canonical Gradients (OPCG)]  

---
class: left, top <!-- formatting the slide -->

<!-- the title --> 
 
# Generalized Forward Regression for SDR

.center[
<img class="image" src="images/fsdr_plot3-1.png" width="40%">
]  

- In OPG, we fit a linear regression about $x_0$, i.e. we minimize
\begin{align*}
\{Y - a_{x_0} + B_{x_0}^\top(X-x_0) \}^\top \{Y - a_{x_0} + B_{x_0}^\top(X-x_0) \} 
\end{align*}
and estimate $\partial E(Y|X=x_0)/ \partial x^\top$ using $\hat B_{x_0}$. 
    <!-- $$\hat B_0 = \frac{ \widehat{ \partial E(Y|x_0)} }{\partial x^\top}.$$ -->

---
class: left, top 
 
# Generalized Forward Regression for SDR

.center[
<img class="image" src="images/fsdr_plot3-1.png" width="40%">
]  

- Instead, fit a multivariate GLM about $x_0$ and minimize
\begin{align*}
  -\ell (a_{x_0}, B_{x_0},x_0)  = - [a_{x_0} + B_{x_0}^\top(x-x_0)]^\top y + b[a_{x_0} + B_{x_0}^\top(x-x_0)] 
  .
\end{align*}
We will use the minimizer $\hat B_{x_0}$. 

---
class: left, top

# Why use $B_{x_0}$ in the GLM?

## Our Dimension Reduction Assumption

- In OPG, the dimension reduction assumption is $\beta$ satisfies $Y \indep E(Y|X)|\beta^\top X$.

  - This is equivalent to $E(Y|X) = E(Y|\beta^\top X)$. 
  
- Our the dimension reduction assumption is $\beta$ satisfies $Y \indep \theta(X)|\beta^\top X$; 

  - $\theta(X)$ is the canonical parameter of the multivariate GLM, through which $X$ relates to $Y$ 
  - Our dimension reduction assumption is equivalent to $\theta(X) = \theta(\beta^\top X)$
      - For linear exponential families, $\theta(X) = link^{-1}(E(Y|X))$, so our dimension reduction assumption is the same as OPG.

---
class: left, top

# Why use $B_{x_0}$ in the GLM?

## Our Dimension Reduction Assumption

  - Because $\theta(X) = \theta(\beta^\top X)$, the gradient of $\theta(x)$ satisfies
  
\begin{align*}
\partial \theta(x)^\top /\partial x = \beta \partial
\theta(u)^\top/\partial u
\end{align*}
  
  - That is, $\mathrm{span}(\partial \theta(x)^\top /\partial x) \subseteq \mathrm{span}(\beta)$
  
      - We say $\partial \theta(x)^\top /\partial x$ is *unbiased* for the central mean subspace.
      
---
class: left, top

# Why use $B_{x_0}$ in the GLM?

## Our Dimension Reduction Assumption

- In OPG, $\hat B_{x_0}$ estimates the gradient of $E(Y|X=x)$ at $x_0$, i.e. $\partial E(Y|X=x_0) /\partial x^\top$.

- Similarly, the $\hat B_{x_0}$ obtained from minimizing
\begin{align*}
-\ell_0(a_0, B_0;y,x,x_0) = -[a_0 + B_0^\top(x-x_0)]^\top y + b[a_0 + B_0^\top(x-x_0)]
\end{align*}

** $\hat B_{x_0}$ estimates the gradient of $\theta(x)$ at $x_0$, i.e. $\partial \theta(x_0) /\partial x^\top$, i.e. the canonical gradient at $x_0$.**

---
class: left, top

# Outer Product of Canonical Gradients (OPCG)

Given a random sample $Y_{1:n}$, $X_{1:n}$, fit a local linear multivariate GLM about $x_0$ by minimizing
\begin{align*}
& -\ell (a_{x_0}, B_{x_0}, x_0, ;X_{1:n}, Y_{1:n}) \\
= &
\frac 1n \sum_{i=1}^n
K \bigg ( \frac{X_i - x_0}{h} \bigg )
\{-[a_{x_0} + B_{x_0}^\top (X_i - x_0)]^\top Y_i + b[a_{x_0} + B_{x_0}^\top (X_i - x_0) ] \}
\end{align*}
where $b(\cdot)$ determines the GLM, and $K(\cdot)$ is a kernel weight with bandwidth $h$. 
The minimizer $\hat B_{x_0}$ is used to estimate $\partial \theta(x_0) /\partial x^\top$.

.center[
<img class="image" src="images/fsdr_plot3-1.png" width="35%"> 
]

---
class: left, top

# Outer Product of Canonical Gradients (OPCG)

We fit a local linear multivariate GLM about each $X_j$, for $j=1,...,n$, by minimizing the full negative local linear log-likelihood:

\begin{align*}
& L(a_1,..,a_n, B_1,...,B_n; X_{1:n}, Y_{1:n})  \\
= & -\frac {1}{n} \sum_{j,i=1}^n
K \bigg ( \frac{X_i - X_j}{h} \bigg ) 
\{[a_{j} + B_{j}^\top (X_i - X_j)]^\top Y_i - 
b(a_{j} + B_{j}^\top (X_i - X_j)) \} 
.
\end{align*}

This provides a collection of minimizers $\hat B_1,\ldots,\hat B_n$ that estimate $\partial \theta(X_j)/\partial x^\top$.

.center[
<img class="image" src="images/fsdr_plot4.png" width="35%"> 
]
---
class: left, top

# The OPCG Estimator

We use $\hat B_1,\ldots, \hat B_n$ to construct the average outer product
$$\hat \Lambda_{\mathrm{opcg}} = \frac 1n \sum_{j=1}^n \hat B_j \hat B_j^\top.$$ 

The **Outer Product of Canonical Gradients (OPCG) Estimator, $\hat \beta_{\mathrm{opcg}}$**, is the $d$ leading eigenvectors of $\hat \Lambda_{\mathrm{opcg}}$.

.center[
<img class="image" src="images/fsdr_plot5.png" width="35%">
]  

---
class: left, top 
# Properties related to OPCG
## Under some regularity assumptions...

  <div class="prop" text="Unbiasedness and Exhaustiveness">
  Let
  \begin{align*}
  \Lambda_{\mathrm{opcg}}
  =
  E 
  \bigg \{ 
  \frac{\partial \theta(X)^\top}{\partial x}
  \frac{\partial \theta(X)}{\partial x^\top}
  \bigg \}
  .
  \end{align*}
  Under SDR and rank assumptions, \(\mathrm{span}( \Lambda_{\mathrm{opcg}} ) = \SS_{E(Y|X)}  \).
  </div>   
--
  <br/><br>
  <div class="theorem" text="Consistency of OPCG">
  Let \(\eta\) be the leading \(d\) eigenvectors of \(\Lambda_{\mathrm{opcg}}\). Then, under some regularity, compactness and bandwidth assumptions, we have
  \begin{align*}
  \| \hat \Lambda_{\mathrm{opcg}} -
  \Lambda_{\mathrm{opcg}} \|_F = O_{a.s}
  ( h + h^{-1} \delta_{ph} + ( \log(n)/n )^{1/2} ),\\
   \Rightarrow \| \hat \beta_{\mathrm{opcg}} - \eta \|_F = O_{a.s}
  ( h + h^{-1} \delta_{ph} + ( \log(n)/n )^{1/2} ),
  \end{align*}
  where \(\delta_{ph} = \sqrt{ \frac{\log n}{ nh^p} }\),
  \(  h \downarrow 0\), and \( h^{-1}\delta_{ph} \to 0\).
  </div>
  
<!-- --- -->
<!-- class: left, top  -->
<!-- # A more direct approach to $\beta$ -->

<!-- - Recall: the gradient is unbiased -->
<!-- \begin{align*} -->
<!-- \partial \theta(x)^\top /\partial x = \beta \partial -->
<!-- \theta(u)^\top/\partial u -->
<!-- \end{align*} -->


<!-- -- -->

<!-- We can incorporate this into the objective function directly by minimizing -->

<!-- \begin{align*} -->
<!-- & L(\beta, a_1,..,a_n, B_1,...,B_n; X_{1:n}, Y_{1:n})  \\ -->
<!-- = & -\frac {1}{n} \sum_{j,i=1}^n -->
<!-- W_{ij}(h) -->
<!-- \{[a_{j} + B_{j}^\top \beta^\top (X_i - X_j)]^\top Y_i -  -->
<!-- b(a_{j} + B_{j}^\top \beta^\top  (X_i - X_j)) \}  -->
<!-- . -->
<!-- \end{align*} -->
<!-- where $W_{ij}(h) = K \bigg ( \frac{X_i - X_j}{h} \bigg )$. -->

<!-- - Alternate between minimizing w.r.t. $\beta$ and $a_1,\ldots,a_n, B_1,\ldots,B_n$ until $\beta$ estimates converge. -->

<!-- - Minimizer of $\beta$ is the **Minimum Average Deviance Estimator (MADE), $\hat \beta_{\mathrm{made}}$**.  -->


<!-- --- -->
<!-- class: left, top  -->
<!-- # MADE -->


<!-- - MADE generalizes the MAVE estimator by `r Citep(bib_sdr, author=c("xia"),title=c("adaptive"))` -->

<!-- - Our MADE estimator, $\hat \beta_{\mathrm{made}}$, is the multivariate version proposed by `r Citet(bib_sdr, author=c("adragni"),title=c("average"))` -->

<!--   - But our MADE can handle multi-label problems simulatenously using the multivariate link function. -->


---
class: left, top 
# Estimating the dimension, $d$ 

  - Ladle plot and Predictor Augmentation methods are fast, eigen-based methods that can be applied to OPCG estimate $d$. `r Citep(bib_sdr, author=c("luo"),title=c("combining", "augmentation"))`
  
    - Uses variation in eigenvectors and eigenvalues of $\hat \Lambda_{\mathrm{opcg}}$ to determine $d$.
  
  <!-- - Cross-validation or sequential testing methods can be used to estimate $d$ for MADE. `r Citep(bib_sdr, author=c("adragni", "xia"),title=c("average", "adaptive"))` -->
 
---
class: inverse, center, middle

<!-- inverse makes the background black and text white -->

# .bg-text[Tuning the bandwidth]

---
class: left, top 
# Tuning the bandwidth, $h$.

.center[
<img class="image" src="images/fsdr_plot3-1.png" width="45%">
]  

The bandwidth $h$ in the kernel $K(h^{-1} \|X_i - X_j \|)$ determines the size of the local neighbourhoods about points $X_j$.

This bandwidth needs to be tuned in our forward regression approaches. 

---
class: left, top
# Tuning the bandwidth, $h$.
 
We need to tune $h$ in OPCG:

  - Cross Validation requires specifying a prediction method beforehand. 

  - Can choose $h$ according to optimal bandwidth, such as $h^{opt} = cn^{-\frac{1}{(p+6)}}$ `r Cite(bib_sdr, author=c("xia"), title="constructive")`
  
    - but suggested values of $c$ does not always work, and then you need to tune $c$.

--

For classification problems, we propose using a K-means clustering procedure for tuning $h$.

Our intuition: 

  - SDR should make classification easier, and classification is easiest when the dimension reduced predictors $\hat \beta^\top X$ are clustered into their respective labels.

---
class: left, top
# Tuning the bandwidth, $h$.

Let $Y \in \{1,...,m\}$ be categorical response and our training set is size $n$. We split the training set into $(Y, X)_{1:n_1}$ for estimation, and $(Y,X)_{1:n_2}^{\mathbb{V}}$ for validation.

--

Main idea: For each $h$, 

1. Estimate $\hat \beta_{\mathrm{opcg}}$ on $(Y, X)_{1:n_1}$ and construct $\hat \beta_{\mathrm{opcg}}^\top X_{1:n_2}^{\mathbb{V}}$.

2. Apply K-means to sufficient predictors $\hat \beta_{\mathrm{opcg}}^\top X_{1:n_2}^{\mathbb{V}}$ for $m$ clusters.

  - This returns $m$ estimated clusters and the F-ratio, i.e. the Within Sums-of-Squares (WSS) over Between Sums-of-Squares (BSS), for each $h$.

3. Select $h$ that minimizes the F-ratio from K-means. 

  - Small F-ratio means small WSS and large BSS

---
class: left, top
# Tuning the bandwidth, $h$.
 
But 
 - Estimating $m$ clusters implicitly assumes we have only 1 cluster per class
 - we ought to incorporate the class/label information from $Y_{1:n_2}^{\mathbb{V}}$, when available.

We modifiy K-means so that:

1. we can estimate more than 1 cluster per class;

2. k-means uses label information from $Y_{1:n_2}^{\mathbb{V}}$; a "supervised" K-means

We apply this supervised k-means on the training set, as before, in a r-fold manner.

---
class: inverse, center, middle

<!-- inverse makes the background black and text white -->

# .bg-text[Simulations and Data Analyses]


---
class: left, top
# Simulations


Our predictor will be $X=(X_1,X_2,X_3,...,X_{10}) \in \R^{10}$.

  - $(X_3, X_7)$ is sampled from one of 5 clusters, generated by a bivariate normal
    - Then augmented with 8 standard normals, so $p=10$.

  - Two clusters are labeled 1, two are labeled 2, and one cluster is labeled 3; So $Y \in \{1,2,3\}$ is categorical.
 
  - We sample 300 for our training set and 150 for testing. We split the training set in half for estimation and validation. All clusters are sampled equally. 
 

---
class: ani-slide
# K-mean Tuning for $h$
 
<iframe src="images/tuning_sc.html" width="100%" height="95%" frameborder="0" ></iframe>



---
count: false
class: left, top
# Simulations - tuning and estimation

5-fold supervised K-means Tuning: $h \approx 1.26$;

<img align="centered" class="image" src="images/sim_dist_table4.png" width="100%">

  - DR is Directional Regression `r Cite(bib_sdr, author=c("li"), title="directional")`  
  
  - PL-method is a per-label approach; Lambert-Lacroix and Peyre's suggestion for multi-label problems.
    - estimates 2 SDR directions per binary logistic problem for each class, for 6 total, and selects the 2 that explain the most variation.
    
  - PW-method is pairwise approach; Adragni's suggestion for multi-label problems.  
    - estimates 2 SDR directions per pair of classes \{1,2\}, \{1,3\}, and \{2,3\}, for 6 total, and selects the 2 that explain the most variation. 

---
class: left, top
# Ordinal-Categorical Data Analysis
## Red Wine Quality

We a wine quality rating data set from the UCI repository: 

  - Red Wine Quality

    - p=11; train/test: 1000/599; Ordinal response - Wine Quality Score;


<img align="centered" class="image" src="images/opcg_wine_test.png" width="100%">


---
class: left, top
# Categorical Data Analysis  

We analyze three datasets with categorical responses: 

  - Handwritten Digits (Pendigit) from UCI
  
    - p=16; train/test:=1000/1000; resp=0-9
    
  - USPS Handwritten Digits  
  
    - p=256; train/test:=1000/1007; resp=0-9
    
  - ISOLET from UCI
  
    - p=618; train/test:=6334/1553; resp=a-z

---
class: left, top
# Categorical Classification Error using SVM

.center[
<img align="center" class="image" src="images/cat_class_table5-1.jpg" width="80%">
]






<!-- --- -->
<!-- class: ani-slide -->
<!-- # Pen Digit 10 - SIR/DR - 1,6,7,9 -->
<!-- <iframe src="images/dr_pendigit4.html" width="90%" height="95%" frameborder="0"></iframe> -->

<!-- --- -->
<!-- count: false -->
<!-- class: ani-slide -->
<!-- # Pen Digit 10 - OPCG - 1,6,7,9 -->

<!-- <iframe src="images/opcg_pendigit4.html" width="90%" height="95%" frameborder="0"></iframe>  -->

---
class: left, middle, inverse
# .bg-text[Summary]

1. Provided the Multivariate Link Functions for Ordinal-Categorical Responses.

3. Generalized OPG to categorical and ordinal responses using multivariate GLMs 

3. Introduced a supervised K-means tuning procedure for classification

---
class: left, top
# Why Sufficient Dimension Reduction? 

  - Sufficient Dimension Reduction is a method for **supervised feature extraction**
  
  - unique in that the extracted features, $\beta^\top X$, SDR preserves specific information

  - This is in contrast to other dimension reduction methods, which serve different purposes:
    - supervised feature selection (e.g. LASSO)
    - unsupervised feature extraction (e.g. PCA, tsne)

---
class: left, top
# Why Sufficient Dimension Reduction?
## USPS - Handwritten Images 


```{r opcg_lasso, fig.show = "hold", out.width = "50%", fig.align = "default", echo=FALSE}

knitr::include_graphics("images/opcg2_fred_hutch.png")

knitr::include_graphics("images/lasso_fred_hutch.png")
```

OPCG: d=9, Error= 9.53%; LASSO: d=165, Error= 10.92%

---
class: left, top
# Why Sufficient Dimension Reduction?
## USPS - Handwritten Images 


```{r opcg_tsne, fig.show = "hold", out.width = "50%", fig.align = "default", echo=FALSE}

knitr::include_graphics("images/opcg_fred_hutch.png")

knitr::include_graphics("images/tsne_fred_hutch.png")
```

OPCG: d=9, Error= 9.53%; t-SNE: d=2, Error= 18.1%

---
class: center, middle, inverse
# .bg-text[Thank you!]

---
layout: false
# References

```{r, echo=FALSE, results="asis"}
PrintBibliography(bib_sdr, start=1, end=5)
```

---
layout: false
# References

```{r, echo=FALSE, results="asis"}
PrintBibliography(bib_sdr, start=6, end=11)
```

---
layout: false
# References

```{r, echo=FALSE, results="asis"}
PrintBibliography(bib_sdr, start=12)
```






