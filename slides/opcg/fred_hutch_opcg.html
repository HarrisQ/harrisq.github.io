<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>On Forward Sufficient Dimension Reduction for Categorical and Ordinal Responses</title>
    <meta charset="utf-8" />
    <meta name="author" content="Harris Quach" />
    <script src="libs/header-attrs-2.11/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="mytheme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: { 
      independenT: ["{\\mathrel{\\rlap{ #1 }\\mkern2mu{ #1 }}}",1],
      indep: "{\\independenT{\\perp}}",
      SS: "{\\mathscr{S}}",
      R:"{\\mathbb{R}}",
      Xcal: "{\\mathcal{X}}",
      water: "{H_2O}",
      braket: ['{\\langle #1 \\rangle}', 1], 
      Abs: ['\\left\\lvert #2 \\right\\rvert_{\\text{#1}}', 2, ""]
    }
  }
});
</script>




 


&lt;!-- class: title-slide --&gt;

# On Forward Sufficient Dimension Reduction for Categorical and Ordinal Responses
&lt;!-- # .bg-text[Generalized Forward Sufficient Dimension Reduction for Classification] --&gt;
&lt;!-- &lt;hr width="700" align="left" /&gt; --&gt;
&lt;hr/&gt;

Harris Quach (joint work with Dr. Bing Li) &lt;br/&gt; Date: "2022/04/04"


---
class: inverse, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text.center[Overview of Talk]  

.md-text[
  
1. A brief introduction to sufficient dimension reduction (SDR)
  
2. Broad idea of the SDR method in the paper
  
3. SDR for categorical and ordinal responses
  
]

---
class: left, top
# What is Sufficient Dimension Reduction? 


Suppose we have a large dataset with some response `\(Y \in \R^m\)` and predictors `\(X \in \R^p\)`.

  - When `\(p\)` is large, lower dimensional summaries of `\(X\)` are helpful for visualization and application of conventional statistical methods

  - Finding a lower dimensional summary of `\(X\)` means finding `\(\beta \in \R^{p \times d}\)`, where `\(d &lt; p\)`, in order to construct the lower dimensional summary `\(\beta^\top X\)`
  
  - Sufficient Dimension Reduction (SDR) are approaches for finding `\(\beta\)` such that `\(\beta^\top X\)` retains all relevant information about `\(Y\)`
  
  - `\(\beta\)` can preserve different information by satisfying different criteria.

---
class: left, top
# Central Subspaces

The information preserved by `\(\beta\)` is characterized by conditional independence:

  - `\(Y \indep X | \beta^\top X\)`; (Li, 2018)
    - `\(\beta\)` preserves all information between `\(Y\)` and `\(X\)` and `\(\mathrm{span}(\beta)\)` is an SDR subspace for `\(Y|X\)`; the smallest such subspace is called the **Central Subspace**. 

  - `\(Y \indep E(Y|X) | \beta^\top X\)`; (Cook and Li, 2002; Ma and Zhu, 2014)
    - `\(\beta\)` preserves all information between `\(Y\)` and `\(E(Y|X)\)` and `\(\mathrm{span}(\beta)\)` is an SDR subspace for `\(E(Y|X)\)`; the smallest such subspace is called the **Central Mean Subspace**. 

--

**NOTE**

We actually want the subspace generated by `\(\beta\)`, i.e. `\(\mathrm{span}(\beta)\)`.
  
  - **When we say we are estimating `\(\beta\)`, we mean estimating a basis for the subspace generated by `\(\beta\)`, i.e a basis for `\(\mathrm{span}(\beta)\)`.**
  
  
---
class: left, top
# Alternative Subspaces

The information preserved by `\(\beta\)` is characterized by conditional independence:

  - `\(Y - E(Y|X) \indep \mathrm{Var}(Y|X) | \beta^\top X\)`; (Zhu and Zhu, 2009)   
    - `\(\beta\)` preserves all information between `\(Y - E(Y|X)\)` and `\(\mathrm{Var}(Y|X)\)`; 
    &lt;!-- the smallest such subspace is called the **Central Variance Subspace** --&gt;
    
  - `\(Y  \indep Q_\tau(X) | \beta^\top X\)`; (Kong and Xia, 2012; Luo, Li, and Yin, 2014)
    - `\(\beta\)` preserves all information between `\(Y\)` and `\(Q_\tau(X)=\inf\{y : P(Y \leq y |X) \geq \tau\}\)`; 
    &lt;!-- the smallest such subspace is called the **Central `\(\tau\)`-th Quantile Subspace**   --&gt;
  
  - `\(Y  \indep \theta(X) | \beta^\top X\)`; (Luo, Li, and Yin, 2014)
    - `\(\beta\)` preserves all information between `\(Y\)` and `\(\theta(X)\)`, where `\(\theta(X)\)` is defined through a conditional statistical functional; 
    
  - `\(N(B_1),\ldots, N(B_k)  \indep X(B_1),\ldots,X(B_k) | \beta^\top X(B_1),\ldots,\beta^\top X(B_k)\)`; (Guan and Wang, 2010)
    - `\(\beta\)` preserves all information between the counting process `\(N\)` and the Gaussian random field `\(X\)` over sets `\(B_1,\ldots,B_k\)`;  

---
class: left, top
# Why Sufficient Dimension Reduction? 

&lt;blockquote&gt;
"...I’ve never been comfortable with the way in which
our discipline has embraced sparsity to the exclusion of
everything else. I mentioned before that I like the availability
of options in statistics, but for a long time now the
prevailing attitude toward high-dimensional problems has
been that its natural to assume sparsity. I find no sense in
which it’s natural. That overwhelming emphasis on sparsity
has, I think, kept us from seeing other solutions and
options. To be clear, I have nothing against sparsity per se,
and I think it’s a reasonable modeling construct. But it’s
not reasonable just because you have high-dimensional
data."  
.right[~ &lt;cite&gt;*Dennis Cook* (2021)&lt;/cite&gt;]
.right[(Bura, Li, Li, Nachtsheim, Pena, Setodji, and Weiss, 2021)]
&lt;/blockquote&gt;
  
---
class: left, top
# Why Sufficient Dimension Reduction? 

  - Sufficient Dimension Reduction is a method for **supervised feature extraction**
  
  - unique in that the extracted features, `\(\beta^\top X\)`, SDR preserves specific information

  - This is in contrast to other dimension reduction methods, which serve different purposes:
    - supervised feature selection (e.g. LASSO)
    - unsupervised feature extraction (e.g. PCA, tsne)
  
  
---
class: inverse, center, middle

# .bg-text[Our approach to Forward Linear SDR]  

---
class: left, top
# Motivating Example: 

Consider a response `\(Y\)` and predictor `\(X = (X_1, X_2) \in [0,1]^2\)`. 
Let `\(Y=X_1^2\)`.
Then `\(Y= (\beta^\top X)^2\)`, where `\(\beta = (1,0) \in \R^2\)`. 
- we want to recover `\(span(\beta) = \{ (c,0): c \in \R\}\)`; we take `\(d=1\)` as known.
.center[
&lt;img align="centered" class="image" src="images/sdr_plot.png" width="50%"&gt;
]


---
class: left, top  

# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, and Zhu, 2002)

.center[
&lt;img class="image" src="images/fsdr_plot1.png" width="55%"&gt;
] 

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)


.center[
&lt;img class="image" src="images/fsdr_plot2.png" width="55%"&gt;
] 

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)
 

.center[
&lt;img class="image" src="images/fsdr_plot3.png" width="55%"&gt;
] 



---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)
 

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="55%"&gt;
] 

--
- "Forward" Regression because we are estimating `\(E(Y|x)\)` and `\(\partial E(Y|x)/\partial x^\top\)`

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)


.center[
&lt;img class="image" src="images/fsdr_plot4.png" width="55%"&gt;
] 

---
count: false
class: left, top
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)


.center[
&lt;img class="image" src="images/fsdr_plot5.png" width="55%"&gt;
]

---
class: inverse, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .center[.bg-text[Forward SDR for Categorical and Ordinal Response]  ]

.md-text[
  
1. We show ordinal random variables are linear exponential families
  
2. We extend a popular Sufficient Dimension Reduction method to linear exponential families, and provide some theoretical results
  
3. We propose a K-means tuning procedure for our method
  
]


---
class: left, top
# Forward Regression for SDR

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="45%"&gt;
&lt;img class="image" src="images/fsdr_plot5.png" width="45%"&gt;
]

- OPG applies to uni-variate, continuous `\(Y\)`
- Doesn't work as well when `\(Y\)` is categorical or ordinal 
- **We fit a local multivariate GLM**


---
class: left, top
# Why fit a local multivariate GLM?

We are interested in categorical and ordinal `\(Y\)`.

--

Categorical variables are linear exponential families (hence GLM) because they are a special case of the multinomial

  - the multivariate Logistic and Expit functions are the canonical and inverse canonical links

--

**Contribution:** We show ordinal variables can be represented explicitly as a linear exponential family by deriving its multivariate canonical link, inverse canonical link and cumulant generating function.

  - the canonical link the multivariate **Adjacent Categories** logistic link (Agresti, 2010) 
 

---
class: left, top
# Ordinal Response

Suppose `\(Y \in \{1,...,m\}\)` is an ordinal-categorical variable for `\(m\)` ordered categories.  

  - We can represent `\(Y\)` with a vector `\(T = (T_1,...T_{m-1}) \in \{0,1\}^{m-1}\)`;

  - We can interpret the entries of `\(T\)` as `\(T_j = I\{Y &gt; j\}\)` for categories `\(j=1,\ldots,m-1\)` 
  
  &lt;!-- - If `\(Y = k\)`, then `\(T_j = 1\)` for `\(j \leq {k-1}\)` and `\(T_j=0\)` for `\(j &gt; k-1\)`.  --&gt;

  - Eg. `\(m=5\)`; if `\(Y=3\)`, then `\(T=(1,1,0,0)\)`; if `\(Y=1\)`, then `\(T=(0,0,0,0)\)`

---
class: left, top
# Ordinal Response

We show the random vector `\(T\)` has log-likelihood:
`\begin{align*}
\ell(\theta; T)
= \theta^{\top} T - \log ( 1 + e_{1}^{\top}  P   L \exp (L \theta ) )  
,
\end{align*}`
`\(P\)` is a permutation matrix and `\(L\)` is lower triangular matrix of `\(1\)`'s. 

 - So `\(T\)` is a linear exponential family (hence GLM). 
 - We say the random vector `\(T\)` has a **ordinal-categorical (Or-Cat)** distribution.

--

The **adjacent-categories (Ad-Cat)** link is the canonical link 
`\begin{align*}
\theta(\tau) 
=
\log \left\{ \mathrm{diag}[ (P^{-1} - I)\tau ]^{-1} (P^{-1} - I)\tau \right \},
\end{align*}`
where `\(E(T)=\tau\)`.

--

The inverse canonical link (i.e. mean function) is
`\begin{align*}
\tau(\theta)
=\frac{Q P  L \exp (L \theta ) }{1 + e_1^{\top} P   L \exp (L \theta ) }
,
\end{align*}`
where `\(Q\)` is a difference matrix 

---
class: left, top
# Forward Regression for SDR

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="45%"&gt;
&lt;img class="image" src="images/fsdr_plot5.png" width="45%"&gt;
]

- OPG developed for uni-variate, continuous `\(Y\)`
- Doesn't work as well when `\(Y\)` is categorical or ordinal 
- **We fiit a local multivariate GLM**

---
class: left, top
# Forward Regression for SDR 
##Proposal: Fit a local multivariate GLM

Existing work in this direction:

1. Generalized Single Index Model (GSIM): Lambert-Lacroix and Peyre (2006)
  - Local Linear GLM for uni-variate `\(Y\)`; 
  - Goal was to generalize OPG, but method is actually the Average Derivative Estimator (ADE) Härdle and Stoker (1989); 
  - ADE has known drawbacks; e.g. requires gradient has non-zero mean. 

2. Minium Average Deviance Estimation (MADE): Adragni (2018)
  - Local Linear GLM for uni-variate `\(Y\)` 
    - Generalizes the Minimum Average Variance Estimator (MAVE) of Xia, Tong, Li, et al. (2002)  

&lt;!-- Only binary `\(Y\)` can be reduced to uni-variate `\(Y\)`. --&gt;
&lt;!-- &lt;br/&gt;&lt;br&gt; --&gt;
&lt;!-- Eg. Response with 3 labels needs bi-variate `\(Y\)` representation --&gt;
**Can't handle multi-labels simultaneously.**


---
class: inverse, center, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text[Outer Product of Canonical Gradients (OPCG)]  

---
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Generalized Forward Regression for SDR

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="40%"&gt;
]  

- In OPG, we fit a linear regression about `\(x_0\)`, i.e. we minimize
`\begin{align*}
\{Y - a_{x_0} + B_{x_0}^\top(X-x_0) \}^\top \{Y - a_{x_0} + B_{x_0}^\top(X-x_0) \} 
\end{align*}`
and estimate `\(\partial E(Y|X=x_0)/ \partial x^\top\)` using `\(\hat B_{x_0}\)`. 
    &lt;!-- `$$\hat B_0 = \frac{ \widehat{ \partial E(Y|x_0)} }{\partial x^\top}.$$` --&gt;

---
class: left, top 
 
# Generalized Forward Regression for SDR

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="40%"&gt;
]  

- Instead, fit a multivariate GLM about `\(x_0\)` and minimize
`\begin{align*}
  -\ell (a_{x_0}, B_{x_0},x_0)  = - [a_{x_0} + B_{x_0}^\top(x-x_0)]^\top y + b[a_{x_0} + B_{x_0}^\top(x-x_0)] 
  .
\end{align*}`
We will use the minimizer `\(\hat B_{x_0}\)`. 

---
class: left, top

# Why use `\(B_{x_0}\)` in the GLM?

## Our Dimension Reduction Assumption

- In OPG, the dimension reduction assumption is `\(\beta\)` satisfies `\(Y \indep E(Y|X)|\beta^\top X\)`.

  - This is equivalent to `\(E(Y|X) = E(Y|\beta^\top X)\)`. 
  
- Our the dimension reduction assumption is `\(\beta\)` satisfies `\(Y \indep \theta(X)|\beta^\top X\)`; 

  - `\(\theta(X)\)` is the canonical parameter of the multivariate GLM, through which `\(X\)` relates to `\(Y\)` 
  - Our dimension reduction assumption is equivalent to `\(\theta(X) = \theta(\beta^\top X)\)`
      - For linear exponential families, `\(\theta(X) = link^{-1}(E(Y|X))\)`, so our dimension reduction assumption is the same as OPG.

---
class: left, top

# Why use `\(B_{x_0}\)` in the GLM?

## Our Dimension Reduction Assumption

  - Because `\(\theta(X) = \theta(\beta^\top X)\)`, the gradient of `\(\theta(x)\)` satisfies
  
`\begin{align*}
\partial \theta(x)^\top /\partial x = \beta \partial
\theta(u)^\top/\partial u
\end{align*}`
  
  - That is, `\(\mathrm{span}(\partial \theta(x)^\top /\partial x) \subseteq \mathrm{span}(\beta)\)`
  
      - We say `\(\partial \theta(x)^\top /\partial x\)` is *unbiased* for the central mean subspace.
      
---
class: left, top

# Why use `\(B_{x_0}\)` in the GLM?

## Our Dimension Reduction Assumption

- In OPG, `\(\hat B_{x_0}\)` estimates the gradient of `\(E(Y|X=x)\)` at `\(x_0\)`, i.e. `\(\partial E(Y|X=x_0) /\partial x^\top\)`.

- Similarly, the `\(\hat B_{x_0}\)` obtained from minimizing
`\begin{align*}
-\ell_0(a_0, B_0;y,x,x_0) = -[a_0 + B_0^\top(x-x_0)]^\top y + b[a_0 + B_0^\top(x-x_0)]
\end{align*}`

** `\(\hat B_{x_0}\)` estimates the gradient of `\(\theta(x)\)` at `\(x_0\)`, i.e. `\(\partial \theta(x_0) /\partial x^\top\)`, i.e. the canonical gradient at `\(x_0\)`.**

---
class: left, top

# Outer Product of Canonical Gradients (OPCG)

Given a random sample `\(Y_{1:n}\)`, `\(X_{1:n}\)`, fit a local linear multivariate GLM about `\(x_0\)` by minimizing
`\begin{align*}
&amp; -\ell (a_{x_0}, B_{x_0}, x_0, ;X_{1:n}, Y_{1:n}) \\
= &amp;
\frac 1n \sum_{i=1}^n
K \bigg ( \frac{X_i - x_0}{h} \bigg )
\{-[a_{x_0} + B_{x_0}^\top (X_i - x_0)]^\top Y_i + b[a_{x_0} + B_{x_0}^\top (X_i - x_0) ] \}
\end{align*}`
where `\(b(\cdot)\)` determines the GLM, and `\(K(\cdot)\)` is a kernel weight with bandwidth `\(h\)`. 
The minimizer `\(\hat B_{x_0}\)` is used to estimate `\(\partial \theta(x_0) /\partial x^\top\)`.

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="35%"&gt; 
]

---
class: left, top

# Outer Product of Canonical Gradients (OPCG)

We fit a local linear multivariate GLM about each `\(X_j\)`, for `\(j=1,...,n\)`, by minimizing the full negative local linear log-likelihood:

`\begin{align*}
&amp; L(a_1,..,a_n, B_1,...,B_n; X_{1:n}, Y_{1:n})  \\
= &amp; -\frac {1}{n} \sum_{j,i=1}^n
K \bigg ( \frac{X_i - X_j}{h} \bigg ) 
\{[a_{j} + B_{j}^\top (X_i - X_j)]^\top Y_i - 
b(a_{j} + B_{j}^\top (X_i - X_j)) \} 
.
\end{align*}`

This provides a collection of minimizers `\(\hat B_1,\ldots,\hat B_n\)` that estimate `\(\partial \theta(X_j)/\partial x^\top\)`.

.center[
&lt;img class="image" src="images/fsdr_plot4.png" width="35%"&gt; 
]
---
class: left, top

# The OPCG Estimator

We use `\(\hat B_1,\ldots, \hat B_n\)` to construct the average outer product
`$$\hat \Lambda_{\mathrm{opcg}} = \frac 1n \sum_{j=1}^n \hat B_j \hat B_j^\top.$$` 

The **Outer Product of Canonical Gradients (OPCG) Estimator, `\(\hat \beta_{\mathrm{opcg}}\)`**, is the `\(d\)` leading eigenvectors of `\(\hat \Lambda_{\mathrm{opcg}}\)`.

.center[
&lt;img class="image" src="images/fsdr_plot5.png" width="35%"&gt;
]  

---
class: left, top 
# Properties related to OPCG
## Under some regularity assumptions...

  &lt;div class="prop" text="Unbiasedness and Exhaustiveness"&gt;
  Let
  \begin{align*}
  \Lambda_{\mathrm{opcg}}
  =
  E 
  \bigg \{ 
  \frac{\partial \theta(X)^\top}{\partial x}
  \frac{\partial \theta(X)}{\partial x^\top}
  \bigg \}
  .
  \end{align*}
  Under SDR and rank assumptions, \(\mathrm{span}( \Lambda_{\mathrm{opcg}} ) = \SS_{E(Y|X)}  \).
  &lt;/div&gt;   
--
  &lt;br/&gt;&lt;br&gt;
  &lt;div class="theorem" text="Consistency of OPCG"&gt;
  Let \(\eta\) be the leading \(d\) eigenvectors of \(\Lambda_{\mathrm{opcg}}\). Then, under some regularity, compactness and bandwidth assumptions, we have
  \begin{align*}
  \| \hat \Lambda_{\mathrm{opcg}} -
  \Lambda_{\mathrm{opcg}} \|_F = O_{a.s}
  ( h + h^{-1} \delta_{ph} + ( \log(n)/n )^{1/2} ),\\
   \Rightarrow \| \hat \beta_{\mathrm{opcg}} - \eta \|_F = O_{a.s}
  ( h + h^{-1} \delta_{ph} + ( \log(n)/n )^{1/2} ),
  \end{align*}
  where \(\delta_{ph} = \sqrt{ \frac{\log n}{ nh^p} }\),
  \(  h \downarrow 0\), and \( h^{-1}\delta_{ph} \to 0\).
  &lt;/div&gt;
  
&lt;!-- --- --&gt;
&lt;!-- class: left, top  --&gt;
&lt;!-- # A more direct approach to `\(\beta\)` --&gt;

&lt;!-- - Recall: the gradient is unbiased --&gt;
&lt;!-- \begin{align*} --&gt;
&lt;!-- \partial \theta(x)^\top /\partial x = \beta \partial --&gt;
&lt;!-- \theta(u)^\top/\partial u --&gt;
&lt;!-- \end{align*} --&gt;


&lt;!-- -- --&gt;

&lt;!-- We can incorporate this into the objective function directly by minimizing --&gt;

&lt;!-- \begin{align*} --&gt;
&lt;!-- &amp; L(\beta, a_1,..,a_n, B_1,...,B_n; X_{1:n}, Y_{1:n})  \\ --&gt;
&lt;!-- = &amp; -\frac {1}{n} \sum_{j,i=1}^n --&gt;
&lt;!-- W_{ij}(h) --&gt;
&lt;!-- \{[a_{j} + B_{j}^\top \beta^\top (X_i - X_j)]^\top Y_i -  --&gt;
&lt;!-- b(a_{j} + B_{j}^\top \beta^\top  (X_i - X_j)) \}  --&gt;
&lt;!-- . --&gt;
&lt;!-- \end{align*} --&gt;
&lt;!-- where `\(W_{ij}(h) = K \bigg ( \frac{X_i - X_j}{h} \bigg )\)`. --&gt;

&lt;!-- - Alternate between minimizing w.r.t. `\(\beta\)` and `\(a_1,\ldots,a_n, B_1,\ldots,B_n\)` until `\(\beta\)` estimates converge. --&gt;

&lt;!-- - Minimizer of `\(\beta\)` is the **Minimum Average Deviance Estimator (MADE), `\(\hat \beta_{\mathrm{made}}\)`**.  --&gt;


&lt;!-- --- --&gt;
&lt;!-- class: left, top  --&gt;
&lt;!-- # MADE --&gt;


&lt;!-- - MADE generalizes the MAVE estimator by (Xia, Tong, Li, et al., 2002) --&gt;

&lt;!-- - Our MADE estimator, `\(\hat \beta_{\mathrm{made}}\)`, is the multivariate version proposed by Adragni (2018) --&gt;

&lt;!--   - But our MADE can handle multi-label problems simulatenously using the multivariate link function. --&gt;


---
class: left, top 
# Estimating the dimension, `\(d\)` 

  - Ladle plot and Predictor Augmentation methods are fast, eigen-based methods that can be applied to OPCG estimate `\(d\)`. (Luo and Li, 2020; Luo and Li, 2016)
  
    - Uses variation in eigenvectors and eigenvalues of `\(\hat \Lambda_{\mathrm{opcg}}\)` to determine `\(d\)`.
  
  &lt;!-- - Cross-validation or sequential testing methods can be used to estimate `\(d\)` for MADE. (Adragni, 2018; Xia, Tong, Li, et al., 2002) --&gt;
 
---
class: inverse, center, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text[Tuning the bandwidth]

---
class: left, top 
# Tuning the bandwidth, `\(h\)`.

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="45%"&gt;
]  

The bandwidth `\(h\)` in the kernel `\(K(h^{-1} \|X_i - X_j \|)\)` determines the size of the local neighbourhoods about points `\(X_j\)`.

This bandwidth needs to be tuned in our forward regression approaches. 

---
class: left, top
# Tuning the bandwidth, `\(h\)`.
 
We need to tune `\(h\)` in OPCG:

  - Cross Validation requires specifying a prediction method beforehand. 

  - Can choose `\(h\)` according to optimal bandwidth, such as `\(h^{opt} = cn^{-\frac{1}{(p+6)}}\)` (Xia, 2007)
  
    - but suggested values of `\(c\)` does not always work, and then you need to tune `\(c\)`.

--

For classification problems, we propose using a K-means clustering procedure for tuning `\(h\)`.

Our intuition: 

  - SDR should make classification easier, and classification is easiest when the dimension reduced predictors `\(\hat \beta^\top X\)` are clustered into their respective labels.

---
class: left, top
# Tuning the bandwidth, `\(h\)`.

Let `\(Y \in \{1,...,m\}\)` be categorical response and our training set is size `\(n\)`. We split the training set into `\((Y, X)_{1:n_1}\)` for estimation, and `\((Y,X)_{1:n_2}^{\mathbb{V}}\)` for validation.

--

Main idea: For each `\(h\)`, 

1. Estimate `\(\hat \beta_{\mathrm{opcg}}\)` on `\((Y, X)_{1:n_1}\)` and construct `\(\hat \beta_{\mathrm{opcg}}^\top X_{1:n_2}^{\mathbb{V}}\)`.

2. Apply K-means to sufficient predictors `\(\hat \beta_{\mathrm{opcg}}^\top X_{1:n_2}^{\mathbb{V}}\)` for `\(m\)` clusters.

  - This returns `\(m\)` estimated clusters and the F-ratio, i.e. the Within Sums-of-Squares (WSS) over Between Sums-of-Squares (BSS), for each `\(h\)`.

3. Select `\(h\)` that minimizes the F-ratio from K-means. 

  - Small F-ratio means small WSS and large BSS

---
class: left, top
# Tuning the bandwidth, `\(h\)`.
 
But 
 - Estimating `\(m\)` clusters implicitly assumes we have only 1 cluster per class
 - we ought to incorporate the class/label information from `\(Y_{1:n_2}^{\mathbb{V}}\)`, when available.

We modifiy K-means so that:

1. we can estimate more than 1 cluster per class;

2. k-means uses label information from `\(Y_{1:n_2}^{\mathbb{V}}\)`; a "supervised" K-means

We apply this supervised k-means on the training set, as before, in a r-fold manner.

---
class: inverse, center, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text[Simulations and Data Analyses]


---
class: left, top
# Simulations


Our predictor will be `\(X=(X_1,X_2,X_3,...,X_{10}) \in \R^{10}\)`.

  - `\((X_3, X_7)\)` is sampled from one of 5 clusters, generated by a bivariate normal
    - Then augmented with 8 standard normals, so `\(p=10\)`.

  - Two clusters are labeled 1, two are labeled 2, and one cluster is labeled 3; So `\(Y \in \{1,2,3\}\)` is categorical.
 
  - We sample 300 for our training set and 150 for testing. We split the training set in half for estimation and validation. All clusters are sampled equally. 
 

---
class: ani-slide
# K-mean Tuning for `\(h\)`
 
&lt;iframe src="images/tuning_sc.html" width="100%" height="95%" frameborder="0" &gt;&lt;/iframe&gt;



---
count: false
class: left, top
# Simulations - tuning and estimation

5-fold supervised K-means Tuning: `\(h \approx 1.26\)`;

&lt;img align="centered" class="image" src="images/sim_dist_table4.png" width="100%"&gt;

  - DR is Directional Regression (Li and Wang, 2007)  
  
  - PL-method is a per-label approach; Lambert-Lacroix and Peyre's suggestion for multi-label problems.
    - estimates 2 SDR directions per binary logistic problem for each class, for 6 total, and selects the 2 that explain the most variation.
    
  - PW-method is pairwise approach; Adragni's suggestion for multi-label problems.  
    - estimates 2 SDR directions per pair of classes \{1,2\}, \{1,3\}, and \{2,3\}, for 6 total, and selects the 2 that explain the most variation. 

---
class: left, top
# Ordinal-Categorical Data Analysis
## Red Wine Quality

We a wine quality rating data set from the UCI repository: 

  - Red Wine Quality

    - p=11; train/test: 1000/599; Ordinal response - Wine Quality Score;


&lt;img align="centered" class="image" src="images/opcg_wine_test.png" width="100%"&gt;


---
class: left, top
# Categorical Data Analysis  

We analyze three datasets with categorical responses: 

  - Handwritten Digits (Pendigit) from UCI
  
    - p=16; train/test:=1000/1000; resp=0-9
    
  - USPS Handwritten Digits  
  
    - p=256; train/test:=1000/1007; resp=0-9
    
  - ISOLET from UCI
  
    - p=618; train/test:=6334/1553; resp=a-z

---
class: left, top
# Categorical Classification Error using SVM

.center[
&lt;img align="center" class="image" src="images/cat_class_table5-1.jpg" width="80%"&gt;
]






&lt;!-- --- --&gt;
&lt;!-- class: ani-slide --&gt;
&lt;!-- # Pen Digit 10 - SIR/DR - 1,6,7,9 --&gt;
&lt;!-- &lt;iframe src="images/dr_pendigit4.html" width="90%" height="95%" frameborder="0"&gt;&lt;/iframe&gt; --&gt;

&lt;!-- --- --&gt;
&lt;!-- count: false --&gt;
&lt;!-- class: ani-slide --&gt;
&lt;!-- # Pen Digit 10 - OPCG - 1,6,7,9 --&gt;

&lt;!-- &lt;iframe src="images/opcg_pendigit4.html" width="90%" height="95%" frameborder="0"&gt;&lt;/iframe&gt;  --&gt;

---
class: left, middle, inverse
# .bg-text[Summary]

1. Provided the Multivariate Link Functions for Ordinal-Categorical Responses.

3. Generalized OPG to categorical and ordinal responses using multivariate GLMs 

3. Introduced a supervised K-means tuning procedure for classification

---
class: left, top
# Why Sufficient Dimension Reduction? 

  - Sufficient Dimension Reduction is a method for **supervised feature extraction**
  
  - unique in that the extracted features, `\(\beta^\top X\)`, SDR preserves specific information

  - This is in contrast to other dimension reduction methods, which serve different purposes:
    - supervised feature selection (e.g. LASSO)
    - unsupervised feature extraction (e.g. PCA, tsne)

---
class: left, top
# Why Sufficient Dimension Reduction?
## USPS - Handwritten Images 


&lt;img src="images/opcg2_fred_hutch.png" width="50%" /&gt;&lt;img src="images/lasso_fred_hutch.png" width="50%" /&gt;

OPCG: d=9, Error= 9.53%; LASSO: d=165, Error= 10.92%

---
class: left, top
# Why Sufficient Dimension Reduction?
## USPS - Handwritten Images 


&lt;img src="images/opcg_fred_hutch.png" width="50%" /&gt;&lt;img src="images/tsne_fred_hutch.png" width="50%" /&gt;

OPCG: d=9, Error= 9.53%; t-SNE: d=2, Error= 18.1%

---
class: center, middle, inverse
# .bg-text[Thank you!]

---
layout: false
# References

Adragni, K. P. (2018). "Minimum average deviance estimation for
sufficient dimension reduction". In: _Journal of Statistical
Computation and Simulation_ 88.3, pp. 411-431.

Agresti, A. (2010). _Analysis of ordinal categorical data_. Vol. 656.
John Wiley &amp; Sons.

Bura, E., B. Li, L. Li, et al. (2021). "A Conversation with Dennis
Cook". In: _Statistical Science_ 36.2, pp. 328 - 337. DOI:
[10.1214/20-STS801](https://doi.org/10.1214%2F20-STS801). URL:
[https://doi.org/10.1214/20-STS801](https://doi.org/10.1214/20-STS801).

Cook, R. D. and B. Li (2002). "Dimension reduction for conditional mean
in regression". In: _The Annals of Statistics_ 30.2, pp. 455-474.

Guan, Y. and H. Wang (2010). "Sufficient dimension reduction for
spatial point processes directed by Gaussian random fields". In:
_Journal of the Royal Statistical Society: Series B (Statistical
Methodology)_ 72.3, pp. 367-387.

---
layout: false
# References

Härdle, W. and T. M. Stoker (1989). "Investigating smooth multiple
regression by the method of average derivatives". In: _Journal of the
American statistical Association_ 84.408, pp. 986-995.

Kong, E. and Y. Xia (2012). "A single-index quantile regression model
and its estimation". In: _Econometric Theory_ 28.4, pp. 730-768.

Lambert-Lacroix, S. and J. Peyre (2006). "Local likelihood regression
in generalized linear single-index models with applications to
microarray data". In: _Computational statistics &amp; data analysis_ 51.3,
pp. 2091-2113.

Li, B. (2018). _Sufficient Dimension Reduction: Methods and
Applications with R_. CRC Press.

Li, B. and S. Wang (2007). "On directional regression for dimension
reduction". In: _Journal of the American Statistical Association_
102.479, pp. 997-1008.

Luo, W. and B. Li (2020). "On order determination by predictor
augmentation". In: _Biometrika_.

---
layout: false
# References

Luo, W. and B. Li (2016). "Combining eigenvalues and variation of
eigenvectors for order determination". In: _Biometrika_ 103.4, pp.
875-887.

Luo, W., B. Li, and X. Yin (2014). "On efficient dimension reduction
with respect to a statistical functional of interest". In: _The annals
of statistics_ 42.1, pp. 382-412.

Ma, Y. and L. Zhu (2014). "On estimation efficiency of the central mean
subspace". In: _Journal of the Royal Statistical Society: Series B:
Statistical Methodology_, pp. 885-901.

Xia, Y. (2007). "A constructive approach to the estimation of dimension
reduction directions". In: _The Annals of Statistics_ 35.6, pp.
2654-2690.

Xia, Y., H. Tong, W. Li, et al. (2002). "An adaptive estimation of
dimension reduction space". In: _Journal of the Royal Statistical
Society: Series B (Statistical Methodology)_ 64.3, pp. 363-410.

Zhu, L. and L. Zhu (2009). "Dimension reduction for conditional
variance in regressions". In: _Statistica Sinica_, pp. 869-883.






    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
