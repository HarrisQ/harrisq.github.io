<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Forward Sufficient Dimension Reduction for Categorical and Ordinal Responses</title>
    <meta charset="utf-8" />
    <meta name="author" content="Harris Quach" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="mytheme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: { 
      independenT: ["{\\mathrel{\\rlap{ #1 }\\mkern2mu{ #1 }}}",1],
      indep: "{\\independenT{\\perp}}",
      SS: "{\\mathscr{S}}",
      R:"{\\mathbb{R}}",
      Xcal: "{\\mathcal{X}}",
      water: "{H_2O}",
      braket: ['{\\langle #1 \\rangle}', 1], 
      Abs: ['\\left\\lvert #2 \\right\\rvert_{\\text{#1}}', 2, ""]
    }
  }
});
</script>




 

&lt;!-- class: title-slide --&gt;

# Forward Sufficient Dimension Reduction for Categorical and Ordinal Responses
&lt;!-- # .bg-text[Generalized Forward Sufficient Dimension Reduction for Classification] --&gt;
&lt;!-- &lt;hr width="700" align="left" /&gt; --&gt;
&lt;hr/&gt;

Harris Quach (joint work with Dr. Bing Li) &lt;br/&gt; Date: "2021/06/10 (updated: 2021-06-10)"

---
class: inverse, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text.center[Overview]  

.md-text[
  
1. We show ordinal variables are linear exponential families
  
2. We generalize a popular SDR method to linear exponential families
  
3. We proposed a K-means tuning procedure
  
]
 

&lt;!-- --- --&gt;
&lt;!-- class: left, top --&gt;
&lt;!-- # Multivariate Links for Linear Exponential Families --&gt;

&lt;!-- The crux of our proposed method is fitting a multivariate Generalized Linear Model to categorical or ordinal response variables, `\(Y\)`, which are linear exponential families. --&gt;

&lt;!-- - Linear exponential families have log-likelihood of the form --&gt;

&lt;!-- \begin{align*} --&gt;
&lt;!-- \ell(\theta;y) = \theta^\top y - b(\theta);    --&gt;
&lt;!-- \end{align*} --&gt;

&lt;!-- - They are characterized by their means `\(\mu\)`, through the canonical parameter `\(\theta\)`, using  --&gt;

&lt;!--   - the canonical link function `\(\theta( \cdot ): \mu \mapsto \theta\)`;  --&gt;
&lt;!--   - the inverse canonical link `\(\mu(\cdot): \theta \mapsto \mu\)`;  --&gt;
&lt;!--   - the cumulant generating function `\(b(\cdot)\)`. --&gt;

&lt;!-- --- --&gt;
&lt;!-- class: left, top --&gt;
&lt;!-- # Categorical Response --&gt;

&lt;!-- Suppose `\(Y \in \{1,...,m\}\)` is a categorical variable for `\(m\)` nominal categories.  --&gt;

&lt;!-- - We can represent `\(Y\)` as a vector `\(S = (S^1,...,S^{m-1}) \in \{0,1\}^{m-1}\)` --&gt;

&lt;!--   - we set `\(S^m = 1 - \sum_{j=1}^{m-1} S^m\)`. --&gt;

&lt;!--   - If `\(Y=k\)`, then `\(S^k = 1\)` and `\(S^j=0\)` for `\(j \neq k\)`.  --&gt;

&lt;!--   - Eg. `\(m=3\)`; if `\(Y = 2\)`, then `\(S = (0,1)\)`; if `\(Y=3\)`, then `\(S=(0,0)\)`.  --&gt;



&lt;!-- --- --&gt;
&lt;!-- count: false --&gt;
&lt;!-- class: left, top --&gt;
&lt;!-- # Categorical Response --&gt;

&lt;!-- Suppose `\(Y \in \{1,...,m\}\)` is a categorical variable for `\(m\)` nominal categories.  --&gt;

&lt;!--   - We can represent `\(Y\)` as a vector `\(S = (S^1,...,S^{m-1}) \in \{0,1\}^{m-1}\)` --&gt;

&lt;!--   - Let `\(p = (p^1,...p^{m-1})\)` be the `\(m-1\)` probabilities for each category.  --&gt;
&lt;!--   The canonical link of and its inverse is are the multivariate logit and expit functions: --&gt;

&lt;!-- \begin{align*} --&gt;
&lt;!-- \theta(p) = \log \frac{ p }{ 1 - \boldsymbol{1}^{\top} p } --&gt;
&lt;!-- \qquad --&gt;
&lt;!-- p(\theta) =  \frac { \exp(\theta) } {  1 - \boldsymbol{1}^{\top} \exp( \theta) }  --&gt;
&lt;!-- \end{align*} --&gt;

&lt;!-- The multinomial log-likelihood of `\(S\)` is  --&gt;
&lt;!-- \begin{align*} --&gt;
&lt;!-- \ell(\theta;S) = \theta^{\top}S - \log ( 1 + \boldsymbol 1^{\top}  e^\theta   ) --&gt;
&lt;!-- , --&gt;
&lt;!-- \end{align*} --&gt;
&lt;!-- where  --&gt;
&lt;!-- `\(b(\theta) = \log( 1 + \boldsymbol{1}^{\top} e^{  \theta }  )\)`. --&gt;


&lt;!-- --- --&gt;
&lt;!-- class: left, top --&gt;
&lt;!-- # Ordinal Response --&gt;

&lt;!-- Suppose `\(Y \in \{1,...,m\}\)` is an ordinal-categorical variable for `\(m\)` ordered categories.   --&gt;

&lt;!-- - We can represent `\(Y\)` as a vector `\(S = (S^1,...S^{m-1}) \in \{0,1\}^{m-1}\)` as for categorical `\(Y\)`.  --&gt;

&lt;!-- - We can represent `\(S\)` as a vector `\(T = (T^1,...T^{m-1}) \in \{0,1\}^{m-1}\)`, and we set `\(T^{m} = 0\)` and `\(T^0=1\)`.  --&gt;

&lt;!--   - If `\(Y = k\)`, then `\(T^j = 1\)` for `\(j \leq {k-1}\)` and `\(T^j=0\)` for `\(j &gt; k-1\)`. --&gt;

&lt;!--   - We can interpret `\(T^j = I\{Y &gt; j\}\)` --&gt;

&lt;!--   - Eg. `\(m=5\)`; if `\(Y=3\)`, then `\(T=(1,1,0,0)\)`; if `\(Y=1\)`, then `\(T=(0,0,0,0)\)` --&gt;


---
class: left, top
# Categorical and Ordinal-Categorical Variables

We consider response variables `\(Y\)` that are categorical or ordinal in nature.

  - Eg. Categorical `\(Y\)`: digits 0-9, alphabet a-z; 
  - Eg. Ordinal `\(Y\)`: wine quality 0-9, ratings/review 1-5; 

Categorical variables are linear exponential families because they are a special case of the multinomial

  - the multivariate Logistic and Expit functions are the canonical and inverse canonical links

**We show Ordinal variables can be represented explicitly as a linear exponential family**

  - the multivariate **Adjacent Categories** logistic link is the canonical link.
  - we compute the inverse canonical link and cumulant generating function explicitly.

&lt;!-- We say the random vector `\(T\)` has a **ordinal-categorical (Or-Cat)** distribution. --&gt;
&lt;!-- The ordinal-categorical distribution is a linear exponential family with the **adjacent-categories (Ad-Cat)** canonical link function  --&gt;
&lt;!-- \begin{align*} --&gt;
&lt;!-- \theta(\tau)  --&gt;
&lt;!-- = --&gt;
&lt;!-- \log \left\{ \mathrm{diag}[ (P^{-1} - I)\tau ]^{-1} (P^{-1} - I)\tau \right \}. --&gt;
&lt;!-- \end{align*} --&gt;
&lt;!-- and inverse canonical link function --&gt;
&lt;!-- \begin{align*} --&gt;
&lt;!-- \tau(\theta) --&gt;
&lt;!-- =\frac{Q P  L \exp (L \theta ) }{1 + e_1^{\top} P   L \exp (L \theta ) } --&gt;
&lt;!-- , --&gt;
&lt;!-- \end{align*} --&gt;
&lt;!-- with log-likelihood --&gt;
&lt;!-- \begin{align*} --&gt;
&lt;!-- \ell(\theta; T) --&gt;
&lt;!-- = \theta^{\top} T - \log ( 1 + e_{1}^{\top}  P   L \exp (L \theta ) )   --&gt;
&lt;!-- , --&gt;
&lt;!-- \end{align*} --&gt;
&lt;!-- where --&gt;
&lt;!-- `\(b(\theta) = \log ( 1 + e_{1}^{\top}  P   L \exp (L \theta ) )\)`. --&gt;

---
count: false
class: inverse, center, middle

# .bg-text[Inverse and Forward Linear SDR]  

---
class: left, top
# Why Sufficient Dimension Reduction? 


Suppose we have a large dataset with some response `\(Y \in \R^m\)` and predictors `\(X \in \R^p\)`.

  - When `\(p\)` is large, lower dimensional summaries of `\(X\)` are helpful for visualization and application of conventional statistical methods

  - Finding a linear lower dimensional summary of `\(X\)` means finding `\(\beta \in \R^{p \times d}\)`, where `\(d &lt; p\)`, in order to construct the lower dimensional summary `\(\beta^\top X\)`
  
  - Sufficient Dimension Reduction (SDR) are approaches for finding `\(\beta\)` such that `\(\beta^\top X\)` retains all relevant information about `\(Y\)`
  
  - Inverse and Forward SDR describe general SDR approaches for finding such a dimension reduction `\(\beta\)`



---
class: left, top
# Motivating Example: 

Consider a response `\(Y\)` and predictor `\(X = (X_1, X_2) \in [0,1]^2\)`. 
Let `\(Y=X_1^2\)`.
Then `\(Y= (\beta^\top X)^2\)`, where `\(\beta = (1,0) \in \R^2\)`. 
- we want to recover `\(span(\beta) = \{ (c,0): c \in \R\}\)`.
.center[
&lt;img align="centered" class="image" src="images/sdr_plot.png" width="50%"&gt;
]



---
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Sliced Inverse Regression (Li, 1991)

.center[
&lt;img align="centered" class="image" src="images/sir_plot1.png" width="55%"&gt;
]

&lt;!-- &lt;iframe src="images/almost_sir.html" width="90%" height="90%" frameborder="0"&gt;&lt;/iframe&gt; --&gt;


---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Sliced Inverse Regression (Li, 1991)

.center[
&lt;img align="centered" class="image" src="images/sir_plot2.png" width="55%"&gt;
]

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Sliced Inverse Regression (Li, 1991)

.center[
&lt;img align="centered" class="image" src="images/sir_plot3.png" width="55%"&gt;
]

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Sliced Inverse Regression (Li, 1991)

.center[
&lt;img align="centered" class="image" src="images/sir_plot4.png" width="55%"&gt;
]

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Sliced Inverse Regression (Li, 1991)

.center[
&lt;img align="centered" class="image" src="images/sir_plot5.png" width="55%"&gt;
]
--

  - 'Inverse' because we estimate `\(E(X|Y)\)`.

---
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Drawbacks

.center[
&lt;img align="centered" class="image" src="images/sir_plot5.png" width="55%"&gt;
]

---
count: false
class: left, top  

# Motivating Example: 
## Inverse Regression for SDR - Drawbacks

.center[
&lt;img class="image" src="images/bad_sir_plot1.png" width="55%"&gt;
] 

---
count: false
class: left, top  

# Motivating Example: 
## Inverse Regression for SDR - Drawbacks

.center[
&lt;img class="image" src="images/bad_sir_plot2.png" width="55%"&gt;
] 

---
count: false
class: left, top  

# Motivating Example: 
## Inverse Regression for SDR - Drawbacks

.center[
&lt;img class="image" src="images/bad_sir_plot3.png" width="55%"&gt;
] 
--
- Inverse methods require assumptions on the support of the predictor.

---
class: left, top  

# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, and Zhu, 2002)

.center[
&lt;img class="image" src="images/fsdr_plot1.png" width="55%"&gt;
] 

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)


.center[
&lt;img class="image" src="images/fsdr_plot2.png" width="55%"&gt;
] 

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)
 

.center[
&lt;img class="image" src="images/fsdr_plot3.png" width="55%"&gt;
] 



---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)
 

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="55%"&gt;
] 

--
- "Forward" Regression because we are estimating `\(E(Y|x)\)` and `\(\partial E(Y|x)/\partial x^\top\)`

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)


.center[
&lt;img class="image" src="images/fsdr_plot4.png" width="55%"&gt;
] 

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)


.center[
&lt;img class="image" src="images/fsdr_plot5.png" width="55%"&gt;
]  

 
---
class: left, top
# Forward Regression for SDR 


- OPG developed for scalar `\(Y\)`; may not work as well for categorical `\(Y\)`.

- **Idea: Estimate a GLM locally instead of a Linear Regression.**


.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="55%"&gt;
]  


---
count: false
class: inverse, center, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text[Outer Product of Canonical Gradients (OPCG)]  

---
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Generalized Forward Regression for SDR

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="40%"&gt;
]  

- In OPG, we fit a linear regression about `\(x_0\)`, i.e. we minimize
`\begin{align*}
\{Y - a_0 + B_0^\top(X-x_0) \}^\top \{Y - a_0 + B_0^\top(X-x_0) \} 
\end{align*}`
and estimate `\(\partial E(Y|x_0)/ \partial x^\top\)` using `\(\hat B_0\)`. 
    &lt;!-- `$$\hat B_0 = \frac{ \widehat{ \partial E(Y|x_0)} }{\partial x^\top}.$$` --&gt;


---
class: left, top 
 
# Generalized Forward Regression for SDR

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="40%"&gt;
]  

- Instead, fit a linear exponential family about `\(x_0\)`, i.e. minimize
`\begin{align*}
  -\ell_0(a_0, B_0;Y,X,x_0)  = - [a_0 + B_0^\top(x-x_0)]^\top y + b[a_0 + B_0^\top(x-x_0)]  
\end{align*}`
and use the estimate `\(\hat B_0\)`. 

---
class: left, top

# Outer Product of Canonical Gradients (OPCG)

Given a random sample `\(Y_{1:n}\)`, `\(X_{1:n}\)`, fit a local linear GLM about each `\(X_j\)`, for `\(j=1,...,n\)`, by minimizing the full negative local linear log-likelihood:
`\begin{align*}
&amp; L(a_1,..,a_n, B_1,...,B_n; X_{1:n}, Y_{1:n}) \\  
= &amp; -\frac {1}{n} \sum_{j,i=1}^n
K \bigg ( \frac{X_i - X_j}{h} \bigg )\\
&amp; \times 
\{[a_{j} + B_{j}^\top (X_i - X_j)]^\top Y_i - 
b(a_{j} + B_{j}^\top (X_i - X_j)) \} 
,
\end{align*}`
where `\(b(\cdot)\)` determines the GLM, and `\(K(\cdot)\)` is a kernel weight with bandwidth `\(h\)`. 

--

This provides minimizers `\(\hat B_j\)` that estimate `\(\partial \theta(X_j)/\partial x^\top\)`, which we use to construct the average outer product
`$$\hat \Lambda_n = \frac 1n \sum_{j=1}^n \hat B_j \hat B_j^\top.$$` 

---
class: left, top

# The OPCG Estimator

The **Outer Product of Canonical Gradients (OPCG) Estimator** for `\(\beta\)`, `\(\hat \beta_{opcg}\)`, is the first `\(d\)` eigenvectors of 
`$$\hat \Lambda_n = \frac 1n \sum_{j=1}^n \hat B_j \hat B_j^\top,$$` 
corresponding to the `\(d\)` largest eigenvalues.

.center[
&lt;!-- &lt;img class="image" src="images/fsdr_plot3-1.png" width="40%"&gt; --&gt;
&lt;img class="image" src="images/fsdr_plot4.png" width="40%"&gt;
] 

---
class: left, top 
# Properties related to OPCG

  - OPCG is consistent:
  
  &lt;div class="theorem"&gt;
  Under some regularity assumptions, as \( n \to \infty\), we have
  \begin{align*} 
  \| \hat \beta_{opcg}  - \beta \|_F = O_{a.s}
  ( h + h^{-1} \delta_{ph} + \delta_{n} )
  ,
  \end{align*} 
  where \(\delta_{ph} = \sqrt{ \frac{\log n}{ nh^p} }\),
  \(\delta_{n} = \sqrt{ \frac{\log n}{ n } }\),
  \(  h \downarrow 0\), and
  \( h^{-1}\delta_{ph} \to 0\).
  &lt;/div&gt;
  
  - Can be implemented using Newton-Raphson

  - Ladle and Predictor Augmentation methods are fast, eigen-based methods that can be applied to estimate `\(d\)`. (Luo and Li, 2020; Luo and Li, 2016)

  - **For the bandwidth `\(h\)`,  we propose a K-means approach for tuning that selects `\(h\)` by minimizing an F-ratio.**
    - Cross Validation or Optimal bandwidths can be used as well. 
    
---
count: false
class: inverse, center, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text[Simulations and Data Analyses]

--
.left[

1. Goal: generalize OPG to Categorical and Ordinal-Categorical Responses

2. Categorical and Ordinal-Categorical variables are linear exponential families
 
3. Proposal: OPCG generalizes OPG to linear exponential families

]

---
class: left, top
# OPCG Procedure

Given data:

  1. Simulations: we have 3 data sets
  
    - Training set used for estimating `\(\hat \beta_{opcg}\)`; 
    
      - Used for estimating `\(d\)` as well.
      
    - Tuning set used for tuning bandwidth `\(h\)`
    
    - Testing set used for assessing performance
  
  2. Applications: Data is split into two sets:
  
    - Training set used for estimating `\(\hat \beta_{opcg}\)`; 
    
      - Used for estimating `\(d\)` as well.
      
      - Used K-fold tuning procedure for bandwidth `\(h\)`
      
    - Testing set used for assessing performance


---
class: left, top
# Simulations


Our predictor will be `\(X=(X^1,X^2,X^3,...,X^{10}) \in \R^{10}\)`.

  - We generate generate 5 clusters from a bivariate normal `\((X^1, X^2)\)`, augmented with 8 standard normals for random noise, `\((X^3,...,X^{10})\)`.

  - Two clusters are labelled 1, two are labelled 2, and one cluster is labelled 3; So `\(Y \in \{1,2,3\}\)` is categorical.
 
  - We draw an equal number of observations from each cluster for the training set, tuning set and testing set.
 

---
class: ani-slide
# K-mean Tuning for `\(h\)`
 
&lt;iframe src="images/tuning_sc.html" width="100%" height="95%" frameborder="0" &gt;&lt;/iframe&gt;



---
count: false
class: left, top
# Simulations - tuning and estimation

Conventional suggestion: `\(h = 2.34 n^{-1/(p+6)} \approx 1.66\)`;

K-fold K-means Tuning: `\(h \approx 1.25\)`;


--

Using `\(h \approx 1.25\)` to estimate OPCG `\(\hat \beta_{opcg}\)`:

&lt;img align="centered" class="image" src="images/sim_dist_table3-1.jpg" width="100%"&gt;

---
class: left, top
# Categorical Data Analysis  

We analyze three datasets with categorical responses: 

  - Handwritten Digits (Pendigit) from UCI
  
    - p=16; train/test=1333/667; resp=0-9
    
  - USPS Handwritten Digits  
  
    - p=256; train/test=1338/669; resp=0-9
    
  - ISOLET from UCI
  
    - p=618; train/test=6334/1553; resp=a-z

---
class: left, top
# Categorical Classification Error using SVM

.center[
&lt;img align="center" class="image" src="images/cat_class_table5-1.jpg" width="80%"&gt;
]

---
class: left, top, inverse
# .bg-text[Conclusion]


1. Generalized OPG to linear exponential families 

2. Demonstrated a K-means tuning procedure for classification

3. Demonstrated the effectiveness of OPCG in categorical classification problems.

  - Can handle multiple labels simultaneously 
  
  - Noticeable improvement over OPG for larger `\(p\)` 


---
class: middle, center, inverse
#Dedicated To

.bg-text[Donald Alexander Stuart Fraser]


*In Memory of his infectious passion for statistics, immense patience with students, and prevalent use of pictures for complex concepts.* 

.center[
&lt;img align="center" class="image" src="https://upload.wikimedia.org/wikipedia/commons/3/38/Don_Fraser_OOC.jpg" width="30%"&gt;
]

April 29, 1925 - December 21, 2020


---
layout: false
# References

Li, K. (1991). "Sliced inverse regression for dimension reduction". In:
_Journal of the American Statistical Association_ 86.414, pp. 316-327.

Luo, W. and B. Li (2016). "Combining eigenvalues and variation of
eigenvectors for order determination". In: _Biometrika_ 103.4, pp.
875-887.

Luo, W. and B. Li (2020). "On order determination by predictor
augmentation". In: _Biometrika_.

Xia, Y., H. Tong, W. Li, et al. (2002). "An adaptive estimation of
dimension reduction space". In: _Journal of the Royal Statistical
Society: Series B (Statistical Methodology)_ 64.3, pp. 363-410.

&lt;!-- --- --&gt;
&lt;!-- layout: false --&gt;
&lt;!-- # References --&gt;

&lt;!-- ```{r, echo=FALSE, results="asis"} --&gt;
&lt;!-- PrintBibliography(bib_sdr, start=7, end=12) --&gt;
&lt;!-- ``` --&gt;

&lt;!-- --- --&gt;
&lt;!-- layout: false --&gt;
&lt;!-- # References --&gt;

&lt;!-- ```{r, echo=FALSE, results="asis"} --&gt;
&lt;!-- PrintBibliography(bib_sdr, start=13) --&gt;
&lt;!-- ``` --&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
