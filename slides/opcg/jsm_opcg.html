<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Forward Sufficient Dimension Reduction for Categorical and Ordinal Responses</title>
    <meta charset="utf-8" />
    <meta name="author" content="Harris Quach" />
    <script src="libs/header-attrs-2.11/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="mytheme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: { 
      independenT: ["{\\mathrel{\\rlap{ #1 }\\mkern2mu{ #1 }}}",1],
      indep: "{\\independenT{\\perp}}",
      SS: "{\\mathscr{S}}",
      R:"{\\mathbb{R}}",
      Xcal: "{\\mathcal{X}}",
      water: "{H_2O}",
      braket: ['{\\langle #1 \\rangle}', 1], 
      Abs: ['\\left\\lvert #2 \\right\\rvert_{\\text{#1}}', 2, ""]
    }
  }
});
</script>




 

&lt;!-- class: title-slide --&gt;

# On Forward Sufficient Dimension Reduction for Categorical and Ordinal Responses
&lt;!-- # .bg-text[Generalized Forward Sufficient Dimension Reduction for Classification] --&gt;
&lt;!-- &lt;hr width="700" align="left" /&gt; --&gt;
&lt;hr/&gt;

Harris Quach (joint work with Dr. Bing Li) &lt;br/&gt; Date: "2022/08/07 (updated: 2022-08-07)"

---
class: inverse, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text.center[Overview of paper]  

.md-text[
  
1. We show ordinal variables are linear exponential families
  
2. We generalize a popular SDR method to linear exponential families
  
3. We proposed a K-means tuning procedure
  
]
 

---
class: left, top
# Context  

  - Suppose `\(Y| X \sim GLM(\theta(X))\)` and predictors `\(X \in \R^p\)` are continuous.
    - So Y depends on X solely through a smooth function `\(\theta(\cdot)\)`.
    
  
  - Assume a Sufficient Dimension Reduction exists: There exists a linear map `\(\beta \in \R^{p \times d}\)`, `\(d &lt; p\)`, such that
  
`\begin{align*}
 Y \indep \theta(X) | \beta^{\top} X
\end{align*}`


## Goal: Estimate the subspace spanned by `\(\beta\)` 
  - i.e. construct lower-dimensional summaries of `\(X \in \R^p\)` that are informative for `\(Y\)`, where `\(Y\)` is Categorical or Ordinal.

---
class: left, top
# Motivating Example: 

Suppose `\(X = (X_1, X_2) \in [0,1]^2\)` with some restricted support and `\(\theta(X) = X_1^2\)`. 
Then `\(\theta(X) = (\beta^\top X)^2\)`, where `\(\beta = (1,0) \in \R^2\)`.

- we want to recover `\(span(\beta) = \{ (c,0): c \in \R\}\)`;

.center[
&lt;img class="image" src="images/fsdr_plot1.png" width="45%"&gt;
] 

---
class: left, top &lt;!-- formatting the slide --&gt;
# 1. Outer Product of Canonical Gradients (OPCG) 
## Generalizes the Outer Product of Gradients (OPG) (Xia, Tong, Li, and Zhu, 2002)

.center[
&lt;img class="image" src="images/fsdr_plot2.png" width="50%"&gt;
] 

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;
# 1. Outer Product of Canonical Gradients (OPCG) 
## Generalizes the Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)


.center[
&lt;img class="image" src="images/fsdr_plot3.png" width="50%"&gt;
] 


---
count: false
class: left, top &lt;!-- formatting the slide --&gt;
# 1. Outer Product of Canonical Gradients (OPCG) 
## Generalizes the Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)


.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="50%"&gt;
] 

--

- Fitting a local linear GLM to estimate the gradients at each point.

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;
# 1. Outer Product of Canonical Gradients (OPCG) 
## Generalizes the Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)
 


.center[
&lt;img class="image" src="images/fsdr_plot4.png" width="50%"&gt;
] 

--
- Use a eigen decomposition to get direction most variation.

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;
# 1. Outer Product of Canonical Gradients (OPCG) 
## Generalizes the Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)
 


.center[
&lt;img class="image" src="images/fsdr_plot5.png" width="50%"&gt;
]

- Effectively a PCA on the estimated gradients; **Estimated directions form the OPCG Estimator, `\(\hat \beta_{\mathrm{opcg}} \in \R^{p \times d}\)`.**


---
class: left, top

# Outer Product of Canonical Gradients (OPCG)

Given a random sample `\(Y_{1:n}\)`, `\(X_{1:n}\)`, fit a local linear GLM about each `\(X_j\)`, for `\(j=1,...,n\)`, by minimizing the full negative local linear log-likelihood:
`\begin{align*}
&amp; L(a_1,..,a_n, B_1,...,B_n; X_{1:n}, Y_{1:n}) \\  
= &amp; -\frac {1}{n} \sum_{j,i=1}^n
K \bigg ( \frac{X_i - X_j}{h} \bigg )\\
&amp; \times 
\{[a_{j} + B_{j}^\top (X_i - X_j)]^\top Y_i - 
b(a_{j} + B_{j}^\top (X_i - X_j)) \} 
,
\end{align*}`
where `\(b(\cdot)\)` determines the local GLM, and `\(K(\cdot)\)` is a kernel weight with bandwidth `\(h\)`. 

--

This provides minimizers `\(\hat B_j\)` that estimate `\(\partial \theta(X_j)/\partial x^\top\)`, which we use to construct the average outer product
`$$\hat \Lambda_n = \frac 1n \sum_{j=1}^n \hat B_j \hat B_j^\top.$$` 

---
class: left, top

# The OPCG Estimator

The **Outer Product of Canonical Gradients (OPCG) Estimator** for `\(\beta\)`, `\(\hat \beta_{opcg}\)`, is the first `\(d\)` eigenvectors of 
`$$\hat \Lambda_n = \frac 1n \sum_{j=1}^n \hat B_j \hat B_j^\top,$$` 
corresponding to the `\(d\)` largest eigenvalues.

.center[
&lt;!-- &lt;img class="image" src="images/fsdr_plot3-1.png" width="40%"&gt; --&gt;
&lt;img class="image" src="images/fsdr_plot4.png" width="40%"&gt;
] 

---
class: left, top 
# Under some regularity conditions:

&lt;div class="prop" text="Exhaustiveness"&gt;
  Let
  \begin{align*}
  \Lambda_{\mathrm{opcg}}
  =
  E 
  \bigg \{ 
  \frac{\partial \theta(X)^\top}{\partial x}
  \frac{\partial \theta(X)}{\partial x^\top}
  \bigg \},
  \end{align*}
  and \(\eta\) be the leading \(d\) eigenvectors of \(\Lambda_{\mathrm{opcg}}\). 
  Under SDR and a convexity assumption, \(\mathrm{span}(\eta ) = \SS_{E(Y|X)}  \).
  &lt;/div&gt;
  &lt;br&gt;
  &lt;div class="theorem"&gt;
  Under some regularity assumptions, as \( n \to \infty\), we have
  \begin{align*} 
  \| \hat \beta_{opcg}  - \eta \|_F = O_{a.s}
  ( h + h^{-1} \delta_{ph} + \delta_{n} )
  ,
  \end{align*} 
  where \(\delta_{ph} = \sqrt{ \frac{\log n}{ nh^p} }\),
  \(\delta_{n} = \sqrt{ \frac{\log n}{ n } }\),
  \(  h \downarrow 0\), and
  \( h^{-1}\delta_{ph} \to 0\).
  &lt;/div&gt;

  - Ladle and Predictor Augmentation methods are fast, eigen-based methods that can be applied to estimate `\(d\)`. (Luo and Li, 2020; Luo and Li, 2016)

  - **For the bandwidth `\(h\)`,  we propose a K-means approach for tuning that selects `\(h\)` by minimizing an F-ratio.**
    - Cross Validation or Optimal bandwidths can be used as well. 
    

---
class: left, top
# K-means Tuning `\(h\)` for Classification. 

In the local estimation step, we need to select the bandwidth `\(h\)`.

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="55%"&gt;
] 

---
class: left, top
# K-means Tuning `\(h\)` for Classification. 

Split your training sample into two sets. Denote the second set by `\((X^{\mathbb{V}}, Y^{\mathbb{V}})_{1:n_2}\)`. For each `\(h\)`, 

Essential Idea: 
  1. Estimate `\(\hat \beta_{\mathrm{opcg}}\)` on `\((Y, X)_{1:n_1}\)` and construct `\(\hat \beta_{\mathrm{opcg}}^\top X_{1:n_2}^{\mathbb{V}}\)`
  		
  2. Apply K-means to sufficient predictors `\(\hat \beta_{\mathrm{opcg}}^\top X_{1:n_2}^{\mathbb{V}}\)` for `\(m\)` clusters. 
    - `\(m\)` is the number of categories for `\(Y\)`.
    - This returns `\(m\)` estimated clusters and an F-ratio, for each `\(h\)`.
  		
  3. Select `\(h\)` that minimizes the F-ratio from K-means.

**Some modifications are made to this idea to address some practical and tuning issues, but the idea remains the same.**

---
count: false
class: inverse, center, middle

# .bg-text[Simulations and Data Analyses]


---
class: left, top
# Simulations 

Let `\(Y \in \{1,2,3\}\)` be categorical and `\(X \in \R^{10}\)`. 
We have `\((X^3, X^7) \sim N_2(\mu_j, 0.5 I_2)\)` for `\(j=1,2,\ldots,5\)`, and the rest are `\(N(0,0.5)\)`.

  - the 5 clusters are labeled 1, 2, or 3.
  - `\(Y\)` is the label of the cluster `\((X^3, X^7)\)` belongs, so `\(Y\)` depends on only `\(\beta^{\top}X = (X^3, X^7)\)`.

We have a training sample of 300:
  - Use half for `\(\hat \beta_{\mathrm{opcg}}\)` and half for `\(\hat \beta_{\mathrm{opcg}}^\top X_{1:150}^{\mathbb{V}}\)`

  - Assume we know `\(d=2\)` for `\(\hat \beta_{\mathrm{opcg}} \in \R^{ 10 \times 2}\)`;
  
---
class: ani-slide
# K-mean Tuning for `\(h\)`
 
&lt;iframe src="images/tuning_sc.html" width="100%" height="90%" frameborder="0" &gt;&lt;/iframe&gt;

---
count: false
class: left, top
# Simulations - tuning and estimation

Conventional suggestion: `\(h = 2.34 n^{-1/(p+6)} \approx 1.66\)`;

K-fold K-means Tuning: `\(h \approx 1.25\)`;

&lt;br&gt;

Using `\(h \approx 1.25\)` to estimate OPCG `\(\hat \beta_{opcg}\)`:

&lt;img align="centered" class="image" src="images/sim_dist_table5.png" width="100%"&gt;

---
class: left, top
# Categorical Data Analysis  

We analyze three datasets with categorical responses: 

  - Handwritten Digits (Pendigit) from UCI
  
    - p=16; train/test=1333/667; resp=0-9
    
  - USPS Handwritten Digits  
  
    - p=256; train/test=1338/669; resp=0-9
    
  - ISOLET from UCI
  
    - p=618; train/test=6334/1553; resp=a-z

---
class: left, top
# Categorical Classification Error using SVM

.center[
&lt;img align="center" class="image" src="images/cat_class_table5-1.jpg" width="80%"&gt;
]


---
class: left, top
# Ordinal-Categorical Data Analysis
## Red Wine Quality

We analyze the wine quality rating data set from the UCI repository:  
  - Ordinal response - Wine Quality Score, `\(Y \in \{3,4,5\}\)`; `\(p=11\)`, characteristics of the wine; `\(n_{train}\)` / `\(n_{test}\)`: 1000/599; 

Plots of `\(\hat \beta^{\top} X_{\mathrm{test}}\)`:

&lt;img align="centered" class="image" src="images/opcg_wine_test.png" width="100%"&gt;


---
class: left, top, inverse
# .bg-text[Conclusion]


1. Generalized OPG to linear exponential families via OPCG

2. Demonstrated a K-means tuning procedure for classification

3. Demonstrated the effectiveness of OPCG in categorical classification problems. 

---
layout: false
# References


```
## Warning in `[[.BibEntry`(x, ind): subscript out of bounds
```

Luo, W. and B. Li (2016). "Combining eigenvalues and variation of
eigenvectors for order determination". In: _Biometrika_ 103.4, pp.
875-887.

Luo, W. and B. Li (2020). "On order determination by predictor
augmentation". In: _Biometrika_.

Xia, Y., H. Tong, W. Li, et al. (2002). "An adaptive estimation of
dimension reduction space". In: _Journal of the Royal Statistical
Society: Series B (Statistical Methodology)_ 64.3, pp. 363-410.

&lt;!-- --- --&gt;
&lt;!-- layout: false --&gt;
&lt;!-- # References --&gt;

&lt;!-- ```{r, echo=FALSE, results="asis"} --&gt;
&lt;!-- PrintBibliography(bib_sdr, start=7, end=12) --&gt;
&lt;!-- ``` --&gt;

&lt;!-- --- --&gt;
&lt;!-- layout: false --&gt;
&lt;!-- # References --&gt;

&lt;!-- ```{r, echo=FALSE, results="asis"} --&gt;
&lt;!-- PrintBibliography(bib_sdr, start=13) --&gt;
&lt;!-- ``` --&gt;






    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
