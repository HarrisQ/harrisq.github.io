<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Generalized Forward Sufficient Dimension Reduction for Classification</title>
    <meta charset="utf-8" />
    <meta name="author" content="Harris Quach" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="mytheme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: { 
      independenT: ["{\\mathrel{\\rlap{ #1 }\\mkern2mu{ #1 }}}",1],
      indep: "{\\independenT{\\perp}}",
      SS: "{\\mathscr{S}}",
      R:"{\\mathbb{R}}",
      Xcal: "{\\mathcal{X}}",
      water: "{H_2O}",
      braket: ['{\\langle #1 \\rangle}', 1], 
      Abs: ['\\left\\lvert #2 \\right\\rvert_{\\text{#1}}', 2, ""]
    }
  }
});
</script>




 

&lt;!-- class: title-slide --&gt;

# Generalized Forward Sufficient Dimension Reduction for Classification
&lt;!-- # .bg-text[Generalized Forward Sufficient Dimension Reduction for Classification] --&gt;
&lt;!-- &lt;hr width="700" align="left" /&gt; --&gt;
&lt;hr/&gt;
## Harris Quach (joint work with Bing Li) &lt;br/&gt; Date: 2021-03-05 (updated: 2021-02-24)

---
# Outline

1. Sufficient Dimension Reduction (SDR)
  - Inverse Regression for SDR
  - Forward Regression for SDR
  
2. Multivariate Categorical and Ordinal Canonical Link functions

3. Generalized Forward Regression for SDR
  - Outer Product of Canonical Gradients (OPCG)
  - Minimum Average Deviance Estimator (MADE) and Refinements 
  
4. Tuning the bandwidth via K-means 

4. Simulations and Data Analysis
  - Simulations
  - Wine Quality
  - USPS Digits
  - ISOLET
  &lt;!-- - Amazon Commerce Reviews --&gt;
  &lt;!-- - Anuran Frogs --&gt;

---
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# What is Sufficient Dimension Reduction?

A Sufficient Dimension Reduction (SDR) method extracts a lower dimensional function of the predictors that "preserves" relationships of interest between the predictors and response.

Let `\(Y \in \R^m\)` be a response and `\(X \in \R^p\)` be a predictor. 

- Linear SDR seeks a linear function `\(s(X) = \beta^\top X\)`, where `\(\beta \in \R^{p \times d}\)` with `\(d &lt; p\)`.

  - We focus mainly on Linear SDR.

---
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# What is Sufficient Dimension Reduction?


- If we want to preserve all relevant information available in `\(X\)` about `\(Y\)`, then the goal of SDR is to find `\(\beta\)` so that `\(Y \indep X \; | \; \beta^\top X\)`.

  - For any `\(\beta\)` such that `\(Y \indep X \; | \; \beta^\top X\)`, the `\(span(\beta)\)`     is a _Sufficient Dimension Reduction Subspace_.
  
  - Under mild conditions, a _minimial_ Sufficient Dimension Reduction Subspace exists (Yin, Li, and Cook (2008))
  
      - This is the **Central Subspace (CS)** and is denoted `\(\SS_{Y \indep X}\)`.

---
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# What is Sufficient Dimension Reduction?

- If we want to preserve all relevant information available in `\(X\)` about the _regression relation_ `\(E(Y|X)\)`, then the goal is to find `\(\beta\)` so that 
`\(Y \indep E(Y|X) \; | \; \beta^\top X\)`.

  - The dimension reduction for the regression relation can be phrased as finding       `\(\beta\)` such that `\(E(Y|X) = E(Y | \beta^\top X )\)`
  
  - The minimial Sufficient Dimension Reduction Subspace for the regression            relation is the **Central Mean Subspace (CMS)** and is denoted `\(\SS_{E(Y | X)}\)`.
  
  - In general, the CMS is a subspace of the CS: `\(\SS_{E(Y | X)} \leq \SS_{Y \indep X}\)`
   
&lt;div class=prop text="CMS=CS"&gt;
If \(Y\) depends on \(X\) only through the Conditional Mean, 
i.e. \(Y \indep X \;|\; E(Y|X)\), then \(\SS_{E(Y | X)} = \SS_{Y \indep X}\).
&lt;/div&gt;

&lt;!-- - This framework can be applied broadly to define the Central Median Subspace, Central Quantile Subspace, Central `\(k^{th}\)`-Moment Subspace, and so on. --&gt;

&lt;!-- `\(\Xcal\)` and `\(\water\)` and `\(\braket{4}\)` and `\(\Abs{4}{3}\)`. --&gt;    
 
---
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 

Given response `\(Y\)` and predictor `\(X = (X_1, X_2) \in [0,1]^2\)`, let `\(Y=X_1^2\)`.
So `\(Y= (\beta^\top X)^2\)`, where `\(\beta = (1,0) \in \R^2\)`. 

.center[
&lt;img align="centered" class="image" src="images/sdr_plot.png" width="50%"&gt;
]

- we want to recover `\(span(\beta)\)`.

---
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Sliced Inverse Regression (Li, 1991)

.center[
&lt;img align="centered" class="image" src="images/sir_plot1.png" width="55%"&gt;
]

&lt;!-- &lt;iframe src="images/almost_sir.html" width="90%" height="90%" frameborder="0"&gt;&lt;/iframe&gt; --&gt;


---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Sliced Inverse Regression (Li, 1991)

.center[
&lt;img align="centered" class="image" src="images/sir_plot2.png" width="55%"&gt;
]

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Sliced Inverse Regression (Li, 1991)

.center[
&lt;img align="centered" class="image" src="images/sir_plot3.png" width="55%"&gt;
]

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Sliced Inverse Regression (Li, 1991)

.center[
&lt;img align="centered" class="image" src="images/sir_plot4.png" width="55%"&gt;
]

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Sliced Inverse Regression (Li, 1991)

.center[
&lt;img align="centered" class="image" src="images/sir_plot5.png" width="55%"&gt;
]
---
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Drawbacks

.center[
&lt;img align="centered" class="image" src="images/sir_plot5.png" width="55%"&gt;
]

---
count: false
class: left, top  

# Motivating Example: 
## Inverse Regression for SDR - Drawbacks

.center[
&lt;img class="image" src="images/bad_sir_plot1.png" width="55%"&gt;
] 

---
count: false
class: left, top  

# Motivating Example: 
## Inverse Regression for SDR - Drawbacks

.center[
&lt;img class="image" src="images/bad_sir_plot2.png" width="55%"&gt;
] 

---
count: false
class: left, top  

# Motivating Example: 
## Inverse Regression for SDR - Drawbacks

.center[
&lt;img class="image" src="images/bad_sir_plot3.png" width="55%"&gt;
] 
--
- Inverse methods require assumptions on the support of the predictor.

---
class: left, top  

# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, and Zhu, 2002)

.center[
&lt;img class="image" src="images/fsdr_plot1.png" width="55%"&gt;
] 

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)


.center[
&lt;img class="image" src="images/fsdr_plot2.png" width="55%"&gt;
] 

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)
 

.center[
&lt;img class="image" src="images/fsdr_plot3.png" width="55%"&gt;
] 

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)
 

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="55%"&gt;
] 

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)


.center[
&lt;img class="image" src="images/fsdr_plot4.png" width="55%"&gt;
] 

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)


.center[
&lt;img class="image" src="images/fsdr_plot5.png" width="55%"&gt;
]  

 
---
class: left, top
# Forward Regression for SDR 

- OPG can only estimate dimension reduction for the regression relationship `\(E(Y|X)\)`

  - OPG can only recover `\(\SS_{E(Y|X)}\)`
  
  - Extensions exist to recover `\(\SS_{Y|X}\)` 
  
      - dOPG (Xia, 2007); 
        Ensemble OPG (Yin, Li, and others, 2011); 
        Sliced Regression (Wang and Xia, 2008)


---
class: left, top
# Generalized Forward Regression for SDR 


- OPG does not work for categorical responses since we can not take derivatives of `\(Y\)`.
  
- Idea: Estimate a GLM locally instead of a Linear Regression.
&lt;br/&gt;&lt;br/&gt;

--

Existing extensions of Forward SDR:

1. Generalized Single Index Model (GSIM): Lambert-Lacroix and Peyre (2006)
  - Local Linear Single Index GLM with specified link to estimate the Average Derivative of the Conditional Mean

2. Minium Average Deviance Estimation (MADE): Adragni (2018)
  - Local Linear GLM for scalar response
  
3. gradient Kernel Dimension Reducion (gKDR): Fukumizu and Leng (2014)
  - Estimate the gradients via spline approximation using RKHS

---
class: inverse, center, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text[Multivariate Link functions]  


---
class: left, top
# Sufficient Dimension Reduction for Classification

Multinomial distributions are linear exponential family and are characterized entirely by their means through the canonical parameter `\(\theta\)`. 

  - This includes *categorical* responses and *ordinal* responses

--

&lt;div class="prop"&gt;(Cook and Li, 2002)
If \(Y\) depends on \(X\) solely through the canonical parameter \(\theta(X)\), then the Central Mean Subspace coincides with the Central Subspace, i.e. \(\SS_{Y|X} = \SS_{E(Y|X)}\). 
&lt;/div&gt;

So extending OPG to categorical responses will recover `\(\SS_{E(Y|X)}=\SS_{Y|X}\)`.

---
class: left, top
# Sufficient Dimension Reduction for Classification

Our idea is to fit local linear GLMs instead of local linear regression.

- But to fit local linear GLMs, we need the cumulant generating function `\(b(\cdot)\)` to evaluate the log-likelihoods for our optimization algorithms.

- For `\(b(\cdot)\)`, we need the inverse canonical link function that maps the mean to the canonical parameter `\(\theta\)`, and the inverse canonical link.

--

- We can also use the canonical link to construct initial values for an optimization algorithm via the regression
`\begin{align*}
\theta(S) = a + B^\top X + \varepsilon,
\end{align*}`
with `\(\varepsilon \sim N(0,I_{m-1})\)`.



---
class: left, top
# Categorical Response

Suppose `\(Y\)` is a categorical variable taking values in `\(\{1,...,m\}\)` nominal categories. 

- We can represent `\(Y\)` as a vector `\(S = (S^1,...,S^{m-1}) \in \{0,1\}^{m-1}\)` with `\(S^m = 1 - \sum_{j=1}^{m-1} S^m\)`.

  - If `\(Y=k\)`, then `\(S^k = 1\)` and `\(S^j=0\)` for `\(j \neq k\)`. 

  - e.g. If `\(m=3\)` and `\(Y = 2\)`, then `\(S = (0,1)\)`; if `\(Y=3\)`, then `\(S=(0,0)\)`. 

--

Let `\(p = (p^1,...p^{m-1})\)` be the `\(m-1\)` vector of probabilities for each category.  The mean and variance of `\(S\)` are
`\begin{align*}
E(S) =  p , 
\quad 
Var(S)
= \left [ Diag \left ( \frac{  e^{\theta   } }{ 1 + \boldsymbol 1^{\top}e^{\theta } }\right ) - \frac{ e^{\theta }  (e^{\theta }  )^{\top} }{ [1 + \boldsymbol 1^{\top}e^{\theta} ]^2 }  \right ]
,
\end{align*}`

---
class: left, top
# Categorical Response

The canonical link is the multivariate logistic function. 
The multivariate logistic link and its inverse are
`\begin{align*}
\theta(p) = \log \frac{ p }{ 1 - \boldsymbol{1}^{\top} p }
\qquad
p(\theta) =  \frac { \exp(\theta) } {  1 - \boldsymbol{1}^{\top} \exp( \theta) } 
\end{align*}`

--

The density and log-likelihood of `\(S\)` are 
`\begin{align*}
f(S;p) \propto \prod_{j=1}^{m-1} (p^j)^{S^j},
\quad  
\ell(\theta;S) = \theta^{\top}S - \log ( 1 - \boldsymbol 1^{\top}  e^\theta   )
,
\end{align*}`
where the cumulant generating function is 
`\(b(\theta) = -\log( 1 - \boldsymbol{1}^{\top} e^{  \theta }  )\)`.


---
class: left, top
# Ordinal Response

Suppose `\(Y\)` is an ordinal categorical variable, taking value in `\(\{1,...,m\}\)` ordered categories. 

- We can represent `\(Y\)` as a vector `\(S = (S^1,...S^{m-1}) \in \{0,1\}^{m}\)` as for categorical `\(Y\)`. 

- We can represent `\(S\)` as a vector `\(T = (T^1,...T^{m-1}) \in \{0,1\}^{m-1}\)` with  `\(T^{m} = 0\)` and `\(T^0=1\)`. 
  
  - If `\(Y = k\)`, then `\(T^j = 1\)` for `\(j \leq {k-1}\)` and `\(T^j=0\)` for `\(j &gt; k-1\)`. 

  - If `\(m=5\)` and `\(Y=3\)`, then `\(T=(1,1,0,0)\)`; if `\(Y=1\)`, then `\(T=(0,0,0,0)\)`


---
class: left, top
# Ordinal Response

Let 

 - `\(p = (p^1,...p^{m-1})\)` be the `\(m-1\)` vector of probabilities,
 - `\(\gamma = (p^1, p^1 + p^2, ..., p^1 + \cdots + p^{m-1})\)` be the `\(m-1\)` vector of cumulative sum of probabilities, and
 - `\(\tau = 1 - \gamma\)`. 

The mean and variance of `\(T\)` is
`\begin{align*}
E(T) = \tau, \quad 
Var(T) = \Gamma - \tau  \tau^{\top} ,
\end{align*}`
where
`\begin{align*}
\Gamma = \left ( \begin{matrix}
\tau^{1}  &amp; \tau^{2}  &amp; \cdots &amp; \tau^{m-2}&amp; \tau^{m-1} \\
\tau^{2}  &amp; \tau^{2}  &amp; \cdots &amp; \tau^{m-2} &amp; \tau^{m-1} \\
\vdots &amp; \vdots  &amp; \cdots  &amp; \cdots &amp; \vdots \\
\tau^{m-2} &amp; \tau^{m-2} &amp; \cdots &amp; \tau^{m-2}  &amp; \tau^{m-1} \\
\tau^{m-1} &amp; \tau^{m-1}  &amp; \cdots &amp; \tau^{m-1} &amp; \tau^{m-1 } \end{matrix} \right )
\end{align*}`

---
class: left, top
# Ordinal Response

We define the canonical parameter as `\(\theta(\tau) = (\theta(\tau)^1,...,\theta(\tau)^{m-1} )\)`, where the `\(j^{th}\)` entry of `\(\theta(\tau)\)` is
`\begin{align*}
\theta(\tau)^j
=
\log \bigg ( \frac{ p^{j+1} }{p^{j} } \bigg )
%= 
%\log \bigg ( \frac{ \gamma^{j+1} - \gamma^{j} }{ \gamma^{j} - \gamma^{j-1}  } \bigg ) 
=
\log \bigg ( \frac{  \tau^{j} - \tau^{j+1} }{  \tau^{j-1} - \tau^{j}  } \bigg )
.
\end{align*}`

The canonical link corresponds to the "Adjacent-Categories Logit Model for cumulative probabilities" (Agresti, 2010; Agresti, 2013). 

--

Letting `\(\phi(\theta) = (\phi^{1}(\theta),...,\phi^{m-1}(\theta) )\)` with entries `\(\phi^{j} (\theta) = \sum_{r=1}^j \exp \left \{ \sum_{s=1}^{r}  \theta^{s}  \right \}\)`,
&lt;!-- \begin{align*} --&gt;
&lt;!-- \phi^{j} (\theta) = \sum_{r=1}^j \exp \left \{ \sum_{s=1}^{r}  \theta^{s}  \right \} --&gt;
&lt;!-- , --&gt;
&lt;!-- \end{align*} --&gt;
the inverse canonical link is
`\begin{align*}
\tau(\theta) =
\bigg [ -I_{m-1}   - 
\frac{ \{\boldsymbol 1  + e_1 + P \phi(\theta) \} e_1^{\top} } { 1 - e_1^{\top} \{\boldsymbol 1  + e_1 + P \phi(\theta) \}  } 
\bigg ] 
P \phi(\theta) 
=\frac{QP\phi(\theta)}{1 + e_1^{\top} P \phi(\theta)}
,
\end{align*}`
where `\(P\)` is a permutation matrix that maps vectors `\((a^1,...,a^m) \mapsto (a^m, a^1...,a^{m-1})\)`.

---
class: left, top
# Ordinal Response

From `\(S\)`, the density of `\(T\)` is 
`\begin{align*}
f(S;p) 
\propto  &amp; \prod_{j=1}^{m} (p^j)^{S^j} \\
=  &amp;
p^1
\bigg ( \frac{ p^2 }{p^1} \bigg )^{T^1} 
\bigg ( \frac{ p^3 }{p^2} \bigg )^{T^2} 
\bigg ( \frac{ p^4 }{p^3} \bigg )^{T^3} 
\bigg ( \frac{ p^5 }{p^4} \bigg )^{T^4} 
\cdots 
\bigg ( \frac{ p^{m-1} }{p^{m-2}} \bigg )^{T^{m-2} } 
\bigg ( \frac{ p^{m} }{p^{m-1}} \bigg )^{T^{m-1} } 
%(p^m)^{0}  
.
\end{align*}`

The log-likelihood for `\(T\)` is 
`\begin{align*}
\ell(\theta; T)
% = &amp; \sum_{j=1}^{m-1} T^j \log \bigg ( \frac{ p^{j+1} }{p^{j} } \bigg ) + \log ( p^1  ) 
= \theta^{\top} T - [ - \log ( 1 - e_{1}^{\top}  \tau(\theta)  ) ] 
,
\end{align*}`
where the cumulant generating function is `\(b(\theta) = -\log \{ 1 - e_{1}^{\top}  \tau(\theta) \}\)`. 

---
class: left, top
# Ordinal Response

- The Adjacent-Categories Model is not as popular as other alternative link functions for ordinal responses, such as 

  - the Cumulative Logit, `\(\psi(\tau) = \log\{ ( 1 - \tau)/\tau \}\)`
  - the Cumulative Probit, `\(\psi(\tau) = \Phi^{-1} (1 - \tau)\)`
  - the Complementary Log-Log, `\(\psi(\tau) = \log \{ - \log(\tau) \}\)`

- We can estimate `\(\psi\)` using the inverse canonical link function `\(\tau(\theta)\)`.

--


- Since `\(T\)` is a linear exponential family, the derivative of the mean satisfies
`\begin{align*}
\frac{\partial \tau(\theta)}{\partial \theta} 
= 
\frac{\partial^2 b^{-1} (\theta)}{\partial \theta \partial \theta^{\top}} 
= V\{\tau(\theta)\}
= \Gamma(\theta) - \tau(\theta) \tau(\theta)^{\top},
\end{align*}`


- And so we can also estimate the derivative of `\(\psi\)` with respect to `\(\theta\)` using
`\begin{align*}
\frac{\partial \psi(\theta) }{\partial \theta^{\top}} 
= \frac{\partial \psi\{\tau( \theta)\} }{\partial \tau^{\top}} 
\{ \Gamma(\theta) - \tau(\theta) \tau(\theta)^{\top} \}.
\end{align*}`


---
class: inverse, center, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text[Outer Product of Canonical Gradients (OPCG)]  

---
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Generalized Forward Regression for SDR

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="40%"&gt;
]  

- In OPG, we assume that `\(Y\)` depends on `\(X\)` at least through `\(E(Y|X)\)` 
  - we fit a linear regression about `\(\chi\)`, `\(Y = a + B^\top(X-\chi) +                  \varepsilon\)` and estimate `\(\partial E(Y|X)/ \partial \chi^\top\)` using 
    `$$\hat B = \frac{ \widehat{ \partial E(Y|\chi)} }{\partial \chi^\top}.$$`


---
class: left, top 
 
# Generalized Forward Regression for SDR

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="40%"&gt;
]  

- For generalized Forward SDR, we fit a linear exponential family about `\(x_0\)`, where a single observation of the local log-likelihood given by
$$
\ell_0(\theta;y,x,x_0) = \theta(x-x_0)^\top y - b[ \theta(x-x_0) ] 
$$ 



---
class: left, top

# Outer Product of Canonical Gradients (OPCG)

- In OPG, the dimension reduction assumption is `\(E(Y|X) = E(Y|\beta^\top X)\)`

- With GLMs, the dimension reduction assumption is on the canonical parameter         `\(\theta(X) = \theta(\beta^\top X) \in \R^m\)`

  - Since the canonical parameter is the inverse canonical link applied to            `\(E(Y|X)\)`, i.e. `\(\theta(X) = g^{-1}(E(Y|X))\)`, the dimension reduction             assumption coincides with that of OPG.
  
--

- As in OPG, we want to estimate the direction of change in `\(\theta(x)\)` about `\(x_0\)`, i.e. `\(\partial \theta(x_0) /\partial x^\top\)`.

- To achieve this, we fit a linear function to the canonical parameter about `\(x_0\)`, giving the local-linear likelihood
$$
\ell_0(a, B; y,x,x_0) = [a + B^\top(x-x_0)]^\top y - b[ a + B^\top(x-x_0) ] 
$$ 
---
class: left, top

# Outer Product of Canonical Gradients (OPCG)

The local linear log-likelihood for the linear exponential family about `\(\chi\)` is
`\begin{align*}
&amp; l(a_{\chi}, B_{\chi}; \chi, X_{1:n}, Y_{1:n}) \\
= &amp;
\frac 1n \sum_{i=1}^n
K \bigg ( \frac{X_i - \chi}{h} \bigg )
\{[a_{\chi} + B_{\chi}^\top (X_i - \chi)]^\top Y_i - 
b(a_{\chi} + B_{\chi}^\top (X_i - \chi)) \} 
\end{align*}`
where `\(b(\cdot)\)` is the corresponding cumulant generating function, and `\(K(\cdot)\)` is a kernel weight with bandwidth `\(h\)`. 

--

We estimate `\(\partial \theta(\chi) /\partial \chi^\top\)` using
`$$\hat B_\chi = \frac{ \widehat{ \partial \theta(\chi) } }{\partial \chi^\top}.$$`

---
class: left, top

# Outer Product of Canonical Gradients (OPCG)

We estimate a GLM about each `\(\chi=X_j\)` for `\(j=1,...,n\)` by minimizing the full negative local linear log-likelihood:

`\begin{align*}
&amp; L(a_1,..,a_n, B_1,...,B_n; X_{1:n}, Y_{1:n}) \\
= &amp; -\frac 1n \sum_{j=1}^n l(a_j, B_j; X_j, X_{1:n}, Y_{1:n}) \\ 
= &amp; -\frac {1}{n^2} \sum_{j,i=1}^n
K \bigg ( \frac{X_i - X_j}{h} \bigg )\\
&amp; \times 
\{[a_{j} + B_{j}^\top (X_i - X_j)]^\top Y_i - 
b(a_{j} + B_{j}^\top (X_i - X_j)) \} 
.
\end{align*}`

--

&lt;!-- This provides estimates `\(\hat B_1,...\hat B_n\)`. --&gt;

Given our estimates `\(\hat B_j\)`, for `\(j=1,..,n\)`, we construct the average outer product
`$$\hat \Lambda_n = \frac 1n \sum_{j=1}^n \hat B_j \hat B_j^\top.$$` 

---
class: left, top

# The OPCG Estimator

The **Outer Product of Canonical Gradients (OPCG) Estimator** for `\(\beta\)`, `\(\hat \beta_{opcg}\)`, is the first `\(d\)` eigenvectors of 
`$$\hat \Lambda_n = \frac 1n \sum_{j=1}^n \hat B_j \hat B_j^\top,$$` 
corresponding to the `\(d\)` largest eigenvalues.

--

**So far, we have argued by analogy to the OPG case. But is `\(\partial \theta(\chi) /\partial \chi^\top\)` what we should be estimating? Will this work?**




---

# OPCG: Unbiased and Exhaustive 

Some SDR terminology (in statistical functional notation):

- An estimator `\(M(F_n)\)` for a SDR subspace is **unbiased** if `\(M(F_0)\)` is a member of the subspace.
  
- An estimator `\(M(F_n)\)` for a SDR subspace is **exhaustive** if `\(M(F_0)\)` generates the subspace.
  
--
&lt;br/&gt;

OPG is unbiased and, under some conditions, exhaustive for `\(\SS_{E(Y|X)}\)`. (Li, 2018). 




---

# OPCG: Unbiased and Exhaustive 

&lt;div class=prop text="Unbiasedness of the canonical gradient"&gt;

If the canonical parameter satisfies \(\theta(\chi)=\theta(\beta^\top\chi)\), then an estimator for the derivative is unbiased for the Central Mean Subspace, i.e.

$$
\frac{\partial\theta(\chi)}{\partial\chi^{\top}}
\in \SS_{E(Y|X)}
.
$$

&lt;/div&gt;
--

&lt;div class=prop text="Exhaustiveness of the Outer Product"&gt;

If the canonical parameter \(\theta(\beta^\top\chi)\) is convexly supported in \(\beta^\top\chi\), then an estimator for the outer product of the derivative is exhaustive for the Central Mean Subspace, i.e.
$$
span\bigg ( \frac{\partial \theta(\chi)}{\partial \chi^{\top}}\frac{\partial \theta(\chi)^{\top}}{\partial \chi} \bigg ) 
=
\SS_{E(Y|X)}
.
$$
&lt;/div&gt; 



---
class: left, top

# OPCG: Consistency

Our assumptions for Consistency include: 
- compactness of predictor and parameter, 
- smoothness of the likelihood, 
- symmetric kernels with finite moments,
- convergence rates on the bandwidth.  

These assumptions, referred to as Assumptions 1-9, are in an appendix at the end.

--

&lt;div class="theorem" text="Consistency of OPCG"&gt;
  Suppose Assumptions 1-9 hold. Then, as \( n \to \infty\), we have
  \begin{align*} 
  \| \hat \beta_{opcg}  - \beta \|_F = O_{a.s}
  ( h + h^{-1} \delta_{ph} + \delta_{n} )
  ,
  \end{align*}
  &lt;!-- \bigg( h + \sqrt{ \frac{\log n}{ nh^p} } + \sqrt{ \frac{\log n}{ n } } \bigg) --&gt;
  where \(\delta_{ph} = \sqrt{ \frac{\log n}{ nh^p} }\),
  \(\delta_{n} = \sqrt{ \frac{\log n}{ n } }\),
  \(  h \downarrow 0\), and
  \( h^{-1}\delta_{ph} \to 0\).
&lt;/div&gt;

---
class: left, top
# Order Determination

"Order Determination" refers to estimating "$d$", the dimension of the minimal SDR subspace.  

1. An advantage of OPCG is compatibility with recent methods based on eigen variation. We focus on the following methods for OPCG:

  - Ladle Estimator  (Luo and Li, 2016) 
  - Predictor Augmentation (Luo and Li, 2020)




---
class: inverse, center, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text[Improvements to OPCG:]
## Minimum Average Deviance Estimator (MADE), refined OPCG (rOPCG), refined MADE (rMADE) 

---
class: left, top
# Minimum Average Deviance Estimator (MADE)

Recall the dimension reduction assumption: `\(\theta(\chi) = \theta (\beta^\top \chi)\)`. 

Then the derivative satisfies:
$$ \frac{\partial \theta(\chi)}{\partial \chi^{\top}} = \frac{\partial \theta(u(\chi) )}{\partial u^{\top}} \beta^\top $$
where `\(u(\chi)=\beta^\top \chi\)`. 

--

- How can we make use of this property explicitly?

  - What if we substitute into the negative log-likelihood and estimate `\(\beta\)`         directly?

  - In the Linear Regression case with OPG, this method is the Minimum Average          Variance Estimator (MAVE) (Xia, Tong, Li, et al., 2002).
  

---
class: left, top
# The Minimum Average Deviance Estimator (MADE)

We can estimate `\(\beta\)` directly by minimizing the full negative local linear log-likelihood:

`\begin{align*}
&amp; L(\beta, a_1,..,a_n, B_1,...,B_n; X_{1:n}, Y_{1:n}) \\
= &amp; -\frac 1n \sum_{j=1}^n l(a_j, B_j; X_j, X_{1:n}, Y_{1:n}) \\ 
= &amp; -\frac {1}{n^2} \sum_{j,i=1}^n
K \bigg ( \frac{X_i - X_j}{h} \bigg )\\
&amp; \times 
\{[a_{j} + B_{j}^\top \beta^\top (X_i - X_j)]^\top Y_i - 
b(a_{j} + B_{j}^\top \beta^\top (X_i - X_j)) \} 
.
\end{align*}`

--

Minimization can be performed by alternating between two steps until convergence:
1. (MADE-1 step) For fixed `\(\beta\)`, minimize `\(a_1,...,a_n, B_1,...,B_n\)`, 
2. (MADE-2 step) For fixed `\(a_1,...,a_n, B_1,...,B_n\)`, minimize `\(\beta\)`. 

We refer to the resulting estimator `\(\hat \beta_{made}\)` the **Minimum Average Deviance Estimator** (MADE) for `\(\beta\)`.



---
class: left, top
# Refined OPCG (rOPCG) and Refined MADE (rMADE)
 
Once we have the estimators from OPCG and MADE, 
&lt;!-- `\(\hat \beta_{opcg}\)` and `\(\hat \beta_{made}\)`,  --&gt;
we can replace the kernel weights in their respective objective functions by
`\begin{align*}
K[(X - \chi)/h] \mapsto
K[\hat \beta_{opcg}^\top(X - \chi)/h], \\
K[(X - \chi)/h] \mapsto
K[\hat \beta_{made}^\top(X - \chi)/h].
\end{align*}`


--

Then re-estimate `\(\beta\)` using the refined objective functions in the same way, to obtain `\(\hat \beta_{ropcg}\)` and `\(\hat \beta_{rmade}\)`, respectively.

---
class: left, top
# Minimum Average Deviance Estimator (MADE)

- Our definition of MADE differs from Adragni's (2018)

  - Our definition of MADE is analogous to MAVE, while Adragni's definition is our rMADE.

- The alternating minimization for `\(\beta\)` in MADE incurs an additional computational cost relative to OPCG, like MAVE to OPG.

  - But MAVE (i.e. Linear Regression setting) estimates `\(\beta\)` more efficiently. 
  (Xia, 2006; Xia, 2007)

  - We expect MADE to be more efficient compared to OPCG.

---
class: left, top
# Implementation 

For fixed `\(X_j\)`, we make use of vectorization operation, i.e. "vec", to get
`\begin{align*}
a_j + B_j^\top(X-X_j) = 
\left ( 
\left ( \begin{matrix}
1 \\
X-X_j
\end{matrix} \right )
\otimes I_m \right ) 
\left ( \begin{matrix}
a_j \\
{\mathrm{vec} }(B_j^\top)
\end{matrix} \right ) 
\end{align*}`
&lt;!-- = [\nu(X_i - X_j)]^\top c_{j} --&gt;
&lt;!-- where --&gt;
&lt;!-- \begin{align*} --&gt;
&lt;!-- c_j = &amp; --&gt;
&lt;!-- \left ( \begin{matrix} --&gt;
&lt;!-- a_j \\ --&gt;
&lt;!-- {\mathrm{vec} }(B_j^\top) --&gt;
&lt;!-- \end{matrix} \right ) --&gt;
&lt;!-- \in \R^{m(p+1)} --&gt;
&lt;!-- \\ --&gt;
&lt;!-- \nu(X-X_j) = &amp; \left ( --&gt;
&lt;!-- \left ( \begin{matrix} --&gt;
&lt;!-- 1 \\ --&gt;
&lt;!-- X-X_j --&gt;
&lt;!-- \end{matrix} \right ) --&gt;
&lt;!-- \otimes I_m \right ) --&gt;
&lt;!-- \in \R^{m(p+1) \times m} --&gt;
&lt;!-- \end{align*} --&gt;
--

This reformulates OPCG and MADE-1 into a local-linear GLM:

  - Under identifiability assumptions, the objective functions are strictly convex


--

For fixed `\(X_j\)` in MADE-2, vectorization gives
`\begin{align*}
a_j + B_j^\top\beta^\top(X-X_j) = 
a_j + [ (X - X_j)^\top \otimes B_j^\top ]^\top \mathrm{vec}(\beta^\top)
,
\end{align*}`

--

This reformulates MADE-2 into a convex objective:

  - Without identifiability of `\(\beta\)`, we lose strict convexity in the objective

---
class: left, top
# Implementation

For OPCG and MADE, we can use Fisher-Scoring/Newton-Raphson:

  - OPCG and MADE-1, we have good initial values; 
  
    - but still slow for `\(p \approx 20\)` and `\(n \approx 1000\)`; 

--

  - For MADE-2, we don't have a systematic approach finding initial values for `\(\beta\)`, other than using the OPCG estimate, `\(\hat \beta_{opg}\)`; 
   
    - Estimation is extraordinarily slow for MADE-2; 
    
    - With OPCG as the initial value, only a few iterations are needed.
    
--

We implement a Conjugate Gradient Algorithm written in Rcpp

  - Works very well for OPCG and MADE-1, which can be computed in parallel 

  - Still slow for MADE-2, but much faster than Newton

&lt;!-- --- --&gt;
&lt;!-- class: left, top --&gt;
&lt;!-- # Implementation/Algorithm --&gt;
&lt;!-- ## OPCG  --&gt;



&lt;!-- The negative local linear log-likelihood about `\(X_j\)` becomes --&gt;
&lt;!-- \begin{align*} --&gt;
&lt;!-- &amp; l(a_{j}, B_{j}; X_j, X_{1:n}, Y_{1:n}) \\ --&gt;
&lt;!-- = &amp; --&gt;
&lt;!-- \frac 1n \sum_{i=1}^n --&gt;
&lt;!-- K \bigg ( \frac{X_i - X_j}{h} \bigg ) --&gt;
&lt;!-- \{c_{j}^\top [\nu(X_i - X_j)] Y_i -  --&gt;
&lt;!-- b([\nu(X_i - X_j)]^\top c_{j} \}  --&gt;
&lt;!-- \end{align*} --&gt;


&lt;!-- --- --&gt;
&lt;!-- class: left, top --&gt;
&lt;!-- # Implementation/Algorithm --&gt;
&lt;!-- ## OPCG  --&gt;

&lt;!-- 0. Given `\((Y_{1:n},X_{1:n})\)`, initial values `\(c_{1:n}^{(0)}\)`, and tolerance level `\(\varepsilon &gt; 0\)`. --&gt;

&lt;!-- For each `\(j=1,...,n\)` --&gt;
&lt;!-- 1. Substitute `\(\hat c^{(r)}\)` into the objective , --&gt;

&lt;!-- 2. and compute --&gt;
&lt;!-- 	`\(\hat \a_j^{(r+1)} = \hat \a_j^{(r)} + J_j^{-1} (\hat \a_j^{(r)} ) S_j(\hat \a_j^{(r)})\)`; --&gt;
&lt;!-- 	Repeat until $\frac{ || \hat \a_j^{(r+1)} - \hat \a_j^{(r+1)} ||_2 }{ ||\hat \a_j^{(r)} ||_2 } &lt; \vep $ and denote the final estimate by `\(\hat \a_j^*\)`;  --&gt;
&lt;!-- 	Construct matrix estimate `\(\hat D_j\)` as the `\(2,...,p+1\)` rows of the matrix $ \hat A_j = mat(\hat \a_j^*) \in \R^{(p+1) \times m}$, for all `\(j=1,...,n\)`;   --&gt;
&lt;!-- 	Form $\hat G = \frac 1n \sum_{j=1}^n  \hat D_j \hat D_j^{\top} $ ;   --&gt;
&lt;!-- 	Take first `\(d\)` eigenvectors of `\(\hat G\)`, `\(v_1,...,v_d\)`, to be the estimates for dimension reduction directions `\(\hat B_{OPCG} = (v_1,....,v_d)\)` ; --&gt;

&lt;!-- --- --&gt;
&lt;!-- class: left, top --&gt;
&lt;!-- # Implementation/Algorithm --&gt;
&lt;!-- ## MADE  --&gt;

&lt;!-- --- --&gt;
&lt;!-- class: left, top --&gt;
&lt;!-- # Implementation/Algorithm --&gt;
&lt;!-- ## MADE  --&gt;

---
class: left, top
# Order Determination

"Order Determination" refers to estimating "$d$", the dimension of the minimal SDR subspace.  

1. An advantage of OPCG is compatibility with existing methods based on eigen variation. We focus on the following methods for OPCG:

  - Ladle Estimator  (Luo and Li, 2016) 
  - Predictor Augmentation (Luo and Li, 2020)

--

2. For MADE, the standard cross-validation methods can be applicable from MAVE (Xia, Tong, Li, et al., 2002)
&lt;!-- and Adragni's MADE (Adragni, 2018)   --&gt;






---
class: inverse, center, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text[Tuning]


---
class: left, top
# Tuning the bandwidth, `\(h\)`.
 
We need to tune `\(h\)` in OPCG (and MADE):

  - But Cross Validation can be computationally intensive and relative performance may vary between prediction methods. 

  - Can choose `\(h\)` to be the optimal bandwidth, such as `\(h^{opt} = cn^{-\frac{1}{p^{opt}}}\)`, but choosing the proportional constant `\(c\)` requires tuning when recommended values do not work.

--

For classification problems, we propose using a K-means clustering procedure for tuning `\(h\)`.

Let `\(Y \in \{1,...,m\}\)` be categorical response, `\((Y, X)_{1:n}\)` be our sample predictors for estimation, `\((Y,X)_{1:n_2}^{tune}\)` be our sample predictors for Tuning.

---
class: left, top
# Tuning the bandwidth, `\(h\)`.



Main idea: For each `\(h\)`, 

1. Estimate `\(\hat \beta_{opcg}\)` on `\((Y,X)_{1:n}\)` and construct the sufficient predictors `\(\hat \beta_{opcg}^\top X_{1:n_2}^{tune}\)`.

2. Apply K-means to `\(\hat \beta_{opcg}^\top X_{1:n_2}^{tune}\)` for `\(m\)` clusters.

  - This returns `\(m\)` estimated clusters and the F-ratio corresponding to `\(h\)`, i.e. the Within Sums-of-Squares (WSS) over Between Sums-of-Squares (BSS)

3. Select `\(h\)` that minimizes the F-ratio from K-means. 

--

But ...

- We assume that there are only `\(m\)` clusters; i.e. 1 cluster per class.

- We ought to incorporate the class/label information from `\(Y_{1:n}^{tune}\)`, when available.



---
class: left, top
# Tuning the bandwidth, `\(h\)`.

Improvements:

1. For K-means, instead of estimating `\(m\)` clusters, we can estimate `\(M_1+\cdots+M_m\)` clusters, where `\(M_j\)` are the number of clusters for class `\(j\)`.

  - We set `\(M_j = M \in \{1,2,3\}\)` in our examples.
  
  - Our procedure seems robust to the number of clusters per class.  
  
  - Still does not incoporate information from `\(Y_{1:n_2}^{tune}\)`.

---
class: left, top
# Tuning the bandwidth, `\(h\)`.

Improvements:


2. For each `\(h\)`, for each class/label$m$, 

  a. Estimate `\(\hat \beta_{opcg}\)` using `\((Y,X)_{1:n}\)` and construct the sufficient predictors `\(\hat \beta_{opcg}^\top X_{1:n_2}^{tune}\)`.

  b. Apply K-means to `\(\hat \beta_{opcg}^\top X_{1:n_2}^{tune}\)` to get estimated clusters and corresponding  WSS and  BSS

2. Compute the "Supervised WSS" (SWSS) and "Supervised BSS" (SBSS) as the aggregate WSS and BSS over all classes, respectively. 

3. Select `\(h\)` that minimizes the "supervised F-ratio" `\(\frac{SWSS}{SBSS}\)`.

--

But: We ought to incorporate the label information available.


&lt;!-- --- --&gt;
&lt;!-- class: left, top --&gt;
&lt;!-- # Alternative approaches --&gt;

&lt;!-- 1. Generalized Single Index Model (GSIM): --&gt;

&lt;!--   - Estimates the conditional mean directly;  --&gt;
&lt;!--   - Applies GSIM to each label of the response;   --&gt;
&lt;!--   - Applies ridge penalty on the gradient of the conditional mean;  --&gt;

&lt;!-- -- --&gt;

&lt;!-- 2. Adragni's MADE: --&gt;

&lt;!--   - Only available for scalar `\(Y\)`; --&gt;
&lt;!--   - Restricted to binary classification; suggests pairwise computations over all labels --&gt;

&lt;!-- -- --&gt;

&lt;!-- 3. gradient Kernel Dimension Reduction (gKDR): --&gt;

&lt;!--   - Same idea of using an eigen-decomposition of the outer product of estimated local gradients  --&gt;
&lt;!--   - local gradients of the conditional mean are estimated via RKHS --&gt;

---
class: inverse, center, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text[Simulations and Data Analyses]


---
class: left, top
# Simulations


---
class: ani-slide
# Tuning HTML Widget
 
&lt;iframe src="images/tuning_sc.html" width="100%" height="95%" frameborder="0" &gt;&lt;/iframe&gt;

count: false
class: left, top
# Wine Quality

---
class: left, top
# USPS Digit

---
count: false
class: left, top
# USPS Digit
 
---
class: left, top
# ISOLET

---
count: false
class: left, top
# ISOLET


---
class: left, top
# Red Wine Quality

Input variables (based on physicochemical tests):
&lt;!-- 1 - fixed acidity --&gt;
&lt;!-- 2 - volatile acidity --&gt;
&lt;!-- 3 - citric acid --&gt;
4 - residual sugar 
&lt;!-- 5 - chlorides --&gt;
&lt;!-- 6 - free sulfur dioxide --&gt;
7 - total sulfur dioxide
&lt;!-- 8 - density --&gt;
&lt;!-- 9 - pH --&gt;
10 - sulphates
11 - alcohol

# Output variable (based on sensory data): 
12 - quality (score between 0 and 10)


---
class: left, top
# Boston Housing Prices

Input variables (based on physicochemical tests):
&lt;!-- 1 - fixed acidity --&gt;
&lt;!-- 2 - volatile acidity --&gt;
&lt;!-- 3 - citric acid --&gt;
4 - residual sugar 
&lt;!-- 5 - chlorides --&gt;
&lt;!-- 6 - free sulfur dioxide --&gt;
7 - total sulfur dioxide
&lt;!-- 8 - density --&gt;
&lt;!-- 9 - pH --&gt;
10 - sulphates
11 - alcohol

# Output variable (based on sensory data): 
12 - quality (score between 0 and 10)
---

---
class: left, top, inverse
# .bg-text[Conclusion]


1. Provided the Multivariate Link Functions for Categorical and Ordinal Responses.

--

2. Provided a framework for generalized forward SDR: OPCG, MADE. 

  - And applied it to categorical and ordinal responses using the multivariate link functions. 

--

3. Introduced a K-means based tuning procedure.

--

4. Demonstrated the effectiveness of OPCG in categorical and ordinal classification problems.

  - Can handle multiple labels simultaneously
  
  - Competitive with some contemporary categorical and ordinal classification methods.


--





---
layout: false
# References

Adragni, K. P. (2018). "Minimum average deviance estimation for
sufficient dimension reduction". In: _Journal of Statistical
Computation and Simulation_ 88.3, pp. 411-431.

Agresti, A. (2010). _Analysis of ordinal categorical data_. Vol. 656.
John Wiley &amp; Sons.

Agresti, A. (2013). _Categorical Data Analysis_. 3rd ed. Wiley.

Cook, R. D. and B. Li (2002). "Dimension reduction for conditional mean
in regression". In: _The Annals of Statistics_ 30.2, pp. 455-474.

Fukumizu, K. and C. Leng (2014). "Gradient-based kernel dimension
reduction for regression". In: _Journal of the American Statistical
Association_ 109.505, pp. 359-370.

Lambert-Lacroix, S. and J. Peyre (2006). "Local likelihood regression
in generalized linear single-index models with applications to
microarray data". In: _Computational statistics &amp; data analysis_ 51.3,
pp. 2091-2113.

---
layout: false
# References

Li, B. (2018). _Sufficient Dimension Reduction: Methods and
Applications with R_. CRC Press.

Li, K. (1991). "Sliced inverse regression for dimension reduction". In:
_Journal of the American Statistical Association_ 86.414, pp. 316-327.

Luo, W. and B. Li (2016). "Combining eigenvalues and variation of
eigenvectors for order determination". In: _Biometrika_ 103.4, pp.
875-887.

Luo, W. and B. Li (2020). "On order determination by predictor
augmentation". In: _Biometrika_.

Wang, H. and Y. Xia (2008). "Sliced regression for dimension
reduction". In: _Journal of the American Statistical Association_
103.482, pp. 811-821.

Xia, Y. (2007). "A constructive approach to the estimation of dimension
reduction directions". In: _The Annals of Statistics_ 35.6, pp.
2654-2690.

---
layout: false
# References

Xia, Y. (2006). "Asymptotic distributions for two estimators of the
single-index model". In: _Econometric Theory_ 22.6, pp. 1112-1137.

Xia, Y., H. Tong, W. Li, et al. (2002). "An adaptive estimation of
dimension reduction space". In: _Journal of the Royal Statistical
Society: Series B (Statistical Methodology)_ 64.3, pp. 363-410.

Yin, X., B. Li, and R. D. Cook (2008). "Successive direction extraction
for estimating the central subspace in a multiple-index regression".
In: _Journal of Multivariate Analysis_ 99.8, pp. 1733-1757.

Yin, X., B. Li, and others (2011). "Sufficient dimension reduction
based on an ensemble of minimum average variance estimators". In: _The
Annals of Statistics_ 39.6, pp. 3392-3416.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
