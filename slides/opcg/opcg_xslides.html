<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Generalized Forward Sufficient Dimension Reduction for Classification</title>
    <meta charset="utf-8" />
    <meta name="author" content="Harris Quach" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="mytheme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: { 
      independenT: ["{\\mathrel{\\rlap{ #1 }\\mkern2mu{ #1 }}}",1],
      indep: "{\\independenT{\\perp}}",
      SS: "{\\mathscr{S}}",
      R:"{\\mathbb{R}}",
      Xcal: "{\\mathcal{X}}",
      water: "{H_2O}",
      braket: ['{\\langle #1 \\rangle}', 1], 
      Abs: ['\\left\\lvert #2 \\right\\rvert_{\\text{#1}}', 2, ""]
    }
  }
});
</script>




 

&lt;!-- class: title-slide --&gt;

# Generalized Forward Sufficient Dimension Reduction for Categorical and Ordinal Responses
&lt;!-- # .bg-text[Generalized Forward Sufficient Dimension Reduction for Classification] --&gt;
&lt;!-- &lt;hr width="700" align="left" /&gt; --&gt;
&lt;hr/&gt;
## Harris Quach (joint work with Dr. Bing Li) &lt;br/&gt; Date: 2021-03-05

---
# Overview

1. Multivariate Categorical and Ordinal Link functions for GLMs

  - Categorical Variable
  - Ordinal-Categorical Variable

2. Motivation
  - Inverse Regression for SDR
  - Forward Regression for SDR
  
3. Outer Product of Canonical Gradients (OPCG) 
  
4. Tuning the bandwidth via K-means 

5. Simulations and Data Analysis
  - Simulations
  - Ordinal Data 
  - Categorical Data  

---
class: inverse, center, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text[Multivariate Link functions]  


---
class: left, top
# Multivariate Links for Linear Exponential Families

The crux of our proposed method is fitting multivariate Generalized Linear Model to categorical or ordinal response variables.

- Categorical and Ordinal responses, `\(Y\)`, can be derived from a multinomial distribution.

- Multinomials are a linear exponential family, with log-likelihood of the form

`\begin{align*}
\ell(\theta;y) = \theta^\top Y - b(\theta);   
\end{align*}`

- They are characterized by their means `\(\mu\)`, through the canonical parameter `\(\theta\)`, 

- The canonical link function `\(\theta( \cdot ): \mu \mapsto \theta\)`; the inverse canonical link maps `\(\theta\)` to `\(\mu\)` via `\(\mu(\cdot): \theta \mapsto \mu\)`

---
class: left, top
# Multivariate Links for Linear Exponential Families

  - Inverse canonical link is useful for 
    
    - determining the function `\(b(\cdot)\)` that specifies the linear exponential family
    
    - evaluating the Mean and Variance as functions of the canonical parameter
      
    - estimate parameters for alternative link functions

  - Canonical link is useful for constructing starting values for optimization algorithms via the projection
  
`\begin{align*}
X(X X^\top)^{-1} X \theta(Y)     
\end{align*}`


- So we show explicitly the categorical and ordinal-categorical variables are linear exponential families. 

---
class: left, top
# Categorical Response

Suppose `\(Y \in \{1,...,m\}\)` is a categorical variable for `\(m\)` nominal categories. 

- We can represent `\(Y\)` as a vector `\(S = (S^1,...,S^{m-1}) \in \{0,1\}^{m-1}\)`, where we set `\(S^m = 1 - \sum_{j=1}^{m-1} S^m\)`.

  - If `\(Y=k\)`, then `\(S^k = 1\)` and `\(S^j=0\)` for `\(j \neq k\)`. 

  - Eg. `\(m=3\)`; if `\(Y = 2\)`, then `\(S = (0,1)\)`; if `\(Y=3\)`, then `\(S=(0,0)\)`. 

--

Let `\(p = (p^1,...p^{m-1})\)` be the `\(m-1\)` vector of probabilities for each category. 

Then
`\begin{align*}
E(S) =  p , 
\quad 
Var(S)
= Diag(p) - p p^\top
%\left [ Diag \left ( \frac{  e^{\theta   } }{ 1 + \boldsymbol 1^{\top}e^{\theta } } \right ) - \frac{ e^{\theta }  (e^{\theta }  )^{\top} }{ [1 + \boldsymbol 1^{\top}e^{\theta} ]^2 }  \right ]
,
\end{align*}`


---
class: left, top
# Categorical Response

  - The canonical link and its inverse is the multivariate logistic link and its inverse:
  
`\begin{align*}
\theta(p) = \log \frac{ p }{ 1 - \boldsymbol{1}^{\top} p }
\qquad
p(\theta) =  \frac { \exp(\theta) } {  1 - \boldsymbol{1}^{\top} \exp( \theta) } 
\end{align*}`

The density and log-likelihood of `\(S\)` are 
`\begin{align*}
f(S;p) \propto \prod_{j=1}^{m-1} (p^j)^{S^j},
\quad  
\ell(\theta;S) = \theta^{\top}S - \log ( 1 - \boldsymbol 1^{\top}  e^\theta   )
,
\end{align*}`
where 
`\(b(\theta) = \log( 1 - \boldsymbol{1}^{\top} e^{  \theta }  )\)`.


---
class: left, top
# Ordinal Response

Suppose `\(Y \in \{1,...,m\}\)` is an ordinal-categorical variable for `\(m\)` ordered categories.  

- We can represent `\(Y\)` as a vector `\(S = (S^1,...S^{m-1}) \in \{0,1\}^{m}\)` as for categorical `\(Y\)`. 

- We can represent `\(S\)` as a vector `\(T = (T^1,...T^{m-1}) \in \{0,1\}^{m-1}\)`, and we set `\(T^{m} = 0\)` and `\(T^0=1\)`. 
  
  - If `\(Y = k\)`, then `\(T^j = 1\)` for `\(j \leq {k-1}\)` and `\(T^j=0\)` for `\(j &gt; k-1\)`. 

  - Eg. `\(m=5\)`; if `\(Y=3\)`, then `\(T=(1,1,0,0)\)`; if `\(Y=1\)`, then `\(T=(0,0,0,0)\)`


---
class: left, top
# Ordinal Response

Let 

 - `\(p = (p^1,...p^{m-1})\)` be the `\(m-1\)` vector of probabilities,
 - `\(\gamma = (p^1, p^1 + p^2, ..., p^1 + \cdots + p^{m-1})\)` be the `\(m-1\)` vector of cumulative probabilities, 
 - `\(\tau = 1 - \gamma\)`. 

--

The mean and variance of `\(T\)` is
`\begin{align*}
E(T) = \tau, \quad 
Var(T) = \Gamma - \tau  \tau^{\top} ,
\end{align*}`
where
`\begin{align*}
\Gamma = \left ( \begin{matrix}
\tau^{1}  &amp; \tau^{2}  &amp; \cdots &amp; \tau^{m-2}&amp; \tau^{m-1} \\
\tau^{2}  &amp; \tau^{2}  &amp; \cdots &amp; \tau^{m-2} &amp; \tau^{m-1} \\
\vdots &amp; \vdots  &amp; \cdots  &amp; \cdots &amp; \vdots \\
\tau^{m-2} &amp; \tau^{m-2} &amp; \cdots &amp; \tau^{m-2}  &amp; \tau^{m-1} \\
\tau^{m-1} &amp; \tau^{m-1}  &amp; \cdots &amp; \tau^{m-1} &amp; \tau^{m-1 } \end{matrix} \right )
\end{align*}`

---
class: left, top
# Ordinal Response

We define the canonical parameter as `\(\theta(\tau) = (\theta(\tau)^1,...,\theta(\tau)^{m-1} )\)`, with
`\begin{align*}
\theta(\tau)^j
=
\log \bigg ( \frac{  \tau^{j} - \tau^{j+1} }{  \tau^{j-1} - \tau^{j}  } \bigg )
=
\log \bigg ( \frac{ p^{j+1} }{p^{j} } \bigg )
%= 
%\log \bigg ( \frac{ \gamma^{j+1} - \gamma^{j} }{ \gamma^{j} - \gamma^{j-1}  } \bigg ) 
.
\end{align*}`

This canonical link corresponds to the "Adjacent-Categories Logit Model for cumulative probabilities" (Agresti, 2010; Agresti, 2013). 

--

Let `\(P\)` a permutation matrix that maps `\((a^1,...,a^m) \mapsto (a^m, a^1...,a^{m-1})\)`. Then 

`\begin{align*}
\theta(\tau) 
=
\log \left\{ \frac{ (I-P) \tau }{ (P^{-1} - I)\tau } \right \}.
\end{align*}`

--

The Adjacent-Categories Model is usually re-expressed as a logistic multinomial in practice. 

Instead, we derive a closed form for the inverse link and `\(b(\cdot)\)` function to explicitly show that categorical-ordinal variable `\(T\)` is distributed according to a linear exponential family.  


---
class: left, top
# Ordinal Response

&lt;!-- Let `\(\phi(\theta) = (\phi^{1}(\theta),...,\phi^{m-1}(\theta) )\)` with `\(\phi^{j} (\theta) = \sum_{r=1}^j \exp \left \{ \sum_{s=1}^{r}  \theta^{s}  \right \}\)`. --&gt;

  &lt;!-- - Then `\(\phi(\theta) = L \exp (L \theta )\)`, with `\(L\)` a lower triangular matrix of `\(1\)`'s.  --&gt;

Let `\(L\)` a lower triangular matrix of `\(1\)`'s.
The inverse canonical link is
`\begin{align*}
\tau(\theta) =
\bigg [ -I_{m-1}   - 
\frac{ \{\boldsymbol 1  + e_1 + P L \exp (L \theta ) \} e_1^{\top} } { 1 - e_1^{\top} \{\boldsymbol 1  + e_1 + P L \exp (L \theta ) \}  } 
\bigg ] 
P L \exp (L \theta ) 
.
\end{align*}`

--

Letting `\(Q = \{-I_{m-1} + \boldsymbol 1e_1^\top  + e_1 e_1^\top \}\)`, the inverse canonical link is
`\begin{align*}
\tau(\theta)
=\frac{Q P  L \exp (L \theta ) }{1 + e_1^{\top} P   L \exp (L \theta ) }
,
\end{align*}`

--

Since `\(S^j = T^{j+1} - T^j\)`, from the density of `\(S\)`, 
the log-likelihood for `\(T\)` is 
`\begin{align*}
\ell(\theta; T)
= \theta^{\top} T - \log ( 1 + e_{1}^{\top}  P   L \exp (L \theta ) )  
,
\end{align*}`
with `\(b(\theta) = \log ( 1 + e_{1}^{\top}  P   L \exp (L \theta ) )\)`. 

--

We say `\(T\)` has a **ordinal-categorical (Or-Cat)** distribution.
The ordinal-categorical distribution is a linear exponential family with the **adjacent-categories (Ad-Cat)** link function. 


&lt;!-- --- --&gt;
&lt;!-- class: left, top --&gt;
&lt;!-- # Ordinal Response --&gt;

&lt;!-- Since `\(S^j = T^{j+1} - T^j\)`, from the density of `\(S\)`,  --&gt;
&lt;!-- the density of `\(T\)` is  --&gt;
&lt;!-- \begin{align*} --&gt;
&lt;!-- f(S;p)  --&gt;
&lt;!-- \propto  &amp; \prod_{j=1}^{m} (p^j)^{S^j} \\ --&gt;
&lt;!-- =  &amp; --&gt;
&lt;!-- p^1 --&gt;
&lt;!-- \bigg ( \frac{ p^2 }{p^1} \bigg )^{T^1}  --&gt;
&lt;!-- \bigg ( \frac{ p^3 }{p^2} \bigg )^{T^2}  --&gt;
&lt;!-- \bigg ( \frac{ p^4 }{p^3} \bigg )^{T^3}  --&gt;
&lt;!-- \bigg ( \frac{ p^5 }{p^4} \bigg )^{T^4}  --&gt;
&lt;!-- \cdots  --&gt;
&lt;!-- \bigg ( \frac{ p^{m-1} }{p^{m-2}} \bigg )^{T^{m-2} }  --&gt;
&lt;!-- \bigg ( \frac{ p^{m} }{p^{m-1}} \bigg )^{T^{m-1} }  --&gt;
&lt;!-- %(p^m)^{0}   --&gt;
&lt;!-- . --&gt;
&lt;!-- \end{align*} --&gt;
&lt;!-- the log-likelihood for `\(T\)` is  --&gt;
&lt;!-- \begin{align*} --&gt;
&lt;!-- \ell(\theta; T) --&gt;
&lt;!-- % = &amp; \sum_{j=1}^{m-1} T^j \log \bigg ( \frac{ p^{j+1} }{p^{j} } \bigg ) + \log ( p^1  )  --&gt;
&lt;!-- = \theta^{\top} T - \log ( 1 + e_{1}^{\top}  P   L \exp (L \theta ) )   --&gt;
&lt;!-- , --&gt;
&lt;!-- \end{align*} --&gt;
&lt;!-- where `\(b(\theta) = \log ( 1 + e_{1}^{\top}  P   L \exp (L \theta ) )\)`.  --&gt;

&lt;!-- -- --&gt;

&lt;!-- We say `\(T\)` has a *ordinal-categorical (Or-Cat)* distribution.  --&gt;
&lt;!-- The ordinal-categorical distribution is a linear exponential family with the *adjacent-categories (Ad-Cat)* link function.  --&gt;


---
class: left, top
# Ordinal Response

- The Ad-Cat link is not as popular as other alternative link functions, `\(\psi\)`, for the cumulative probabilities, such as 

  - the Cumulative Logit, `\(\psi(\tau) = \log\{ ( 1 - \tau)/\tau \}\)`
  - the Cumulative Probit, `\(\psi(\tau) = \Phi^{-1} (1 - \tau)\)`
  - the Complementary Log-Log, `\(\psi(\tau) = \log \{ - \log(\tau) \}\)`

- We can estimate `\(\psi\)` using the inverse canonical link function `\(\hat \psi =\psi\{ \tau(\hat \theta) \}\)`.

--


- Since `\(T\)` is a linear exponential family,
`\begin{align*}
\frac{\partial \tau(\theta)}{\partial \theta} 
= 
\frac{\partial^2 b^{-1} (\theta)}{\partial \theta \partial \theta^{\top}} 
= V\{\tau(\theta)\}
= \Gamma(\theta) - \tau(\theta) \tau(\theta)^{\top},
\end{align*}`


- We can estimate the derivative of `\(\psi\)` via chain rule
`\begin{align*}
\frac{\partial \psi(\hat \theta) }{\partial \theta^{\top}} 
= \frac{\partial \psi\{\tau( \hat \theta)\} }{\partial \tau^{\top}} 
\{ \Gamma( \hat \theta) - \tau( \hat \theta) \tau( \hat \theta)^{\top} \}.
\end{align*}`

---
class: top, left
# Summary so far:

  - Categorical variables are a linear exponential family
  
  - Ordinal-Categorical variables are also a linear exponential family
  
    - The Ad-Cat link and the inverse Ad-Cat link are
`\begin{align*}
\theta
= \log \left\{ \frac{ (I-P) \tau }{ (P^{-1} - I)\tau } \right \},
\qquad
\tau(\theta)
=\frac{Q P  L \exp (L \theta ) }{1 + e_1^{\top} P   L \exp (L \theta ) }
,
\end{align*}`

    - `\(b(\theta) = \log ( 1 + e_{1}^{\top}  P   L \exp (L \theta ) )\)`. 

  - From the Or-Cat distribution, we can estimate any alternative link `\(\psi\)` and its derivatives `\(\partial \psi (\theta) / \partial \theta\)` using the inverse Ad-Cat link `\(\tau(\theta)\)`.
  
---
class: inverse, center, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text[Inverse and Forward Linear SDR]  

---
class: left, top
# Motivating Example: 

Given response `\(Y\)` and predictor `\(X = (X_1, X_2) \in [0,1]^2\)`. 
Let `\(Y=X_1^2\)`.
Then `\(Y= (\beta^\top X)^2\)`, where `\(\beta = (1,0) \in \R^2\)`. 
- we want to recover `\(span(\beta) = \{ (c,0): c \in \R\}\)`.
.center[
&lt;img align="centered" class="image" src="images/sdr_plot.png" width="50%"&gt;
]



---
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Sliced Inverse Regression (Li, 1991)

.center[
&lt;img align="centered" class="image" src="images/sir_plot1.png" width="55%"&gt;
]

&lt;!-- &lt;iframe src="images/almost_sir.html" width="90%" height="90%" frameborder="0"&gt;&lt;/iframe&gt; --&gt;


---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Sliced Inverse Regression (Li, 1991)

.center[
&lt;img align="centered" class="image" src="images/sir_plot2.png" width="55%"&gt;
]

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Sliced Inverse Regression (Li, 1991)

.center[
&lt;img align="centered" class="image" src="images/sir_plot3.png" width="55%"&gt;
]

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Sliced Inverse Regression (Li, 1991)

.center[
&lt;img align="centered" class="image" src="images/sir_plot4.png" width="55%"&gt;
]

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Sliced Inverse Regression (Li, 1991)

.center[
&lt;img align="centered" class="image" src="images/sir_plot5.png" width="55%"&gt;
]
--

  - 'Inverse' because we estimate `\(E(X|Y)\)`.

---
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Drawbacks

.center[
&lt;img align="centered" class="image" src="images/sir_plot5.png" width="55%"&gt;
]

---
count: false
class: left, top  

# Motivating Example: 
## Inverse Regression for SDR - Drawbacks

.center[
&lt;img class="image" src="images/bad_sir_plot1.png" width="55%"&gt;
] 

---
count: false
class: left, top  

# Motivating Example: 
## Inverse Regression for SDR - Drawbacks

.center[
&lt;img class="image" src="images/bad_sir_plot2.png" width="55%"&gt;
] 

---
count: false
class: left, top  

# Motivating Example: 
## Inverse Regression for SDR - Drawbacks

.center[
&lt;img class="image" src="images/bad_sir_plot3.png" width="55%"&gt;
] 
--
- Inverse methods require assumptions on the support of the predictor.

---
class: left, top  

# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, and Zhu, 2002)

.center[
&lt;img class="image" src="images/fsdr_plot1.png" width="55%"&gt;
] 

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)


.center[
&lt;img class="image" src="images/fsdr_plot2.png" width="55%"&gt;
] 

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)
 

.center[
&lt;img class="image" src="images/fsdr_plot3.png" width="55%"&gt;
] 



---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)
 

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="55%"&gt;
] 

--
- "Forward" Regression because we are estimating `\(E(Y|x)\)` and `\(\partial E(Y|x)/\partial x^\top\)`

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)


.center[
&lt;img class="image" src="images/fsdr_plot4.png" width="55%"&gt;
] 

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)


.center[
&lt;img class="image" src="images/fsdr_plot5.png" width="55%"&gt;
]  

 
---
class: left, top
# Forward Regression for SDR 

&lt;!-- - OPG can only preserve information on the regression relation  --&gt;
&lt;!-- recover `\(\SS_{E(Y|X)}\)` --&gt;
  
&lt;!--   - Extensions exist to recover `\(\SS_{Y|X}\)`  --&gt;

&lt;!--     - dOPG (Xia, 2007);  --&gt;
&lt;!--       Ensemble OPG (Yin, Li, and others, 2011);  --&gt;
&lt;!--       Sliced Regression (Wang and Xia, 2008) --&gt;

&lt;!-- -- --&gt;


- OPG developed for scalar `\(Y\)`; does not work as well for categorical `\(Y\)`.

- Idea: Estimate a GLM locally instead of a Linear Regression.
&lt;!-- &lt;br/&gt;&lt;br/&gt; --&gt;

--

Existing extensions of Forward SDR:

1. Generalized Single Index Model (GSIM): Lambert-Lacroix and Peyre (2006)
  - Local Linear GLM for scalar Y; 
  - Uses Average Derivative of Conditional Mean; more similar to Average Derivative Estimator (ADE) than OPG; 
  - ADE has known drawbacks; e.g. gradient has non-zero mean. 

2. Minium Average Deviance Estimation (MADE): Adragni (2018)
  - Local Linear GLM for scalar `\(Y\)` to generalize the Minimum Average Variance Estimator of Xia, Tong, Li, et al. (2002) 
  
3. gradient Kernel Dimension Reducion (gKDR): Fukumizu and Leng (2014)
  - Estimate the gradients via spline approximation using RKHS


---
class: inverse, center, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text[Outer Product of Canonical Gradients (OPCG)]  

---
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Generalized Forward Regression for SDR

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="40%"&gt;
]  

- In OPG, we fit a linear regression about `\(x_0\)`, i.e. we minimize
`\begin{align*}
\{Y - a_0 + B_0^\top(X-x_0) \}^\top \{Y - a_0 + B_0^\top(X-x_0) \} 
\end{align*}`
and estimate `\(\partial E(Y|x_0)/ \partial x^\top\)` using `\(\hat B_0\)`. 
    &lt;!-- `$$\hat B_0 = \frac{ \widehat{ \partial E(Y|x_0)} }{\partial x^\top}.$$` --&gt;


---
class: left, top 
 
# Generalized Forward Regression for SDR

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="40%"&gt;
]  

- Instead, fit a linear exponential family about `\(x_0\)`, i.e. minimize
`\begin{align*}
  -\ell_0(a_0, B_0;Y,X,x_0)  = - [a_0 + B_0^\top(x-x_0)]^\top y + b[a_0 + B_0^\top(x-x_0)]  
\end{align*}`
and use the estimate `\(\hat B_0\)`. 

---
class: left, top

# Outer Product of Canonical Gradients (OPCG)

  - In OPG, the dimension reduction assumption is `\(E(Y|X) = E(Y|\beta^\top X)\)`
  
  - In GLMs, 
  
    - the predictor `\(X\)` relates to `\(Y\)` through the canonical parameter `\(\theta(X)\)`
  
    - the dimension reduction assumption is on the canonical parameter, i.e. `\(\theta(X) = \theta(\beta^\top X) \in \R^m\)`
  
      - But for linear exponential families, `\(\theta(X) = link^{-1}(E(Y|X))\)`, so the dimension reduction assumption coincides with that of OPG.
  
--

- Like in OPG, we want to estimate the gradient of `\(\theta(x)\)` at `\(x_0\)`, i.e. `\(\partial \theta(x_0) /\partial x^\top\)`.

- `\(\hat B_0\)` obtained from
`\begin{align*}
-\ell_0(a_0, B_0;y,x,x_0) = -[a_0 + B_0^\top(x-x_0)]^\top y + b[a_0 + B_0^\top(x-x_0)]
\end{align*}`
estimates `\(\partial \theta(x_0) /\partial x^\top\)`.


---
class: left, top

# Outer Product of Canonical Gradients (OPCG)

Given a random sample `\(Y_{1:n}\)`, `\(X_{1:n}\)`, fit a local linear log-likelihood about `\(x_0\)` by minimizing
`\begin{align*}
&amp; -\ell_0 (a_{0}, B_{0}; x_0, X_{1:n}, Y_{1:n}) \\
= &amp;
\frac 1n \sum_{i=1}^n
K \bigg ( \frac{X_i - x_0}{h} \bigg )
\{-[a_{0} + B_{0}^\top (X_i - x_0)]^\top Y_i + b[a_{0} + B_{0}^\top (X_i - x_0) ] \}
\end{align*}`
where `\(b(\cdot)\)` determines the GLM, and `\(K(\cdot)\)` is a kernel weight with bandwidth `\(h\)`. 

The minimizer `\(\hat B_0\)` is used to estimate `\(\partial \theta(x_0) /\partial x^\top\)`.

---
class: left, top

# Outer Product of Canonical Gradients (OPCG)

We fit a local linear GLM about each `\(X_j\)`, for `\(j=1,...,n\)`, by minimizing the full negative local linear log-likelihood:

`\begin{align*}
&amp; L(a_1,..,a_n, B_1,...,B_n; X_{1:n}, Y_{1:n}) \\
= &amp; -\frac 1n \sum_{j=1}^n \ell_j (a_j, B_j; X_j, X_{1:n}, Y_{1:n}) \\ 
= &amp; -\frac {1}{n} \sum_{j,i=1}^n
K \bigg ( \frac{X_i - X_j}{h} \bigg )\\
&amp; \times 
\{[a_{j} + B_{j}^\top (X_i - X_j)]^\top Y_i - 
b(a_{j} + B_{j}^\top (X_i - X_j)) \} 
.
\end{align*}`

This provides minimizers `\(\hat B_j\)` that estimate `\(\partial \theta(X_j)/\partial x^\top\)`, which we use to construct the average outer product
`$$\hat \Lambda_n = \frac 1n \sum_{j=1}^n \hat B_j \hat B_j^\top.$$` 

---
class: left, top

# The OPCG Estimator

The **Outer Product of Canonical Gradients (OPCG) Estimator** for `\(\beta\)`, `\(\hat \beta_{opcg}\)`, is the first `\(d\)` eigenvectors of 
`$$\hat \Lambda_n = \frac 1n \sum_{j=1}^n \hat B_j \hat B_j^\top,$$` 
corresponding to the `\(d\)` largest eigenvalues.

---
class: left, top 
# Properties related to OPCG

  - The canonical gradients are *unbiased*; their outer product is *exhaustive*.

  - OPCG is consistent under some assumptions:
  
  &lt;div class="theorem" text="Consistency of OPCG"&gt;
  Suppose Assumptions 1-9 hold. Then, as \( n \to \infty\), we have
  \begin{align*} 
  \| \hat \beta_{opcg}  - \beta \|_F = O_{a.s}
  ( h + h^{-1} \delta_{ph} + \delta_{n} )
  ,
  \end{align*} 
  where \(\delta_{ph} = \sqrt{ \frac{\log n}{ nh^p} }\),
  \(\delta_{n} = \sqrt{ \frac{\log n}{ n } }\),
  \(  h \downarrow 0\), and
  \( h^{-1}\delta_{ph} \to 0\).
  &lt;/div&gt;
  
  - Can be implemented using Newton-Raphson

  - Ladle and Predictor Augmentation methods are fast, eigen-based methods that can be applied to estimate `\(d\)`. (Luo and Li, 2020; Luo and Li, 2016)

--

**What about the bandwidth `\(h\)`?**

---
class: inverse, center, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text[Tuning]


---
class: left, top
# Tuning the bandwidth, `\(h\)`.
 
We need to tune `\(h\)` in OPCG:

  - But Cross Validation can be computationally intensive and relative performance may vary between prediction methods. 

  - Can choose `\(h\)` according to optimal bandwidth, such as `\(h^{opt} = cn^{-\frac{1}{(p+6)}}\)` (Xia, 2007), but suggested `\(c\)` does not always work.

--

For classification problems, we propose using a K-means clustering procedure for tuning `\(h\)`.

Our motivation: 

  - SDR should make classification easier, and classification is easiest when the dimension reduced predictors `\(\hat \beta^\top X\)` are clustered into their respective labels.

---
class: left, top
# Tuning the bandwidth, `\(h\)`.

Let `\(Y \in \{1,...,m\}\)` be categorical response, `\((Y, X)_{1:n}\)` be our sample predictors for estimation, `\((Y,X)_{1:n_2}^{tune}\)` be our sample predictors for Tuning.

--

Main idea: For each `\(h\)`, 

1. Estimate `\(\hat \beta_{opcg}\)` on `\((Y,X)_{1:n}\)` and construct the sufficient predictors `\(\hat \beta_{opcg}^\top X_{1:n_2}^{tune}\)`.

2. Apply K-means to `\(\hat \beta_{opcg}^\top X_{1:n_2}^{tune}\)` for `\(m\)` clusters.

  - This returns `\(m\)` estimated clusters and the F-ratio corresponding to `\(h\)`, i.e. the Within Sums-of-Squares (WSS) over Between Sums-of-Squares (BSS)

3. Select `\(h\)` that minimizes the F-ratio from K-means. 


---
count: false
class: left, top 
# Tuning the bandwidth, `\(h\)`.

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="40%"&gt;
]  

  - `\(h\)` too small means 
    - likelihood is undersmoothed 
    - noisy gradient estimates
    - noisy dimension reduction; no separation


---
count: false
class: left, top 
# Tuning the bandwidth, `\(h\)`.

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="40%"&gt;
]  

  - `\(h\)` too large means 
      - likelihood is oversmoothed, 
      - gradient estimates can give only some separation of classes
        - large enough `\(h\)` to include the entire sample means fitting the same glm centered differently; recover at most one dimension;
  - as `\(h \to \infty\)`, all information is washed out and we recover noise

---
count: false
class: left, top 
# Tuning the bandwidth, `\(h\)`.

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="40%"&gt;
]  

  - Goal: find `\(h\)` not too small and not too large; just enough to separate all classes 

---
class: left, top
# Tuning the bandwidth, `\(h\)`.
 
But 
 - Estimating `\(m\)` clusters implicitly assumes 1 cluster per class
 - we ought to incorporate the class/label information from `\(Y_{1:n}^{tune}\)`, when available.

Our modifications to K-means:

1. K-means estimates more than 1 cluster per class;

2. "Supervised" K-means: Uses label information from `\(Y_{1:n}^{tune}\)`;

3. K-fold Tuning: Applies k-means, supervised k-means, or an average of both, in a k-fold manner;

&lt;!-- --- --&gt;
&lt;!-- class: left, top --&gt;
&lt;!-- # Tuning the bandwidth, `\(h\)`. --&gt;

&lt;!-- Improvements: --&gt;


&lt;!-- 2. For each `\(h\)`,  --&gt;

&lt;!--   a. Estimate `\(\hat \beta_{opcg}\)` using `\((Y,X)_{1:n}\)` and construct the sufficient predictors `\(\hat \beta_{opcg}^\top X_{1:n_2}^{tune}\)`. --&gt;

&lt;!--   b. For each class/label in `\((Y,X)_{1:n_2}^{tune}\)`,  --&gt;
&lt;!--     - apply K-means to those `\(\hat \beta_{opcg}^\top X_{1:n_2}^{tune}\)` belonging that class to estimate `\(M\)` clusters  --&gt;
&lt;!--     - Compute the corresponding WSS and BSS for that class in the tuning set --&gt;

&lt;!-- 2. Compute the "Supervised WSS" (SWSS) and "Supervised BSS" (SBSS) by summing the WSS and BSS over all classes, respectively.  --&gt;

&lt;!-- 3. Select `\(h\)` that minimizes the "supervised F-ratio" `\(\frac{SWSS}{SBSS}\)`. --&gt;




---
class: inverse, center, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text[Simulations and Data Analyses]

--
.left[

1. Goal: generalize OPG to Categorical and Ordinal-Categorical Responses

2. Propose: OPCG that generalizes OPG to linear exponential families

3. Categorical and Ordinal-Categorical variables are linear exponential families, so we can apply OPCG to Categorical and Ordinal-Categorical data
  
]

---
class: left, top
# OPCG Procedure

Given data:

  1. Split into three sets: 
  
    - Training set used for estimating `\(\hat \beta_{opcg}\)`; 
    
      - Can use for estimating `\(d\)` as well.
      
    - Tuning set used for tuning bandwidth `\(h\)`
    
    - Testing set used for assessing performance
  
  2. Split into two sets:
  
    - Training set used for estimating `\(\hat \beta_{opcg}\)`; 
    
      - Can use for estimating `\(d\)` as well.
      
      - Use K-fold tuning for tuning bandwidth `\(h\)`
      
    - Testing set used for assessing performance


---
class: left, top
# Simulations


Our predictor will be `\(X=(X^1,X^2,X^3,...,X^{10}) \in \R^{10}\)`.

  - `\((X^1, X^2) \sim N(\mu, \sigma^2 I_2)\)`, with `\(\sigma^2=.25\)` and `\(\mu \in \{ (0,0), (3,3), (-3,-3), (-2,2),(2,-2)\}\)`; there are 5 clusters total

  - Augmented with 8 standard normals for random noise, `\((X^3,...,X^{10})\)`


The clusters with centers `\((-2,2),(2,-2)\)` are labeled 1; `\((3,3),(-3,-3)\)` labeled 2 and `\((0,0)\)` is labeled 3.
  
  - Draw a total of 550 observations; 110 from each cluster  
  
We split into three sets evenly across all 5 clusters, i.e. clusters are all balanced in each set.
  
  - 250 for training; 150 for tuning; 150 for testing
  
---
class: left, top
# Simulations

So `\(Y \in \{1,2,3\}\)` is categorical; `\(X \in \R^{10}\)`; and `\(Y\)` is generated by `\((e_1, e_2)^\top X \in \R^{10 \times 2}\)`, where `\(d=2\)`.

  - we shuffle the 10 predictors so that so that `\(Y\)` is generated by `\(\beta^\top X\)`, where 
  
`\begin{align*}
  \beta =  \left( \begin{matrix}
  0&amp;0&amp;1&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0 \\
  0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;1&amp;0&amp;0&amp;0
  \end{matrix} \right )^\top \in \R^{10 \times 2}
\end{align*}`

---
count: false
class: left, top
# Simulations - Ladle, PA 


&lt;img align="center" class="image" src="images/opcg_sim_ladle.png" width="49%"&gt;
&lt;img align="center" class="image" src="images/opcg_sim_pa.png" width="49%"&gt;

Used `\(h=1\)` for order determination, 200 samples for Ladle and PA.



---
class: ani-slide
# K-mean Tuning for `\(h\)`
 
&lt;iframe src="images/tuning_sc.html" width="100%" height="95%" frameborder="0" &gt;&lt;/iframe&gt;



---
count: false
class: left, top
# Simulations - tuning and estimation

Conventional suggestion: `\(h = 2.34 n^{-1/(p+6)} \approx 1.66\)`;

K-fold K-means Tuning: `\(h \approx 1.25\)`;


--

Using `\(h \approx 1.25\)` to estimate OPCG `\(\hat \beta_{opcg}\)`:

&lt;img align="centered" class="image" src="images/sim_dist_table2.png" width="100%"&gt;

--

  - PW-OPCG estimates 2 SDR directions per pair of classes \{1,2\}, \{1,3\}, and \{2,3\}, for 6 total, and selects the 2 that explain the most variation. 

    - Adragni's suggestion for multi-class problems.
    
  - DR is Directional Regression (Li and Wang, 2007)  
---
count: false
class: left, top
# Simulations - Sufficient Predictors `\(\hat \beta^\top X^{test}\)`

&lt;img align="center" class="image" src="images/opcg_sim_test-opg.png" width="100%"&gt;


---
class: left, top
# Ordinal-Categorical Data Analysis

We analyze four ordinal-categorical response datasets: 

  - Red Wine Quality - UCI

    - p=11; n=1599; Ordinal response - Wine Quality Score;
  
  &lt;!-- - Wisconsin Eye Disease --&gt;

  &lt;!--   - p=10;  n=294;  --&gt;
  &lt;!--   - Ordinal response - severity of coronary artery disease (1 = no disease; 2 = degree 1; 3 = degree 2; 4 = degree 3; 5 = degree 4); Classes: 1,2,3,4 --&gt;

  - Boston Housing Prices - 'spData' R package

    - p=13; n=506; resp=Median value of homes; 
    
  - Communities and Crime Rate - UCI

    - p=99; n=1994; resp=pop. crime. rate

  - Superconductors - UCI

    - p=81; n=2000; resp=crit. temp. of superconductors

    
  &lt;!-- - Panel Study of Income Dynamics 1976 (psid76) --&gt;

  &lt;!--   - p=20; n=753 --&gt;
  &lt;!--   - Ordinal response - Family Income; Classes: 1,2,3,4,5; --&gt;


&lt;!-- --- --&gt;
&lt;!-- class: left, top --&gt;
&lt;!-- # Ordinal Classification Error using MCOSVM --&gt;

&lt;!-- - MCOSVM (Waegeman and Boullart, 2009) : `\(m-1\)` SVMs on all binary classifications `\(I\{ Y &gt; l\}\)`; construct estimated probabilities of `\(P\{Y=l\}\)`; highest probability is predicted class.   --&gt;

&lt;!-- -- --&gt;

&lt;!-- &lt;img align="centered" class="image" src="images/ord_class_table2.png" width="100%"&gt; --&gt;


---
count: false
class: left, top
# Red Wine Quality


&lt;img align="centered" class="image" src="images/opcg_wine_preds2.png" width="100%"&gt;


&lt;!-- --- --&gt;
&lt;!-- class: left, top --&gt;
&lt;!-- # Wisconson Eye Disease  --&gt;


&lt;!-- &lt;img align="centered" class="image" src="images/opcg_eye_preds2.png" width="100%"&gt; --&gt;



---
class: left, top
# Boston Housing Prices

&lt;img align="centered" class="image" src="images/opcg_house_preds2.png" width="100%"&gt;

&lt;!-- --- --&gt;
&lt;!-- class: left, top --&gt;
&lt;!-- # PSID 76 --&gt;

&lt;!-- &lt;img align="centered" class="image" src="images/opcg_psid_preds2.png" width="100%"&gt; --&gt;


---
class: left, top
# Communities

&lt;img align="centered" class="image" src="images/opcg_commun_preds2.png" width="100%"&gt;

---
class: left, top
# Superconductors

&lt;img align="centered" class="image" src="images/opcg_super_preds2.png" width="100%"&gt;


---
class: left, top
# Categorical Data Analysis  

We analyze three datasets with categorical responses: 

  - Handwritten Digits (Pendigit) from UCI
  
    - p=16; n=2000; resp=0-9
    
  - USPS Handwritten Digits (Pendigit) from UCI
  
    - p=256; n=2007; resp=0-9
    
  - ISOLET from UCI
  
    - p=618; n=6334/1553; resp=a-z

---
class: left, top
# Categorical Classification Error using SVM

.center[
&lt;img align="center" class="image" src="images/cat_class_table3.png" width="80%"&gt;
]

---
class: ani-slide
# Pen Digit 10 - SIR/DR - 1,6,7,9
&lt;iframe src="images/dr_pendigit4.html" width="100%" height="95%" frameborder="0"&gt;&lt;/iframe&gt;

---
count: false
class: ani-slide
# Pen Digit 10 - OPCG - 1,6,7,9

&lt;iframe src="images/opcg_pendigit4.html" width="100%" height="95%" frameborder="0"&gt;&lt;/iframe&gt;

---
class: ani-slide
# Pen Digit 10 - SIR/DR - 3,5,8,9
&lt;iframe src="images/dr_pendigit4-2.html" width="100%" height="95%" frameborder="0"&gt;&lt;/iframe&gt;

---
count: false
class: ani-slide
# Pen Digit 10 - OPCG - 3,5,8,9

&lt;iframe src="images/opcg_pendigit4-2.html" width="100%" height="95%" frameborder="0"&gt;&lt;/iframe&gt;


---
class: left, top, inverse
# .bg-text[Conclusion]


1. Provided the Multivariate Link Functions for Ordinal-Categorical Responses.

--

2. Generalized OPG to linear exponential families 

--

3. Introduced a K-means tuning procedure for classifcation

--

4. Demonstrated the effectiveness of OPCG in categorical and ordinal classification problems.

  - Can handle multiple labels simultaneously 
  
  - Noticeable improvement over OPG for larger `\(p\)` 


---
layout: false
# References

Adragni, K. P. (2018). "Minimum average deviance estimation for
sufficient dimension reduction". In: _Journal of Statistical
Computation and Simulation_ 88.3, pp. 411-431.

Agresti, A. (2010). _Analysis of ordinal categorical data_. Vol. 656.
John Wiley &amp; Sons.

Agresti, A. (2013). _Categorical Data Analysis_. 3rd ed. Wiley.

Fukumizu, K. and C. Leng (2014). "Gradient-based kernel dimension
reduction for regression". In: _Journal of the American Statistical
Association_ 109.505, pp. 359-370.

Lambert-Lacroix, S. and J. Peyre (2006). "Local likelihood regression
in generalized linear single-index models with applications to
microarray data". In: _Computational statistics &amp; data analysis_ 51.3,
pp. 2091-2113.

Li, B. and S. Wang (2007). "On directional regression for dimension
reduction". In: _Journal of the American Statistical Association_
102.479, pp. 997-1008.

---
layout: false
# References

Li, K. (1991). "Sliced inverse regression for dimension reduction". In:
_Journal of the American Statistical Association_ 86.414, pp. 316-327.

Luo, W. and B. Li (2016). "Combining eigenvalues and variation of
eigenvectors for order determination". In: _Biometrika_ 103.4, pp.
875-887.

Luo, W. and B. Li (2020). "On order determination by predictor
augmentation". In: _Biometrika_.

Wang, H. and Y. Xia (2008). "Sliced regression for dimension
reduction". In: _Journal of the American Statistical Association_
103.482, pp. 811-821.

Xia, Y. (2007). "A constructive approach to the estimation of dimension
reduction directions". In: _The Annals of Statistics_ 35.6, pp.
2654-2690.

Xia, Y., H. Tong, W. Li, et al. (2002). "An adaptive estimation of
dimension reduction space". In: _Journal of the Royal Statistical
Society: Series B (Statistical Methodology)_ 64.3, pp. 363-410.

---
layout: false
# References

Yin, X., B. Li, and others (2011). "Sufficient dimension reduction
based on an ensemble of minimum average variance estimators". In: _The
Annals of Statistics_ 39.6, pp. 3392-3416.


---
class: left, top, inverse
# .bg-text[Appendix]


 
---
class: left, top 
# What is Sufficient Dimension Reduction?

Sufficient Dimension Reduction (SDR) extracts a lower dimensional function of the predictors that "preserves" relationships of interest between the predictors and response.

--

"Preservation" is formulated as conditional independence.

--

Let `\(Y \in \R^m\)` be a response and `\(X \in \R^p\)` be a predictor. 

- Linear SDR finds a linear function `\(s(X) = \beta^\top X\)`, where `\(\beta \in \R^{p \times d}\)` with `\(d &lt; p\)`, such that

`$$Y \indep X \; | \; \beta^\top X$$`


---
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# What is Sufficient Dimension Reduction?


- For any `\(\beta\)` such that `\(Y \indep X \; | \; \beta^\top X\)`, the `\(span(\beta)\)`     is a _Sufficient Dimension Reduction Subspace_.

--

- Under mild conditions, a _minimial_ Sufficient Dimension Reduction (SDR) Subspace exists (Yin, Li, and Cook (2008))

  - This is the **Central Subspace (CS)** and is denoted `\(\SS_{Y \indep X}\)`.
  
  - `\(\SS_{Y \indep X}\)` is minimial in that `\(\SS_{Y \indep X} \leq span(\beta)\)`, for any other SDR subspace.

---
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# What is Sufficient Dimension Reduction?

- If we want to preserve &lt;!--all relevant information available in `\(X\)` about--&gt; 
the _regression relation_ `\(E(Y|X)\)`, then SDR finds `\(\beta\)` such that 
`$$Y \indep E(Y|X) \; | \; \beta^\top X$$`

  - Equivalent to finding `\(\beta\)` such that `\(E(Y|X) = E(Y | \beta^\top X )\)` (Cook and Li (2002))
  
  - The minimial SDR Subspace for `\(E(Y|X)\)` is the **Central Mean Subspace (CMS)**, `\(\SS_{E(Y | X)}\)`.
  
  - In general, the `\(\SS_{E(Y | X)} \leq \SS_{Y \indep X}\)`.

--

&lt;div class=prop text="CMS=CS"&gt;
(Cook and Li (2002))
If \(Y\) depends on \(X\) only through the Conditional Mean, 
i.e. \(Y \indep X \;|\; E(Y|X)\), then \(\SS_{E(Y | X)} = \SS_{Y \indep X}\).
&lt;/div&gt;

&lt;!-- - This framework can be applied broadly to define the Central Median Subspace, Central Quantile Subspace, Central `\(k^{th}\)`-Moment Subspace, and so on. --&gt;

&lt;!-- `\(\Xcal\)` and `\(\water\)` and `\(\braket{4}\)` and `\(\Abs{4}{3}\)`. --&gt;    
 
&lt;div class="prop"&gt;
If \(Y\) depends on \(X\) solely through the canonical parameter \(\theta(X)\), then the \(\SS_{Y|X} = \SS_{E(Y|X)}\). 
&lt;/div&gt;

So extending OPG to categorical and ordinal responses will recover `\(\SS_{E(Y|X)}=\SS_{Y|X}\)`.



---

# OPCG: Unbiased and Exhaustive 

Some SDR terminology (in statistical functional notation):

- An estimator `\(M(F_n)\)` for a SDR subspace is **unbiased** if `\(M(F_0)\)` is a member of the subspace.
  
- An estimator `\(M(F_n)\)` for a SDR subspace is **exhaustive** if `\(M(F_0)\)` generates the subspace.
  
--
&lt;br/&gt;

OPG is unbiased and, under some conditions, exhaustive for `\(\SS_{E(Y|X)}\)`. (Li, 2018). 




---

# OPCG: Unbiased and Exhaustive 

&lt;div class=prop text="Unbiasedness of the canonical gradient"&gt;

If the canonical parameter satisfies \(\theta(x)=\theta(\beta^\top x)\) for `\(x \in \Omega_X\)`, then an estimator for the derivative is unbiased for the Central Mean Subspace, i.e.

$$
\frac{\partial\theta(x)}{\partial x ^{\top}}
\in \SS_{E(Y|X)}
.
$$

&lt;/div&gt;
--

&lt;div class=prop text="Exhaustiveness of the Outer Product"&gt;

If the canonical parameter \(\theta(\beta^\top x)\) is convexly supported in \(\beta^\top x\), then an estimator for the outer product of the derivative is exhaustive for `\(\SS_{E(Y|X)}\)`, i.e.
$$
span\bigg ( \frac{\partial \theta(x)}{\partial x^{\top}}\frac{\partial \theta(x)^{\top}}{\partial x} \bigg ) 
=
\SS_{E(Y|X)}
.
$$
&lt;/div&gt; 


---
class: left, top

# OPCG: Consistency

Let `\(\eta \in \R^{p \times d}\)` be the first `\(d\)` eigenvectors of the matrix
`\begin{align*}
\Lambda = 
E
\bigg ( \frac{\partial \theta(X)}{\partial x^{\top}}\frac{\partial \theta(X)^{\top}}{\partial x} \bigg ).
\end{align*}`

--

&lt;div class="theorem" text="Consistency of OPCG"&gt;
  Suppose Assumptions 1-9 hold. Then, as \( n \to \infty\), we have
  \begin{align*} 
  \| \hat \beta_{opcg}  - \eta \|_F = O_{a.s}
  ( h + h^{-1} \delta_{ph} + \delta_{n} )
  ,
  \end{align*}
  &lt;!-- \bigg( h + \sqrt{ \frac{\log n}{ nh^p} } + \sqrt{ \frac{\log n}{ n } } \bigg) --&gt;
  where \(\delta_{ph} = \sqrt{ \frac{\log n}{ nh^p} }\),
  \(\delta_{n} = \sqrt{ \frac{\log n}{ n } }\),
  \(  h \downarrow 0\), and
  \( h^{-1}\delta_{ph} \to 0\).
&lt;/div&gt;

--

Our Assumptions 1-9 for Consistency can be summarized by: 

  - compactness of the predictor and parameter, 
  - smoothness of the likelihood and density, finite moments of `\(Y\)`,
  - exhaustiveness of OPCG,
  - symmetric kernels with finite moments,
  - convergence rates on the bandwidth.  

&lt;!-- These assumptions, referred to as Assumptions 1-9, are in an appendix at the end. --&gt;

---
class: left, top
# Order Determination

"Order Determination" refers to estimating `\(d\)`, the dimension of the minimal SDR subspace.  

  - An advantage of OPCG is its compatibility with recent eigen-based methods:
  
    - Ladle Estimator  (Luo and Li, 2016) 
    
      - Combines information from bootstrapped variation in eigenvalues and eigenvectors of `\(\hat \Lambda_n\)`
      
    - Predictor Augmentation (Luo and Li, 2020)
  
      - Combines information from variation in eigenvalues and eigenvectors of `\(\hat \Lambda_n\)` from augmenting `\(X\)` with noise. 
  


---
class: inverse, center, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text[Improvements to OPCG:]
## Minimum Average Deviance Estimator (MADE), refined OPCG (rOPCG), refined MADE (rMADE) 

---
class: left, top
# Minimum Average Deviance Estimator (MADE)

Recall the dimension reduction assumption: `\(\theta(x) = \theta (\beta^\top x)\)`. 

Then the derivative satisfies:
$$ \frac{\partial \theta(x)^\top }{\partial x } = \beta \frac{\partial \theta\{u( x ) \}^\top } {\partial u } $$
where `\(u(\chi)=\beta^\top \chi\)`. 

--

- How can we make use of this property explicitly?

  - We can substitute it into the negative log-likelihood and estimate `\(\beta\)`         directly

  - In the local linear regression setting with OPG, this method is the Minimum Average Variance Estimator (MAVE) (Xia, Tong, Li, et al., 2002).
  

---
class: left, top
# The Minimum Average Deviance Estimator (MADE)

Abusing notation for `\(B\)`, we estimate `\(\beta\)` directly by minimizing:

`\begin{align*}
&amp; L(\beta, a_1,..,a_n, B_1,...,B_n; X_{1:n}, Y_{1:n}) \\
= &amp; -\frac 1n \sum_{j=1}^n \ell_j (\beta, a_j, B_j; X_j, X_{1:n}, Y_{1:n}) \\ 
= &amp; -\frac {1}{n} \sum_{j,i=1}^n
K \bigg ( \frac{X_i - X_j}{h} \bigg )\\
&amp; \times 
\{[a_{j} + B_{j}^\top \beta^\top (X_i - X_j)]^\top Y_i - 
b(a_{j} + B_{j}^\top \beta^\top (X_i - X_j)) \} 
.
\end{align*}`

--

Minimization can be performed by alternating between two steps until convergence:
1. (MADE-1 step) For fixed `\(\beta\)`, minimize `\(a_1,...,a_n, B_1,...,B_n\)`, 
2. (MADE-2 step) For fixed `\(a_1,...,a_n, B_1,...,B_n\)`, minimize `\(\beta\)`. 

We refer to the resulting estimator `\(\hat \beta_{made}\)` the **Minimum Average Deviance Estimator** (MADE) for `\(\beta\)`.



---
class: left, top
# Refined OPCG (rOPCG) and Refined MADE (rMADE)
 
Given estimates `\(\hat \beta_{opcg}\)` and `\(\hat \beta_{made}\)`, 
we can replace the kernel weights in their respective objective functions by
`\begin{align*}
K[(X - \chi)/h] \mapsto
K[\hat \beta_{opcg}^\top(X - \chi)/h], \\
K[(X - \chi)/h] \mapsto
K[\hat \beta_{made}^\top(X - \chi)/h].
\end{align*}`



--

We re-estimate `\(\beta\)` using the refined objective functions and iterate the refinement until convergence to obtain `\(\hat \beta_{ropcg}\)` and `\(\hat \beta_{rmade}\)`, respectively.

---
class: left, top
# Minimum Average Deviance Estimator (MADE)

Some notes on MADE:

  - Our presentation of MADE is a multivariate version of Adragni's (2018).
  
  - The parameter `\(\beta\)` in MADE is only identifiable up to orthogonal transformations.

  - The alternating minimization for, and non-identifiability of `\(\beta\)` in MADE is computationally costly relative to OPCG.
  
    - But MAVE (i.e. Linear Regression setting) is more efficiently, 
    (Xia, 2006; Xia, 2007),
    so We expect MADE to be more efficient compared to OPCG.

---
class: left, top
# Implementation 

For fixed `\(X_j\)`, we use the "vec" operation in two ways:

1. in OPCG and MADE-1, 
`\begin{align*}
a_j + B_j^\top(X-X_j) = 
\left ( 
\left ( \begin{matrix}
1 \\
X-X_j
\end{matrix} \right )
\otimes I_m \right ) 
\left ( \begin{matrix}
a_j \\
{\mathrm{vec} }(B_j^\top)
\end{matrix} \right ) 
\end{align*}`

--

2. In MADE-2, 
`\begin{align*}
a_j + B_j^\top\beta^\top(X-X_j) = 
a_j + [ (X - X_j)^\top \otimes B_j^\top ]^\top \mathrm{vec}(\beta^\top)
,
\end{align*}`

--

This allows us to use Newton-Raphson/Fisher-Scoring for OPCG and MADE. 

---
class: left, top
# Implementation

Fisher-Scoring/Newton-Raphson for OPCG and MADE:

  - OPCG and MADE-1, we have good initial values via the inverse canonical link; 
  
    - but still slow for `\(p \approx 20\)` and `\(n \approx 1000\)`; 

--

  - For MADE-2, we don't have a systematic approach finding initial values for `\(\beta\)`, other than using the OPCG estimate, `\(\hat \beta_{opcg}\)`; 
   
    - Estimation is extraordinarily slow for MADE-2, the `\(\beta\)` step; 
    
---
class: left, top
# Implementation

We implement a Conjugate Gradient Algorithm written in Rcpp

  - hybrid Conjugate Gradient (Dai and Yuan, 2001) 
  
    - Only need the descent criterion; have avoided the Weak Wolfe condition in practice so far.
  
  - Works very well for OPCG and MADE-1, which can be computed in parallel 

  - Still slow for MADE-2, but much faster than Newton


---
class: left, top
# Order Determination


"Order Determination" refers to estimating `\(d\)`, the dimension of the minimal SDR subspace.  

  - An advantage of OPCG is its compatibility with recent eigen-based methods:
  
    - Ladle Estimator  (Luo and Li, 2016) 
    
      - Combines information from bootstrapped variation in eigenvalues and eigenvectors of `\(\hat \Lambda_n\)`
      
    - Predictor Augmentation (Luo and Li, 2020)
  
      - Combines information from variation in eigenvalues and eigenvectors of `\(\hat \Lambda_n\)` from augmenting `\(X\)` with noise. 
      
--

- For MADE, cross-validation methods can be generalized from MAVE (Xia, Tong, Li, et al., 2002)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
