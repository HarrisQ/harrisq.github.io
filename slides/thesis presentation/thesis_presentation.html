<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Advances in Forward Sufficient Dimension Reduction for Statistical Learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Harris Quach" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="mytheme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: { 
      independenT: ["{\\mathrel{\\rlap{ #1 }\\mkern2mu{ #1 }}}",1],
      indep: "{\\independenT{\\perp}}",
      SS: "{\\mathscr{S}}",
      R:"{\\mathbb{R}}",
      Xcal: "{\\mathcal{X}}",
      water: "{H_2O}",
      braket: ['{\\langle #1 \\rangle}', 1], 
      Abs: ['\\left\\lvert #2 \\right\\rvert_{\\text{#1}}', 2, ""]
    }
  }
});
</script>




 

&lt;!-- class: title-slide --&gt;

# Developments in Forward Sufficient Dimension Reduction 


&lt;hr/&gt;
## Harris Quach (joint work with Dr. Bing Li) &lt;br/&gt; Date: 2021-07-19 (updated: 2021-07-18)

---
class: left, middle
# Acknowledgements

I would like to thank my committee members for their time and support:

  - Bing Li
  - Bharath K. Sriperumbudur
  - Yanyuan Ma
  - Alexei Novikov


---
class: inverse, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text.center[Overview]  

.md-text[
  
1. &lt;span style="color:white"&gt;&lt;b&gt;Forward Sufficient Dimension Reduction for Categorical and Ordinal Response&lt;/b&gt;&lt;/span&gt;
  
2. Forward Sufficient Dimension Reduction for Categorical and Ordinal Response with functional predictor
  
3. Forward Sufficient Dimension Reduction for functional response with functional predictor
  
]


---
class: left, top
# What is Sufficient Dimension Reduction? 


Suppose we have a large dataset with some response `\(Y \in \R^m\)` and predictors `\(X \in \R^p\)`.

  - When `\(p\)` is large, lower dimensional summaries of `\(X\)` are helpful for visualization and application of conventional statistical methods

  - Finding a lower dimensional summary of `\(X\)` means finding `\(\beta \in \R^{p \times d}\)`, where `\(d &lt; p\)`, in order to construct the lower dimensional summary `\(\beta^\top X\)`
  
  - Sufficient Dimension Reduction (SDR) are approaches for finding `\(\beta\)` such that `\(\beta^\top X\)` retains all relevant information about `\(Y\)`
  
  - Inverse and Forward SDR describe general SDR approaches for finding such a dimension reduction `\(\beta\)`
  
  - `\(\beta\)` can preserve different information by satisfying different criteria.

---
class: left, top
# Central Subspaces

The information preserved by `\(\beta\)` is characterized by conditional independence:

  - `\(Y \indep X | \beta^\top X\)`; (Li, 2018)
    - `\(\beta\)` preserves all information between `\(Y\)` and `\(X\)` and `\(\mathrm{span}(\beta)\)` is an SDR subspace for `\(Y|X\)`; the smallest such subspace is called the **Central Subspace**. 

  - `\(Y \indep E(Y|X) | \beta^\top X\)`; (Cook and Li, 2002; Ma and Zhu, 2014)
    - `\(\beta\)` preserves all information between `\(Y\)` and `\(E(Y|X)\)` and `\(\mathrm{span}(\beta)\)` is an SDR subspace for `\(E(Y|X)\)`; the smallest such subspace is called the **Central Mean Subspace**. 

--

**NOTE**

We actually want the subspace generated by `\(\beta\)`, i.e. `\(\mathrm{span}(\beta)\)`.
  
  - **When we say we are estimating `\(\beta\)`, we mean estimating a basis for the subspace generated by `\(\beta\)`, i.e a basis for `\(\mathrm{span}(\beta)\)`.**
  
  
---
class: left, top
# Alternative Subspaces

The information preserved by `\(\beta\)` is characterized by conditional independence:

  - `\(Y - E(Y|X) \indep \mathrm{Var}(Y|X) | \beta^\top X\)`; (Zhu and Zhu, 2009)   
    - `\(\beta\)` preserves all information between `\(Y - E(Y|X)\)` and `\(\mathrm{Var}(Y|X)\)`; 
    &lt;!-- the smallest such subspace is called the **Central Variance Subspace** --&gt;
    
  - `\(Y  \indep Q_\tau(X) | \beta^\top X\)`; (Kong and Xia, 2012; Luo, Li, and Yin, 2014)
    - `\(\beta\)` preserves all information between `\(Y\)` and `\(Q_\tau(X)=\inf\{y : P(Y \leq y |X) \geq \tau\}\)`; 
    &lt;!-- the smallest such subspace is called the **Central `\(\tau\)`-th Quantile Subspace**   --&gt;
  
  - `\(Y  \indep \theta(X) | \beta^\top X\)`; (Luo, Li, and Yin, 2014)
    - `\(\beta\)` preserves all information between `\(Y\)` and `\(\theta(X)\)`, where `\(\theta(X)\)` is defined through a conditional statistical functional; 
    
  - `\(N(B_1),\ldots, N(B_k)  \indep X(B_1),\ldots,X(B_k) | \beta^\top X(B_1),\ldots,\beta^\top X(B_k)\)`; (Guan and Wang, 2010)
    - `\(\beta\)` preserves all information between the counting process `\(N\)` and the Gaussian random field `\(X\)` over sets `\(B_1,\ldots,B_k\)`;  



---
class: left, top
# Recent Advances in SDR

  - Semi-parametric efficient SDR (Ma and Zhu, 2012; Ma and Zhu, 2013; Ma and Zhu, 2014)  
  
  - gradient-based SDR using RKHS (Fukumizu and Leng, 2014)
  
  - Ladle and Predictor Augmentation Estimator for `\(d\)` (Luo and Li, 2020; Luo and Li, 2016)  

  - Nonlinear SDR (Lee, Li, and Chiaromonte, 2013)  
  - Nonlinear and Linear Functional SDR (Li and Song, 2017; Li and Song, 2021)  

  - Post-dimension Reduction Inference (Kim, Li, Yu, and Li, 2020)  
  

---
class: left, top
# Why Sufficient Dimension Reduction? 

&lt;blockquote&gt;
"...I’ve never been comfortable with the way in which
our discipline has embraced sparsity to the exclusion of
everything else. I mentioned before that I like the availability
of options in statistics, but for a long time now the
prevailing attitude toward high-dimensional problems has
been that its natural to assume sparsity. I find no sense in
which it’s natural. That overwhelming emphasis on sparsity
has, I think, kept us from seeing other solutions and
options. To be clear, I have nothing against sparsity per se,
and I think it’s a reasonable modeling construct. But it’s
not reasonable just because you have high-dimensional
data."  
.right[~ &lt;cite&gt;*Dennis Cook* (2021)&lt;/cite&gt;]
.right[(Bura, Li, Li, Nachtsheim, Pena, Setodji, and Weiss, 2021)]
&lt;/blockquote&gt;
  
  
---
class: inverse, center, middle

# .bg-text[Inverse and Forward Linear SDR]  

---
class: left, top
# Motivating Example: 

Consider a response `\(Y\)` and predictor `\(X = (X_1, X_2) \in [0,1]^2\)`. 
Let `\(Y=X_1^2\)`.
Then `\(Y= (\beta^\top X)^2\)`, where `\(\beta = (1,0) \in \R^2\)`. 
- we want to recover `\(span(\beta) = \{ (c,0): c \in \R\}\)`; we take `\(d=1\)` as known.
.center[
&lt;img align="centered" class="image" src="images/sdr_plot.png" width="50%"&gt;
]



---
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Sliced Inverse Regression (Li, 1991)

.center[
&lt;img align="centered" class="image" src="images/sir_plot1.png" width="55%"&gt;
]

&lt;!-- &lt;iframe src="images/almost_sir.html" width="90%" height="90%" frameborder="0"&gt;&lt;/iframe&gt; --&gt;


---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Sliced Inverse Regression (Li, 1991)

.center[
&lt;img align="centered" class="image" src="images/sir_plot2.png" width="55%"&gt;
]

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Sliced Inverse Regression (Li, 1991)

.center[
&lt;img align="centered" class="image" src="images/sir_plot3.png" width="55%"&gt;
]

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Sliced Inverse Regression (Li, 1991)

.center[
&lt;img align="centered" class="image" src="images/sir_plot4.png" width="55%"&gt;
]

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Sliced Inverse Regression (Li, 1991)

.center[
&lt;img align="centered" class="image" src="images/sir_plot5.png" width="55%"&gt;
]
--

  - 'Inverse' because we estimate `\(E(X|Y)\)`.

---
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 

# Motivating Example: 
## Inverse Regression for SDR - Drawbacks

.center[
&lt;img align="centered" class="image" src="images/sir_plot5.png" width="55%"&gt;
]

---
count: false
class: left, top  

# Motivating Example: 
## Inverse Regression for SDR - Drawbacks

.center[
&lt;img class="image" src="images/bad_sir_plot1.png" width="55%"&gt;
] 

---
count: false
class: left, top  

# Motivating Example: 
## Inverse Regression for SDR - Drawbacks

.center[
&lt;img class="image" src="images/bad_sir_plot2.png" width="55%"&gt;
] 

---
count: false
class: left, top  

# Motivating Example: 
## Inverse Regression for SDR - Drawbacks

.center[
&lt;img class="image" src="images/bad_sir_plot3.png" width="55%"&gt;
] 
--
- Inverse methods require assumptions on the support of the predictor.

---
class: left, top  

# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, and Zhu, 2002)

.center[
&lt;img class="image" src="images/fsdr_plot1.png" width="55%"&gt;
] 

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)


.center[
&lt;img class="image" src="images/fsdr_plot2.png" width="55%"&gt;
] 

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)
 

.center[
&lt;img class="image" src="images/fsdr_plot3.png" width="55%"&gt;
] 



---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)
 

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="55%"&gt;
] 

--
- "Forward" Regression because we are estimating `\(E(Y|x)\)` and `\(\partial E(Y|x)/\partial x^\top\)`

---
count: false
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)


.center[
&lt;img class="image" src="images/fsdr_plot4.png" width="55%"&gt;
] 

---
count: false
class: left, top
# Forward Regression for SDR
## Outer Product of Gradients (OPG) (Xia, Tong, Li, et al., 2002)


.center[
&lt;img class="image" src="images/fsdr_plot5.png" width="55%"&gt;
]

---
class: left, top
# Developments in Forward Regression for SDR

## Summary of Thesis:

- Generalize these two pictures to categorical and ordinal response
- Generalize these two pictures to functional data

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="45%"&gt;
&lt;img class="image" src="images/fsdr_plot5.png" width="45%"&gt;
]

---
class: inverse, left, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .center[.bg-text[Forward SDR for Categorical and Ordinal Response]  ]

1. Multivariate Link Functions

2. Outer Product of Canonical Gradients

4. Minimum Average Deviance Estimation

3. K-means Tuning

4. Simulations and Applications

---
class: left, top
# Forward Regression for SDR

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="45%"&gt;
&lt;img class="image" src="images/fsdr_plot5.png" width="45%"&gt;
]

- OPG applies to uni-variate, continuous `\(Y\)`
- Doesn't work as well when `\(Y\)` is categorical or ordinal
- OPG fits a local linear regression
- **Proposal: Fit a local multivariate GLM**


---
class: left, top
# Why fit a local multivariate GLM?

We are interested in categorical and ordinal `\(Y\)`.

--

Categorical variables are linear exponential families (hence GLM) because they are a special case of the multinomial

  - the multivariate Logistic and Expit functions are the canonical and inverse canonical links

--

**Contribution:** We show ordinal variables can be represented explicitly as a linear exponential family by computing its canonical link, inverse canonical link and cumulant generating function.

  - the canonical link the multivariate **Adjacent Categories** logistic link (Agresti, 2010) 
 

---
class: left, top
# Ordinal Response

Suppose `\(Y \in \{1,...,m\}\)` is an ordinal-categorical variable for `\(m\)` ordered categories.  

  - We can represent `\(Y\)` with a vector `\(T = (T_1,...T_{m-1}) \in \{0,1\}^{m-1}\)`;

  - We can interpret the entries of `\(T\)` as `\(T_j = I\{Y &gt; j\}\)` for categories `\(j=1,\ldots,m-1\)` 
  
  &lt;!-- - If `\(Y = k\)`, then `\(T_j = 1\)` for `\(j \leq {k-1}\)` and `\(T_j=0\)` for `\(j &gt; k-1\)`.  --&gt;

  - Eg. `\(m=5\)`; if `\(Y=3\)`, then `\(T=(1,1,0,0)\)`; if `\(Y=1\)`, then `\(T=(0,0,0,0)\)`

---
class: left, top
# Ordinal Response

We show the random vector `\(T\)` has log-likelihood:
`\begin{align*}
\ell(\theta; T)
= \theta^{\top} T - \log ( 1 + e_{1}^{\top}  P   L \exp (L \theta ) )  
,
\end{align*}`
`\(P\)` is a permutation matrix and `\(L\)` is lower triangular matrix of `\(1\)`'s. 

 - So `\(T\)` is a linear exponential family (hence GLM). 
 - We say the random vector `\(T\)` has a **ordinal-categorical (Or-Cat)** distribution.

--

The **adjacent-categories (Ad-Cat)** link is the canonical link 
`\begin{align*}
\theta(\tau) 
=
\log \left\{ \mathrm{diag}[ (P^{-1} - I)\tau ]^{-1} (P^{-1} - I)\tau \right \},
\end{align*}`
where `\(E(T)=\tau\)`.

--

The inverse canonical link (i.e. mean function) is
`\begin{align*}
\tau(\theta)
=\frac{Q P  L \exp (L \theta ) }{1 + e_1^{\top} P   L \exp (L \theta ) }
,
\end{align*}`
where `\(Q\)` is a difference matrix 

---
class: left, top
# Forward Regression for SDR

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="45%"&gt;
&lt;img class="image" src="images/fsdr_plot5.png" width="45%"&gt;
]

- OPG developed for uni-variate, continuous `\(Y\)`
- Doesn't work as well when `\(Y\)` is categorical or ordinal
- OPG fits a local linear regression
- **Proposal: Fit a local multivariate GLM**

---
class: left, top
# Forward Regression for SDR 
##Proposal: Fit a local multivariate GLM

Existing work in this direction:

1. Generalized Single Index Model (GSIM): Lambert-Lacroix and Peyre (2006)
  - Local Linear GLM for uni-variate `\(Y\)`; 
  - Goal was to generalize OPG, but method is actually the Average Derivative Estimator (ADE) Härdle and Stoker (1989); 
  - ADE has known drawbacks; e.g. requires gradient has non-zero mean. 

2. Minium Average Deviance Estimation (MADE): Adragni (2018)
  - Local Linear GLM for uni-variate `\(Y\)` 
    - Generalizes the Minimum Average Variance Estimator (MAVE) of Xia, Tong, Li, et al. (2002)  

**Can't handle multi-labels simultaneously. Only binary `\(Y\)` can be reduced to uni-variate `\(Y\)`.
&lt;br/&gt;&lt;br&gt;
Eg. Response with 3 labels needs bi-variate `\(Y\)` representation**


---
class: inverse, center, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text[Outer Product of Canonical Gradients (OPCG)]  

---
class: left, top &lt;!-- formatting the slide --&gt;

&lt;!-- the title --&gt; 
 
# Generalized Forward Regression for SDR

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="40%"&gt;
]  

- In OPG, we fit a linear regression about `\(x_0\)`, i.e. we minimize
`\begin{align*}
\{Y - a_{x_0} + B_{x_0}^\top(X-x_0) \}^\top \{Y - a_{x_0} + B_{x_0}^\top(X-x_0) \} 
\end{align*}`
and estimate `\(\partial E(Y|X=x_0)/ \partial x^\top\)` using `\(\hat B_{x_0}\)`. 
    &lt;!-- `$$\hat B_0 = \frac{ \widehat{ \partial E(Y|x_0)} }{\partial x^\top}.$$` --&gt;

---
class: left, top 
 
# Generalized Forward Regression for SDR

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="40%"&gt;
]  

- Instead, fit a multivariate GLM about `\(x_0\)` and minimize
`\begin{align*}
  -\ell (a_{x_0}, B_{x_0},x_0)  = - [a_{x_0} + B_{x_0}^\top(x-x_0)]^\top y + b[a_{x_0} + B_{x_0}^\top(x-x_0)] 
  .
\end{align*}`
We will use the minimizer `\(\hat B_{x_0}\)`. 

---
class: left, top

# Why use `\(B_{x_0}\)` in the GLM?

## Our Dimension Reduction Assumption

- In OPG, the dimension reduction assumption is `\(\beta\)` satisfies `\(Y \indep E(Y|X)|\beta^\top X\)`.

  - This is equivalent to `\(E(Y|X) = E(Y|\beta^\top X)\)`. 
  
- Our the dimension reduction assumption is `\(\beta\)` satisfies `\(Y \indep \theta(X)|\beta^\top X\)`; 

  - `\(\theta(X)\)` is the canonical parameter of the multivariate GLM, through which `\(X\)` relates to `\(Y\)` 
  - Our dimension reduction assumption is equivalent to `\(\theta(X) = \theta(\beta^\top X)\)`
      - For linear exponential families, `\(\theta(X) = link^{-1}(E(Y|X))\)`, so our dimension reduction assumption is the same as OPG.

---
class: left, top

# Why use `\(B_{x_0}\)` in the GLM?

## Our Dimension Reduction Assumption

  - Because `\(\theta(X) = \theta(\beta^\top X)\)`, the gradient of `\(\theta(x)\)` satisfies
  
`\begin{align*}
\partial \theta(x)^\top /\partial x = \beta \partial
\theta(u)^\top/\partial u
\end{align*}`
  
  - That is, `\(\mathrm{span}(\partial \theta(x)^\top /\partial x) \subseteq \mathrm{span}(\beta)\)`
  
      - We say `\(\partial \theta(x)^\top /\partial x\)` is *unbiased* for the central mean subspace.
      
---
class: left, top

# Why use `\(B_{x_0}\)` in the GLM?

## Our Dimension Reduction Assumption

- In OPG, `\(\hat B_{x_0}\)` estimates the gradient of `\(E(Y|X=x)\)` at `\(x_0\)`, i.e. `\(\partial E(Y|X=x_0) /\partial x^\top\)`.

- Similarly, the `\(\hat B_{x_0}\)` obtained from minimizing
`\begin{align*}
-\ell_0(a_0, B_0;y,x,x_0) = -[a_0 + B_0^\top(x-x_0)]^\top y + b[a_0 + B_0^\top(x-x_0)]
\end{align*}`
`\(\hat B_{x_0}\)` estimates the gradient of `\(\theta(x)\)` at `\(x_0\)`, i.e. `\(\partial \theta(x_0) /\partial x^\top\)`.

** `\(\hat B_{x_0}\)` estimates the canonical gradient at `\(x_0\)`.**

---
class: left, top

# Outer Product of Canonical Gradients (OPCG)

Given a random sample `\(Y_{1:n}\)`, `\(X_{1:n}\)`, fit a local linear multivariate GLM about `\(x_0\)` by minimizing
`\begin{align*}
&amp; -\ell (a_{x_0}, B_{x_0}, x_0, ;X_{1:n}, Y_{1:n}) \\
= &amp;
\frac 1n \sum_{i=1}^n
K \bigg ( \frac{X_i - x_0}{h} \bigg )
\{-[a_{x_0} + B_{x_0}^\top (X_i - x_0)]^\top Y_i + b[a_{x_0} + B_{x_0}^\top (X_i - x_0) ] \}
\end{align*}`
where `\(b(\cdot)\)` determines the GLM, and `\(K(\cdot)\)` is a kernel weight with bandwidth `\(h\)`. 
The minimizer `\(\hat B_{x_0}\)` is used to estimate `\(\partial \theta(x_0) /\partial x^\top\)`.

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="35%"&gt; 
]

---
class: left, top

# Outer Product of Canonical Gradients (OPCG)

We fit a local linear multivariate GLM about each `\(X_j\)`, for `\(j=1,...,n\)`, by minimizing the full negative local linear log-likelihood:

`\begin{align*}
&amp; L(a_1,..,a_n, B_1,...,B_n; X_{1:n}, Y_{1:n})  \\
= &amp; -\frac {1}{n} \sum_{j,i=1}^n
K \bigg ( \frac{X_i - X_j}{h} \bigg ) 
\{[a_{j} + B_{j}^\top (X_i - X_j)]^\top Y_i - 
b(a_{j} + B_{j}^\top (X_i - X_j)) \} 
.
\end{align*}`

This provides a collection of minimizers `\(\hat B_1,\ldots,\hat B_j\)` that estimate `\(\partial \theta(X_j)/\partial x^\top\)`.

.center[
&lt;img class="image" src="images/fsdr_plot4.png" width="35%"&gt; 
]
---
class: left, top

# The OPCG Estimator

We use `\(\hat B_1,\ldots, \hat B_n\)` to construct the average outer product
`$$\hat \Lambda_{\mathrm{opcg}} = \frac 1n \sum_{j=1}^n \hat B_j \hat B_j^\top.$$` 

The **Outer Product of Canonical Gradients (OPCG) Estimator, `\(\hat \beta_{\mathrm{opcg}}\)`**, is the `\(d\)` leading eigenvectors of `\(\hat \Lambda_{\mathrm{opcg}}\)`.

.center[
&lt;img class="image" src="images/fsdr_plot5.png" width="35%"&gt;
]  

---
class: left, top 
# Properties related to OPCG
## Under some regularity assumptions...

  &lt;div class="prop" text="Unbiasedness and Exhaustiveness"&gt;
  Let
  \begin{align*}
  \Lambda_{\mathrm{opcg}}
  =
  E 
  \bigg \{ 
  \frac{\partial \theta(X)^\top}{\partial x}
  \frac{\partial \theta(X)}{\partial x^\top}
  \bigg \}
  .
  \end{align*}
  Under some regularity assumptions, \(\mathrm{span}( \Lambda_{\mathrm{opcg}} ) = \mathrm{span}( \beta )  \).
  &lt;/div&gt;   
--
  &lt;br/&gt;&lt;br&gt;
  &lt;div class="theorem" text="Consistency of OPCG"&gt;
  Let \(\eta\) be the leading \(d\) eigenvectors of \(\Lambda_{\mathrm{opcg}}\). Then, under some regularity, compactness and bandwidth assumptions, we have
  \begin{align*}
  \| \hat \beta_{\mathrm{opcg}}  - \eta \|_F = O_{a.s}
  ( h + h^{-1} \delta_{ph} + ( \log(n)/n )^{1/2} ),
  \end{align*}
  where \(\delta_{ph} = \sqrt{ \frac{\log n}{ nh^p} }\),
  \(  h \downarrow 0\), and \( h^{-1}\delta_{ph} \to 0\).
  &lt;/div&gt;
  
---
class: left, top 
# A more direct approach to `\(\beta\)`

- Recall: the gradient is unbiased
`\begin{align*}
\partial \theta(x)^\top /\partial x = \beta \partial
\theta(u)^\top/\partial u
\end{align*}`


--
  
We can incorporate this into the objective function directly by minimizing

`\begin{align*}
&amp; L(\beta, a_1,..,a_n, B_1,...,B_n; X_{1:n}, Y_{1:n})  \\
= &amp; -\frac {1}{n} \sum_{j,i=1}^n
W_{ij}(h)
\{[a_{j} + B_{j}^\top \beta^\top (X_i - X_j)]^\top Y_i - 
b(a_{j} + B_{j}^\top \beta^\top  (X_i - X_j)) \} 
.
\end{align*}`
where `\(W_{ij}(h) = K \bigg ( \frac{X_i - X_j}{h} \bigg )\)`.

- Alternate between minimizing w.r.t. `\(\beta\)` and `\(a_1,\ldots,a_n, B_1,\ldots,B_n\)` until `\(\beta\)` estimates converge.

- Minimizer of `\(\beta\)` is the **Minimum Average Deviance Estimator (MADE), `\(\hat \beta_{\mathrm{made}}\)`**. 


---
class: left, top 
# MADE


- MADE generalizes the MAVE estimator by (Xia, Tong, Li, et al., 2002)

- Our MADE estimator, `\(\hat \beta_{\mathrm{made}}\)`, is the multivariate version proposed by Adragni (2018)

  - But our MADE can handle multi-label problems simulatenously using the multivariate link function.

---
class: left, top 
# Refined OPCG and Refined MADE

We can refined our estimators `\(\hat \beta_{\mathrm{opcg}}\)` and `\(\hat \beta_{\mathrm{made}}\)` through an iterative procedure.

For Refined OPCG:
  1. Estimate `\(\hat \beta_{\mathrm{opcg}}\)`.
  2. Replaced the kernel weight `\(K (h^{-1} \| X_i - X_j \|)\)` in the objective function with the refined kernel weight 
  `\(K (h^{-1} \| \hat \beta_{\mathrm{opcg}}^\top (X_i - X_j)  )\|)\)`.
  3. Repeat 1 and 2 until `\(\hat \beta_{\mathrm{opcg}}\)` converges. The resulting estimator is the Refined OPCG estimator, `\(\hat \beta_{\mathrm{ropcg}}\)`

--

The same iterative procedure for MADE produces the Refined MADE estimator, `\(\hat \beta_{\mathrm{rmade}}\)`. 

---
class: left, top 
# Estimating the dimension, `\(d\)` 

  - Ladle plot and Predictor Augmentation methods are fast, eigen-based methods that can be applied to OPCG estimate `\(d\)`. (Luo and Li, 2020; Luo and Li, 2016)
  
    - Uses variation in eigenvectors and eigenvalues of `\(\hat \Lambda_{\mathrm{opcg}}\)` to determine `\(d\)`.
  
  - Cross-validation or sequential testing methods can be used to estimate `\(d\)` for MADE. (Adragni, 2018; Xia, Tong, Li, et al., 2002)
 
---
class: inverse, center, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text[Tuning the bandwidth]

---
class: left, top 
# Tuning the bandwidth, `\(h\)`.

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="45%"&gt;
]  

The bandwidth `\(h\)` in the kernel `\(K(h^{-1} \|X_i - X_j \|)\)` determines the size of the local neighbourhoods about points `\(X_j\)`.

This bandwidth needs to be tuned in our forward regression approaches. 

---
class: left, top
# Tuning the bandwidth, `\(h\)`.
 
We need to tune `\(h\)` in OPCG:

  - Cross Validation requires specifying a prediction method beforehand. 

  - Can choose `\(h\)` according to optimal bandwidth, such as `\(h^{opt} = cn^{-\frac{1}{(p+6)}}\)` (Xia, 2007)
  
    - but suggested values of `\(c\)` does not always work, and then you need to tune `\(c\)`.

--

For classification problems, we propose using a K-means clustering procedure for tuning `\(h\)`.

Our intuition: 

  - SDR should make classification easier, and classification is easiest when the dimension reduced predictors `\(\hat \beta^\top X\)` are clustered into their respective labels.

---
class: left, top
# Tuning the bandwidth, `\(h\)`.

Let `\(Y \in \{1,...,m\}\)` be categorical response, `\((Y, X)_{1:n}\)` be our training set, `\((Y,X)_{1:n_2}^{\mathbb{V}}\)` be our validation set.

--

Main idea: For each `\(h\)`, 

1. Estimate `\(\hat \beta_{\mathrm{opcg}}\)` on training set and construct the sufficient predictors on the validation set, i.e. construct `\(\hat \beta_{\mathrm{opcg}}^\top X_{1:n_2}^{\mathbb{V}}\)`.

2. Apply K-means to sufficient predictors `\(\hat \beta_{\mathrm{opcg}}^\top X_{1:n_2}^{\mathbb{V}}\)` for `\(m\)` clusters.

  - This returns `\(m\)` estimated clusters and the F-ratio, i.e. the Within Sums-of-Squares (WSS) over Between Sums-of-Squares (BSS), for each `\(h\)`.

3. Select `\(h\)` that minimizes the F-ratio from K-means. 

  - Small F-ratio means small WSS and large BSS

---
class: left, top
# Tuning the bandwidth, `\(h\)`.
 
But 
 - Estimating `\(m\)` clusters implicitly assumes we have only 1 cluster per class
 - we ought to incorporate the class/label information from `\(Y_{1:n}^{\mathbb{V}}\)`, when available.

We modifiy K-means so that:

1. we can estimate more than 1 cluster per class;

2. k-means uses label information from `\(Y_{1:n}^{\mathbb{V}}\)`; a "Supervised" K-means

**In place of a validation set, we apply our supervised k-means on the training set in a k-fold manner.**
---
class: inverse, center, middle

&lt;!-- inverse makes the background black and text white --&gt;

# .bg-text[Simulations and Data Analyses]

--
.left[

1. Goal: generalize OPG to Categorical and Ordinal-Categorical Responses

2. Propose: OPCG that generalizes OPG to linear exponential families

3. Categorical and Ordinal-Categorical variables are linear exponential families, so OPCG we can apply OPCG
  
]

---
class: left, top
# OPCG Procedure

Given data:

  1. Simulations: we have 3 data sets
  
    - Training set used for estimating `\(\hat \beta_{opcg}\)`; 
    
      - Used for estimating `\(d\)` as well.
      
    - Validation set used for tuning bandwidth `\(h\)`
    
    - Testing set used for assessing performance
  
  2. Applications: Data is split into two sets:
  
    - Training set used for estimating `\(\hat \beta_{opcg}\)`; 
    
      - Used for estimating `\(d\)` as well.
      
      - Used K-fold tuning for bandwidth `\(h\)`
      
    - Testing set used for assessing performance


---
class: left, top
# Simulations


Our predictor will be `\(X=(X_1,X_2,X_3,...,X_{10}) \in \R^{10}\)`.

  - We generate generate 5 clusters from a bivariate normal `\((X_3, X_7)\)`, augmented with 8 standard normals.

  - Two clusters are labeled 1, two are labeled 2, and one cluster is labeled 3; So `\(Y \in \{1,2,3\}\)` is categorical.
 
  - We draw an equal number of observations from each cluster for the training, validation and testing set.
 
&lt;!-- --- --&gt;
&lt;!-- count: false --&gt;
&lt;!-- class: left, top --&gt;
&lt;!-- # Simulations - Ladle, PA  --&gt;


&lt;!-- &lt;img align="center" class="image" src="images/opcg_sim_ladle.png" width="49%"&gt; --&gt;
&lt;!-- &lt;img align="center" class="image" src="images/opcg_sim_pa.png" width="49%"&gt; --&gt;

&lt;!-- Used `\(h=1\)` for order determination, 200 samples for Ladle and PA. --&gt;



---
class: ani-slide
# K-mean Tuning for `\(h\)`
 
&lt;iframe src="images/tuning_sc.html" width="100%" height="95%" frameborder="0" &gt;&lt;/iframe&gt;



---
count: false
class: left, top
# Simulations - tuning and estimation

K-fold supervised K-means Tuning: `\(h \approx 1.26\)`;

&lt;img align="centered" class="image" src="images/sim_dist_table4.png" width="100%"&gt;

  - DR is Directional Regression (Li and Wang, 2007)  
  
  - PL-OPCG is a per-label approach; Lambert-Lacroix and Peyre's suggestion for multi-label problems.
    - estimates 2 SDR directions per binary logistic problem for each class, for 6 total, and selects the 2 that explain the most variation.
    
  - PW-OPCG is pairwise approach; Adragni's suggestion for multi-label problems.  
    - estimates 2 SDR directions per pair of classes \{1,2\}, \{1,3\}, and \{2,3\}, for 6 total, and selects the 2 that explain the most variation. 
    
  
  
  
---
count: false
class: left, top
# Simulations - Sufficient Predictors `\(\hat \beta^\top X^{test}\)`

.center[
&lt;img align="centered" class="image" src="images/sim_dist_table4.png" width="80%"&gt;

&lt;img align="center" class="image" src="images/opcg_sim_test-opg.png" width="80%"&gt;
]

---
class: left, top
# Ordinal-Categorical Data Analysis
## Red Wine Quality

We a wine quality rating data set from the UCI repository: 

  - Red Wine Quality

    - p=11; train/test: 1000/599; Ordinal response - Wine Quality Score;


&lt;img align="centered" class="image" src="images/opcg_wine_test.png" width="100%"&gt;


---
class: left, top
# Categorical Data Analysis  

We analyze three datasets with categorical responses: 

  - Handwritten Digits (Pendigit) from UCI
  
    - p=16; train/test:=1000/1000; resp=0-9
    
  - USPS Handwritten Digits  
  
    - p=256; train/test:=1000/1007; resp=0-9
    
  - ISOLET from UCI
  
    - p=618; train/test:=6334/1553; resp=a-z

---
class: left, top
# Categorical Classification Error using SVM

.center[
&lt;img align="center" class="image" src="images/cat_class_table6-1.png" width="90%"&gt;
]

---
class: ani-slide
# Pen Digit 10 - SIR/DR - 1,6,7,9
&lt;iframe src="images/dr_pendigit4.html" width="90%" height="95%" frameborder="0"&gt;&lt;/iframe&gt;

---
count: false
class: ani-slide
# Pen Digit 10 - OPCG - 1,6,7,9

&lt;iframe src="images/opcg_pendigit4.html" width="90%" height="95%" frameborder="0"&gt;&lt;/iframe&gt; 

---
class: left, middle, inverse
# .bg-text[Summary]


1. Provided the Multivariate Link Functions for Ordinal-Categorical Responses.

2. Generalized OPG to categorical and ordinal responses using multivariate GLMs

3. Introduced a supervised K-means tuning procedure for classification


Paper Status: Preparing for submission



---
class: inverse, left, middle


# .center[.bg-text[Forward SDR for Categorical and Ordinal Response with Functional Predictors] ]

1. Local linear multivariate GLM with Functional Predictors

2. Tensor Product of Canonical Gradients



---
class: top, left

# Local linear multivariate GLM for Functional Predictors

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="40%"&gt;
&lt;img class="image" src="images/fsdr_plot5.png" width="40%"&gt;
]  

- `\(Y \in \R^m\)`, e.g. categorical or ordinal variable,
- `\(X \in \mathscr{H}_X\)` a Reproducing Kernel Hilbert Space of functions,
- `\(\partial \theta(x_0)^\top / \partial x \in \mathscr{B}(\mathscr{H}_X, \R^m)\)` is the Frechet Derivative at `\(x_0\)`,

---
count:false
class: top, left

# Local linear multivariate GLM for Functional Predictors

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="40%"&gt;
&lt;img class="image" src="images/fsdr_plot5.png" width="40%"&gt;
]  

- We want a finite-rank linear operator `\(T \in \mathscr{B}(\mathscr{H}_X, \R^d)\)` such that `\(Y \indep E(Y|X) | T(X)\)`

- the sufficient predictors are `\(T(X) = (\langle f_1, X \rangle_{\mathscr{H}_X},\ldots,\langle f_d, X \rangle_{\mathscr{H}_X}) \in \R^d\)`

---
class: top, left

# Progress Report

1. Existing methods in Literature: 
    - Generalized Functional Linear Model (Müller and Stadtmüller, 2005; James, 2002; Li, Wang, and Carroll, 2010)  
    - Fit local quasi-likelihood with functional predictors and euclidean predictors; no dimension reduction for functional predictors. (Li, Wang, and Carroll, 2010)
    
2. We estimate Local linear multivariate GLM with functional predictor by employing a RKHS using the "kernel method" 

3. We develop an estimator for the range of `\(T^*\)`, which is a functional SDR subspace for `\(E(Y|X)\)`
.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="30%"&gt;
&lt;img class="image" src="images/fsdr_plot5.png" width="30%"&gt;
] 


&lt;!-- --- --&gt;
&lt;!-- class: top, left --&gt;

&lt;!-- # Local GLM for Functional Predictors --&gt;

&lt;!-- \begin{align*} --&gt;
&lt;!-- \{ a_{x_0}^\top Y + \langle (X - x_0), B_{x_0}^*Y \rangle_{H_X} -  --&gt;
&lt;!-- b( a_{x_0} +  B_{x_0}(X - x_0)) \}  --&gt;
&lt;!-- . --&gt;
&lt;!-- \end{align*} --&gt;
&lt;!-- --- --&gt;
&lt;!-- class: top, left --&gt;

&lt;!-- # Local GLM for Functional Predictors --&gt;

&lt;!-- \begin{align*} --&gt;
&lt;!-- &amp; L(a_1,..,a_n, B_1,...,B_n; X_{1:n}, Y_{1:n}) \\ --&gt;
&lt;!-- = &amp; -\frac 1n \sum_{j=1}^n \ell_j (a_j, B_j; X_j, X_{1:n}, Y_{1:n}) \\  --&gt;
&lt;!-- = &amp; -\frac {1}{n} \sum_{j,i=1}^n --&gt;
&lt;!-- K \bigg ( \frac{X_i - X_j}{h} \bigg )\\ --&gt;
&lt;!-- &amp; \times  --&gt;
&lt;!-- \{[a_{j} + B_{j}^\top (X_i - X_j)]^\top Y_i -  --&gt;
&lt;!-- b(a_{j} + B_{j}^\top (X_i - X_j)) \}  --&gt;
&lt;!-- . --&gt;
&lt;!-- \end{align*} --&gt;

---
class: left, top
# TPCG - UJI 3,5,9

UJI handwritten characters dataset (version 2) from UCI repository

  - 60 writers, 97 characters, 2 repetitions 
  - 1st repetition is training set; 2nd repetition is testing set
    - 60 samples per character in training set and testing set
  - characters are 2d curves; 
  - Goal: Classification of labels

---
class: left, top
# TPG - UJI 3,5,9 
## Classify digits 3,5,9:
  
   
  - response is the categorical label: '3', '5', '9' ;
  - predictor is 2d curves `\((X_1(t), X_2(t))\)` for '3', '5', '9';
  
.center[
&lt;img align="center" class="image" src="images/tpcg_uji_curves.png" width="85%"&gt;
]
  
---
class: left, top
# TPCG - UJI 3,5,9
## Classify digits 3,5,9:
  
  - response is the categorical label: '3', '5', '9' ;
  - predictor is 2d curve `\((X_1(t), X_2(t))\)` for '3', '5', '9'; 
  
  
  
.center[
&lt;img align="center" class="image" src="images/tpcg_uji_preds359.png" width="50%"&gt;
]

---
class: left, top
# Phoneme Data

Phoneme dataset for speech Recognition from 'the Elements of Statistical Learning' text

  - five phonemes as responses: 
    - 'sh' as in 'she', 
    - 'dcl' as in 'dark', 
    - 'iy' as in the vowel in 'she', 
    - 'aa' as in the vowel in 'dark',
    - 'ao' as in the vowel in 'water'.
  
  - The dataset contains a total of 4509 speech recordings of 32 milliseconds. We compute a curve `\(X(t)\)` for each recording.
  
  &lt;!-- For each recording, the curves are computed using a log-periodogram of length 256, of which we use only the first 150.  --&gt;
  
  - We draw a training and testing set of 500 curves each. Each set is evenly divide between the phonemes.
  
  - Goal: Classification of labels
  
---
class: left, top
# Phoneme Data

.center[
&lt;img align="center" class="image" src="images/tpcg_phoneme_curves.png" width="100%"&gt;
]

---

class: ani-slide
# Phoneme Data

&lt;iframe src="images/tpcg_phenome.html" width="90%" height="95%" frameborder="0"&gt;&lt;/iframe&gt;


---
class: inverse, left, middle


# .center[.bg-text[Forward SDR for Functional Response with Functional Predictors] ]

1. Local function-on-function linear regression

2. Tensor Product of Gradients 


---
class: top, left

# Local Function-on-Function Linear Regression 

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="40%"&gt;
&lt;img class="image" src="images/fsdr_plot5.png" width="40%"&gt;
]  

- `\(Y \in \mathscr{H}_Y\)` a Reproducing Kernel Hilbert Space of functions,
- `\(X \in \mathscr{H}_X\)` a Reproducing Kernel Hilbert Space of functions,
- `\(\partial E(Y|X)^\top / \partial x \in \mathscr{B}(\mathscr{H}_X, \mathscr{H}_Y)\)` is the Frechet Derivative, 

---
count:false
class: top, left

# Local Function-on-Function Linear Regression 

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="40%"&gt;
&lt;img class="image" src="images/fsdr_plot5.png" width="40%"&gt;
]  

- We want a finite-rank linear operator `\(T \in \mathscr{B}(\mathscr{H}_X, \R^d)\)` such that `\(Y \indep E(Y|X) | T(X)\)`

- the sufficient predictors are `\(T(X) = (\langle f_1, X \rangle_{\mathscr{H}_X},\ldots,\langle f_d, X \rangle_{\mathscr{H}_X}) \in \R^d\)`


---
class: top, left

# Progress Report

1. Existing methods in Literature: 
  - Local scalar-on-function linear regression (Baillo and Grané, 2009; Ferraty and Nagy, 2021)  
  - Ferraty and Nagy (2021) generalize ADE to scalar-on-function context  

2. We estimate local function-on-function linear regression by using weighted covariance and cross-covariance operators in an RKHS

3. We have developed an estimator for the range of `\(T^*\)`, which is a functional SDR subspace for `\(E(Y|X)\)`

.center[
&lt;img class="image" src="images/fsdr_plot3-1.png" width="30%"&gt;
&lt;img class="image" src="images/fsdr_plot5.png" width="30%"&gt;
]

---
class: left, top
# TPG - UJI 3,5,9 to a,b,c

UJI handwritten characters dataset (version 2) from UCI repository

  - 60 writers, 97 characters, 2 repetitions 
  - 1st repetition is training set; 2nd repetition is testing set
    - 60 samples per character in training set and testing set
  - characters are 2d curves; 
  - Goal: develop an association between characters

---
class: left, top
# TPG - UJI 3,5,9 to a,b,c
## Associate digits '3', '5', '9' to letters 'a', 'b', 'c':
  
  - response is 2d curve `\((Y_1(t), Y_2(t))\)` for 'a', 'b', 'c';
  - predictor is 2d curves `\((X_1(t), X_2(t))\)` for '3', '5', '9'; 
  

.center[
&lt;img align="center" class="image" src="images/tpg_curves.png" width="85%"&gt;
]
  
---
class: left, top
# TPG - UJI 3,5,9 to a,b,c
## Associate digits '3', '5', '9' to letters 'a', 'b', 'c':
  
  - predictor is 2d curves `\((X_1(t), X_2(t))\)` for '3', '5', '9'; 
  - response is 2d curve `\((Y_1(t), Y_2(t))\)` for 'a', 'b', 'c';
  
.center[
&lt;img align="center" class="image" src="images/tpg_uji_preds359abc.png" width="50%"&gt;
]




---
class: top, left

# Future Work

## (Immediate) Future work

  1. Consistency of TPG and TPCG
  
  2. Tuning TPG and TPCG
  
  3. Estimating the dimension, `\(d\)`.
  
  4. Generalizing MAVE and MADE to functional data
  
  5. Simulations and additional Applications


&lt;!-- -- --&gt;

&lt;!-- ## (Near) Future work --&gt;

&lt;!--   1. Local function-on-function Linear Regression --&gt;

&lt;!--   2. Forward Nonlinear SDR --&gt;

&lt;!--   3. Forward Nonlinear Functional SDR   --&gt;

&lt;!-- -- --&gt;

&lt;!-- ## (Future) Future work --&gt;

&lt;!--   1. Post Dimension Reduction Inference for Forward SDR --&gt;

&lt;!--   2. Forward SDR for Categorical Predictors --&gt;
  
&lt;!-- &lt;div class="centered"&gt; \[\vdots\] &lt;/div&gt; --&gt;
  



---
layout: false
# References

Adragni, K. P. (2018). "Minimum average deviance estimation for
sufficient dimension reduction". In: _Journal of Statistical
Computation and Simulation_ 88.3, pp. 411-431.

Agresti, A. (2010). _Analysis of ordinal categorical data_. Vol. 656.
John Wiley &amp; Sons.

Baillo, A. and A. Grané (2009). "Local linear regression for functional
predictor and scalar response". In: _Journal of Multivariate Analysis_
100.1, pp. 102-111.

Bura, E., B. Li, L. Li, et al. (2021). "A Conversation with Dennis
Cook". In: _Statistical Science_ 36.2, pp. 328 - 337. DOI:
[10.1214/20-STS801](https://doi.org/10.1214%2F20-STS801). URL:
[https://doi.org/10.1214/20-STS801](https://doi.org/10.1214/20-STS801).

Cook, R. D. and B. Li (2002). "Dimension reduction for conditional mean
in regression". In: _The Annals of Statistics_ 30.2, pp. 455-474.

Ferraty, F. and S. Nagy (2021). "Scalar-on-function local linear
regression and beyond". In: _Biometrika_. asab027. ISSN: 0006-3444.
DOI:
[10.1093/biomet/asab027](https://doi.org/10.1093%2Fbiomet%2Fasab027).
eprint:
https://academic.oup.com/biomet/advance-article-pdf/doi/10.1093/biomet/asab027/37231257/asab027.pdf.
URL:
[https://doi.org/10.1093/biomet/asab027](https://doi.org/10.1093/biomet/asab027).

---
layout: false
# References

Härdle, W. and T. M. Stoker (1989). "Investigating smooth multiple
regression by the method of average derivatives". In: _Journal of the
American statistical Association_ 84.408, pp. 986-995.

Kim, K., B. Li, Z. Yu, et al. (2020). "On post dimension reduction
statistical inference". In: _The Annals of Statistics_ 48.3, pp.
1567-1592.

Kong, E. and Y. Xia (2012). "A single-index quantile regression model
and its estimation". In: _Econometric Theory_ 28.4, pp. 730-768.

Lambert-Lacroix, S. and J. Peyre (2006). "Local likelihood regression
in generalized linear single-index models with applications to
microarray data". In: _Computational statistics &amp; data analysis_ 51.3,
pp. 2091-2113.

Lee, K., B. Li, and F. Chiaromonte (2013). "A general theory for
nonlinear sufficient dimension reduction: Formulation and estimation".
In: _The Annals of Statistics_ 41.1, pp. 221-249.

Li, B. (2018). _Sufficient Dimension Reduction: Methods and
Applications with R_. CRC Press.

---
layout: false
# References

Lambert-Lacroix, S. and J. Peyre (2006). "Local likelihood regression
in generalized linear single-index models with applications to
microarray data". In: _Computational statistics &amp; data analysis_ 51.3,
pp. 2091-2113.

Lee, K., B. Li, and F. Chiaromonte (2013). "A general theory for
nonlinear sufficient dimension reduction: Formulation and estimation".
In: _The Annals of Statistics_ 41.1, pp. 221-249.

Li, B. (2018). _Sufficient Dimension Reduction: Methods and
Applications with R_. CRC Press.

Li, B. and J. Song (2017). "Nonlinear sufficient dimension reduction
for functional data". In: _The Annals of Statistics_ 45.3, pp.
1059-1095.

Li, B. and J. Song (2021). "Dimension Reduction For Functional Data
Based on Weak Conditional Moments". In: _The Annals of Statistics_.

Li, B. and S. Wang (2007). "On directional regression for dimension
reduction". In: _Journal of the American Statistical Association_
102.479, pp. 997-1008.

---
layout: false
# References

Li, K. (1991). "Sliced inverse regression for dimension reduction". In:
_Journal of the American Statistical Association_ 86.414, pp. 316-327.

Li, Y., N. Wang, and R. J. Carroll (2010). "Generalized functional
linear models with semiparametric single-index interactions". In:
_Journal of the American Statistical Association_ 105.490, pp. 621-633.

Luo, W. and B. Li (2016). "Combining eigenvalues and variation of
eigenvectors for order determination". In: _Biometrika_ 103.4, pp.
875-887.

Luo, W. and B. Li (2020). "On order determination by predictor
augmentation". In: _Biometrika_.

Luo, W., B. Li, and X. Yin (2014). "On efficient dimension reduction
with respect to a statistical functional of interest". In: _The annals
of statistics_ 42.1, pp. 382-412.

Ma, Y. and L. Zhu (2012). "A semiparametric approach to dimension
reduction". In: _Journal of the American Statistical Association_
107.497, pp. 168-179.

&lt;!-- --- --&gt;
&lt;!-- layout: false --&gt;
&lt;!-- # References --&gt;

&lt;!-- ```{r, echo=FALSE, results="asis"} --&gt;
&lt;!-- PrintBibliography(c(bib_sdr, bib_opg, bib_funsdr), start=25) --&gt;
&lt;!-- ``` --&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
